{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "def get_files(path, files):\n",
    "    \"\"\"get files list\"\"\"\n",
    "    valid_f = re.compile(files)\n",
    "    flist=[]\n",
    "    os.chdir(path)\n",
    "    for f in os.listdir('.'):\n",
    "        if(valid_f.match(f) != None): flist.append(f) \n",
    "    return flist\n",
    "\n",
    "def get_meta(l, d):\n",
    "    \"\"\"get meta data\"\"\"\n",
    "    layer_re = re.compile('.+ Creating layer (.+)')\n",
    "    iters_re = re.compile('Testing for\\s+(\\d+)\\s+iterations.')\n",
    "    th_re = re.compile('Number of OpenMP threads: (\\d+)')\n",
    "    batch_size_re = re.compile('batch_size: (\\d+)')\n",
    "    m = layer_re.findall(l)\n",
    "    if(m is not []): layers = m\n",
    "\n",
    "    m = iters_re.search(l)\n",
    "    if(m is not None): d['iterations'] = int(m.group(1))    \n",
    "\n",
    "    m = th_re.search(l)\n",
    "    if(m is not None): d['threads'] = int(m.group(1))    \n",
    "\n",
    "    m = batch_size_re.search(l)\n",
    "    if(m is not None): d['batch size'] = int(m.group(1))    \n",
    "    \n",
    "    d['layers'] = set(layers)\n",
    "    return d\n",
    "\n",
    "\n",
    "def get_data(l, entry):\n",
    "    \"\"\"get the time of each layer and the total\"\"\"\n",
    "    #total timing\n",
    "    fd_re = re.compile('.+Average Forward pass:\\s+(.+)\\s+ms.')\n",
    "    bd_re = re.compile('.+Average Backward pass:\\s+(.+)\\s+ms.')\n",
    "    total_re = re.compile('.+Total Time:\\s+(.+)\\s+ms.')\n",
    "    m = fd_re.search(l)\n",
    "    if(m is not None): entry['avg forward'] = float(m.groups(1)[0])/1e3\n",
    "    m = bd_re.search(l)\n",
    "    if(m is not None): entry['avg backward'] = float(m.groups(1)[0])/1e3\n",
    "    m = total_re.search(l)\n",
    "    if(m is not None):\n",
    "        entry['total time'] = float(m.groups(1)[0])/1e3\n",
    "        entry['time per iteration'] = entry['total time']/entry['iterations']\n",
    "    # layers timing\n",
    "    ltime_re = re.compile('.+\\s+(\\w+)\\s+(forward|backward):\\s*(.+)\\s+ms.')\n",
    "    m = ltime_re.findall(l)\n",
    "    layers_data = {lname+' '+direction: float(val)/1e3 for lname,direction,val in m}\n",
    "    entry.update(layers_data)\n",
    "    # layers memory footprint\n",
    "    for layer in entry['layers']:\n",
    "        lmem_re = re.compile('Creating layer\\s+('+layer+').*Memory required for data:\\s+(\\d+)',re.DOTALL)\n",
    "        m = lmem_re.search(l)\n",
    "        if(m is not None): entry[m.groups(0)[0]+' memory'] = int(m.groups(0)[1])\n",
    "\n",
    "def get_df(flist):\n",
    "    \"\"\"Parse the files in a dataframe\"\"\"\n",
    "    data = []\n",
    "    for f in flist:\n",
    "        with open(f, 'r') as fp:\n",
    "            entry = dict()\n",
    "            txt = fp.read() #.split('\\n')\n",
    "            get_meta(txt, entry)\n",
    "            get_data(txt, entry)\n",
    "            entry['file name'] =f\n",
    "            data.append(pd.DataFrame(entry))\n",
    "    return pd.concat(data)\n",
    "\n",
    "def normalize_batches(df):\n",
    "    \"\"\"Normalize the time by the batch size\"\"\"\n",
    "    df_norm = df.copy()\n",
    "    time_fields = [s for s in df.columns.values if('ward' in s or 'time' in s)]\n",
    "    for f in time_fields:\n",
    "        df_norm.loc[:, f] = df.loc[:, f]/df.loc[:, 'batch size']\n",
    "    return df_norm\n",
    "\n",
    "def normalize_time(df):\n",
    "    \"\"\"Normalize the time by the total time\"\"\"\n",
    "    df_norm = df.copy()\n",
    "    time_fields = [s for s in df.columns.values if('ward' in s  or 'time per iteration' in s)]\n",
    "    for f in time_fields:\n",
    "        df_norm.loc[:, f] = 100.0*df.loc[:, f]/df.loc[:, 'time per iteration']\n",
    "    del df_norm['total time']\n",
    "    return df_norm\n",
    "\n",
    "def group_small_entries(df, threas):\n",
    "    \"\"\"Sum insignificant entries in one column that fall\n",
    "    below the 'threas' percentile of the total time\"\"\"\n",
    "    df_filt = df.copy()\n",
    "    df_filt['others'] = 0.0\n",
    "    df_norm_time = normalize_time(df)\n",
    "    time_fields = [s for s in df.columns.values if('ward' in s)]\n",
    "    for f in time_fields:\n",
    "        if(not all(df_norm_time[f].apply(lambda x: x > threas))):\n",
    "            df_filt['others'] = df_filt['others'] + df_filt[f]\n",
    "            del df_filt[f]\n",
    "    return df_filt\n",
    "\n",
    "def plot_batch_scaling(df, threas):\n",
    "    \"\"\"Plot batch scaling from a data frame table\"\"\"\n",
    "    df_norm_batch = normalize_batches(df)\n",
    "    df_filt = group_small_entries(df_norm_batch, threas)\n",
    "\n",
    "    layers_cols = [s for s in df_filt.columns.values if('ward' in s and not 'avg' in s)]\n",
    "    layers_cols = layers_cols + ['others']\n",
    "    df_filt.index = df_filt['batch size']\n",
    "    plt_data = pd.DataFrame(df_filt[layers_cols],index=df_filt['batch size'], columns=layers_cols)\n",
    "\n",
    "    plt_data= plt_data.sort_index()\n",
    "\n",
    "    ax = plt_data.plot(marker='o', stacked=True)\n",
    "    ax.set_xscale('log',basex=2)\n",
    "    ax.set_ylabel('Stacked time per batch size per iter. (seconds)')\n",
    "    ax.set_xlabel('Batch size', size=20)\n",
    "    return ax\n",
    "\n",
    "def plot_thread_scaling(df, threas):\n",
    "    \"\"\"Plot thread scaling from a data frame table\"\"\"\n",
    "    df_filt = group_small_entries(df, threas)\n",
    "\n",
    "    layers_cols = [s for s in df_filt.columns.values if('ward' in s and not 'avg' in s)]\n",
    "    layers_cols = layers_cols + ['others']\n",
    "    df_filt.index = df_filt['threads']\n",
    "    plt_data = pd.DataFrame(df_filt[layers_cols],index=df_filt['threads'], columns=layers_cols)\n",
    "\n",
    "    plt_data= plt_data.sort_index()\n",
    "\n",
    "    ax = plt_data.plot(marker='o')\n",
    "    ax.set_xscale('log',basex=2)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylabel('Time per iter. (seconds)')\n",
    "    ax.set_xlabel('Threads #')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flist = get_files('/Users/tmalas/Desktop/caffe/atlas_caffe_scripts/batch_scaling_136th_knl/','.*.out$')\n",
    "df = get_df(flist)\n",
    "ax = plot_batch_scaling(df, 3.0)\n",
    "ax.set_title('KNL batch scaling (Stacked, sums to total time)')\n",
    "ax.set_ylim(ymin=0)\n",
    "plt.savefig('KNL batch scaling.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flist = get_files('/Users/tmalas/Desktop/caffe/atlas_caffe_scripts/batch_scaling_16threads_hsw/','.*.out$')\n",
    "df = get_df(flist)\n",
    "ax = plot_batch_scaling(df, 3.0)\n",
    "ax.set_title('Haswell batch scaling (Stacked, sums to total time)')\n",
    "ax.set_ylim(ymin=0)\n",
    "plt.savefig('Haswell batch scaling.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flist = get_files('/Users/tmalas/Desktop/caffe/atlas_caffe_scripts/thread_scaling_1batch_hsw/','.*.out$')\n",
    "df = get_df(flist)\n",
    "ax = plot_thread_scaling(df, 3.0)\n",
    "ax.set_title('Haswell thread scaling')\n",
    "ax.set_ylim(ymin=0)\n",
    "plt.savefig('Haswell thread scaling.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flist = get_files('/Users/tmalas/Desktop/caffe/atlas_caffe_scripts/thread_scaling_1batch_knl/','.*.out$')\n",
    "df = get_df(flist)\n",
    "ax = plot_thread_scaling(df, 3.0)\n",
    "ax.set_title('KNL thread scaling')\n",
    "ax.set_ylim(ymin=0)\n",
    "plt.savefig('KNL thread scaling.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
