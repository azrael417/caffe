sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer13_dir0.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=13 -prof_forward_direction=0
I1031 14:56:55.196279 101376 caffe.cpp:444] Use CPU.
I1031 14:57:13.103749 101376 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 14:57:13.165335 101376 cpu_info.cpp:455] Total number of sockets: 1
I1031 14:57:13.177971 101376 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 14:57:13.190059 101376 cpu_info.cpp:461] Total number of processors: 256
I1031 14:57:13.208173 101376 cpu_info.cpp:464] GPU is used: no
I1031 14:57:13.217574 101376 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 14:57:13.226872 101376 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 14:57:13.239888 101376 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 14:57:22.429549 101376 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 14:57:23.119979 101376 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 14:57:25.549113 101376 layer_factory.hpp:114] Creating layer data
I1031 14:57:25.706215 101376 net.cpp:160] Creating Layer data
I1031 14:57:25.757495 101376 net.cpp:570] data -> data
I1031 14:57:26.260963 101376 net.cpp:570] data -> label
I1031 14:57:33.756891 101376 net.cpp:210] Setting up data
I1031 14:57:33.840675 101376 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 14:57:33.947418 101376 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 14:57:33.954741 101376 net.cpp:225] Memory required for data: 184516
I1031 14:57:34.034340 101376 layer_factory.hpp:114] Creating layer conv1
I1031 14:57:34.400455 101376 net.cpp:160] Creating Layer conv1
I1031 14:57:34.453740 101376 net.cpp:596] conv1 <- data
I1031 14:57:34.580199 101376 net.cpp:570] conv1 -> conv1
I1031 14:58:11.773288 101376 net.cpp:210] Setting up conv1
I1031 14:58:11.849617 101376 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 14:58:11.859911 101376 net.cpp:225] Memory required for data: 7805124
I1031 14:58:12.192081 101376 layer_factory.hpp:114] Creating layer relu1
I1031 14:58:12.322237 101376 net.cpp:160] Creating Layer relu1
I1031 14:58:12.327219 101376 net.cpp:596] relu1 <- conv1
I1031 14:58:12.362290 101376 net.cpp:557] relu1 -> conv1 (in-place)
I1031 14:58:12.579021 101376 net.cpp:210] Setting up relu1
I1031 14:58:12.581737 101376 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 14:58:12.582110 101376 net.cpp:225] Memory required for data: 15425732
I1031 14:58:12.582332 101376 layer_factory.hpp:114] Creating layer dropout1
I1031 14:58:12.615803 101376 net.cpp:160] Creating Layer dropout1
I1031 14:58:12.616159 101376 net.cpp:596] dropout1 <- conv1
I1031 14:58:12.618921 101376 net.cpp:570] dropout1 -> drop1
I1031 14:58:12.730774 101376 net.cpp:210] Setting up dropout1
I1031 14:58:12.744523 101376 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 14:58:12.744915 101376 net.cpp:225] Memory required for data: 23046340
I1031 14:58:12.745252 101376 layer_factory.hpp:114] Creating layer pool1
I1031 14:58:12.851794 101376 net.cpp:160] Creating Layer pool1
I1031 14:58:12.852113 101376 net.cpp:596] pool1 <- drop1
I1031 14:58:12.852500 101376 net.cpp:570] pool1 -> pool1
I1031 14:58:13.256711 101376 net.cpp:210] Setting up pool1
I1031 14:58:13.261330 101376 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 14:58:13.261705 101376 net.cpp:225] Memory required for data: 24951492
I1031 14:58:13.262012 101376 layer_factory.hpp:114] Creating layer conv2
I1031 14:58:13.324784 101376 net.cpp:160] Creating Layer conv2
I1031 14:58:13.329181 101376 net.cpp:596] conv2 <- pool1
I1031 14:58:13.344728 101376 net.cpp:570] conv2 -> conv2
I1031 14:58:19.863479 101376 net.cpp:210] Setting up conv2
I1031 14:58:19.870338 101376 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 14:58:19.880131 101376 net.cpp:225] Memory required for data: 26733764
I1031 14:58:19.947243 101376 layer_factory.hpp:114] Creating layer relu2
I1031 14:58:19.954502 101376 net.cpp:160] Creating Layer relu2
I1031 14:58:19.959300 101376 net.cpp:596] relu2 <- conv2
I1031 14:58:19.965972 101376 net.cpp:557] relu2 -> conv2 (in-place)
I1031 14:58:19.968993 101376 net.cpp:210] Setting up relu2
I1031 14:58:19.973799 101376 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 14:58:19.980072 101376 net.cpp:225] Memory required for data: 28516036
I1031 14:58:19.988376 101376 layer_factory.hpp:114] Creating layer dropout2
I1031 14:58:19.995049 101376 net.cpp:160] Creating Layer dropout2
I1031 14:58:20.011884 101376 net.cpp:596] dropout2 <- conv2
I1031 14:58:20.015909 101376 net.cpp:570] dropout2 -> drop2
I1031 14:58:20.022779 101376 net.cpp:210] Setting up dropout2
I1031 14:58:20.027217 101376 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 14:58:20.031672 101376 net.cpp:225] Memory required for data: 30298308
I1031 14:58:20.038112 101376 layer_factory.hpp:114] Creating layer pool2
I1031 14:58:20.040307 101376 net.cpp:160] Creating Layer pool2
I1031 14:58:20.040618 101376 net.cpp:596] pool2 <- drop2
I1031 14:58:20.040891 101376 net.cpp:570] pool2 -> pool2
I1031 14:58:20.049612 101376 net.cpp:210] Setting up pool2
I1031 14:58:20.056113 101376 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 14:58:20.066354 101376 net.cpp:225] Memory required for data: 30759108
I1031 14:58:20.069347 101376 layer_factory.hpp:114] Creating layer conv3
I1031 14:58:20.073889 101376 net.cpp:160] Creating Layer conv3
I1031 14:58:20.076429 101376 net.cpp:596] conv3 <- pool2
I1031 14:58:20.086782 101376 net.cpp:570] conv3 -> conv3
I1031 14:58:20.723945 101376 net.cpp:210] Setting up conv3
I1031 14:58:20.731976 101376 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 14:58:20.736964 101376 net.cpp:225] Memory required for data: 31160516
I1031 14:58:20.753075 101376 layer_factory.hpp:114] Creating layer relu3
I1031 14:58:20.761584 101376 net.cpp:160] Creating Layer relu3
I1031 14:58:20.766078 101376 net.cpp:596] relu3 <- conv3
I1031 14:58:20.770377 101376 net.cpp:557] relu3 -> conv3 (in-place)
I1031 14:58:20.779047 101376 net.cpp:210] Setting up relu3
I1031 14:58:20.783697 101376 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 14:58:20.791935 101376 net.cpp:225] Memory required for data: 31561924
I1031 14:58:20.801239 101376 layer_factory.hpp:114] Creating layer dropout3
I1031 14:58:20.810698 101376 net.cpp:160] Creating Layer dropout3
I1031 14:58:20.822155 101376 net.cpp:596] dropout3 <- conv3
I1031 14:58:20.825078 101376 net.cpp:570] dropout3 -> drop3
I1031 14:58:20.825503 101376 net.cpp:210] Setting up dropout3
I1031 14:58:20.834040 101376 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 14:58:20.838095 101376 net.cpp:225] Memory required for data: 31963332
I1031 14:58:20.846889 101376 layer_factory.hpp:114] Creating layer pool3
I1031 14:58:20.855846 101376 net.cpp:160] Creating Layer pool3
I1031 14:58:20.857920 101376 net.cpp:596] pool3 <- drop3
I1031 14:58:20.860913 101376 net.cpp:570] pool3 -> pool3
I1031 14:58:20.863107 101376 net.cpp:210] Setting up pool3
I1031 14:58:20.865700 101376 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 14:58:20.882025 101376 net.cpp:225] Memory required for data: 32063684
I1031 14:58:20.886947 101376 layer_factory.hpp:114] Creating layer conv4
I1031 14:58:20.888876 101376 net.cpp:160] Creating Layer conv4
I1031 14:58:20.891348 101376 net.cpp:596] conv4 <- pool3
I1031 14:58:20.897413 101376 net.cpp:570] conv4 -> conv4
I1031 14:58:21.264880 101376 net.cpp:210] Setting up conv4
I1031 14:58:21.275238 101376 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 14:58:21.282189 101376 net.cpp:225] Memory required for data: 32137412
I1031 14:58:21.290062 101376 layer_factory.hpp:114] Creating layer relu4
I1031 14:58:21.298794 101376 net.cpp:160] Creating Layer relu4
I1031 14:58:21.309468 101376 net.cpp:596] relu4 <- conv4
I1031 14:58:21.316318 101376 net.cpp:557] relu4 -> conv4 (in-place)
I1031 14:58:21.318285 101376 net.cpp:210] Setting up relu4
I1031 14:58:21.318579 101376 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 14:58:21.321667 101376 net.cpp:225] Memory required for data: 32211140
I1031 14:58:21.323966 101376 layer_factory.hpp:114] Creating layer dropout4
I1031 14:58:21.326314 101376 net.cpp:160] Creating Layer dropout4
I1031 14:58:21.338711 101376 net.cpp:596] dropout4 <- conv4
I1031 14:58:21.351405 101376 net.cpp:570] dropout4 -> drop4
I1031 14:58:21.353984 101376 net.cpp:210] Setting up dropout4
I1031 14:58:21.358845 101376 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 14:58:21.367537 101376 net.cpp:225] Memory required for data: 32284868
I1031 14:58:21.371891 101376 layer_factory.hpp:114] Creating layer pool4
I1031 14:58:21.374404 101376 net.cpp:160] Creating Layer pool4
I1031 14:58:21.374687 101376 net.cpp:596] pool4 <- drop4
I1031 14:58:21.374963 101376 net.cpp:570] pool4 -> pool4
I1031 14:58:21.392107 101376 net.cpp:210] Setting up pool4
I1031 14:58:21.396571 101376 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 14:58:21.398941 101376 net.cpp:225] Memory required for data: 32303300
I1031 14:58:21.401382 101376 layer_factory.hpp:114] Creating layer fc1
I1031 14:58:21.476115 101376 net.cpp:160] Creating Layer fc1
I1031 14:58:21.484565 101376 net.cpp:596] fc1 <- pool4
I1031 14:58:21.485301 101376 net.cpp:570] fc1 -> fc1
I1031 14:58:22.333644 101376 net.cpp:210] Setting up fc1
I1031 14:58:22.346273 101376 net.cpp:217] Top shape: 1 1024 (1024)
I1031 14:58:22.354694 101376 net.cpp:225] Memory required for data: 32307396
I1031 14:58:22.364308 101376 layer_factory.hpp:114] Creating layer dropout5
I1031 14:58:22.370707 101376 net.cpp:160] Creating Layer dropout5
I1031 14:58:22.376338 101376 net.cpp:596] dropout5 <- fc1
I1031 14:58:22.385169 101376 net.cpp:570] dropout5 -> drop5
I1031 14:58:22.387593 101376 net.cpp:210] Setting up dropout5
I1031 14:58:22.387898 101376 net.cpp:217] Top shape: 1 1024 (1024)
I1031 14:58:22.388157 101376 net.cpp:225] Memory required for data: 32311492
I1031 14:58:22.392475 101376 layer_factory.hpp:114] Creating layer fc2
I1031 14:58:22.398850 101376 net.cpp:160] Creating Layer fc2
I1031 14:58:22.407317 101376 net.cpp:596] fc2 <- drop5
I1031 14:58:22.411885 101376 net.cpp:570] fc2 -> fc2
I1031 14:58:22.438143 101376 net.cpp:210] Setting up fc2
I1031 14:58:22.447665 101376 net.cpp:217] Top shape: 1 2 (2)
I1031 14:58:22.455466 101376 net.cpp:225] Memory required for data: 32311500
I1031 14:58:22.459049 101376 layer_factory.hpp:114] Creating layer loss
I1031 14:58:22.494501 101376 net.cpp:160] Creating Layer loss
I1031 14:58:22.499166 101376 net.cpp:596] loss <- fc2
I1031 14:58:22.508091 101376 net.cpp:596] loss <- label
I1031 14:58:22.572954 101376 net.cpp:570] loss -> (automatic)
I1031 14:58:22.624588 101376 layer_factory.hpp:114] Creating layer loss
I1031 14:58:23.011013 101376 net.cpp:210] Setting up loss
I1031 14:58:23.019315 101376 net.cpp:217] Top shape: (1)
I1031 14:58:23.030107 101376 net.cpp:220]     with loss weight 1
I1031 14:58:23.173851 101376 net.cpp:225] Memory required for data: 32311504
I1031 14:58:23.224450 101376 net.cpp:287] loss needs backward computation.
I1031 14:58:23.329237 101376 net.cpp:287] fc2 needs backward computation.
I1031 14:58:23.339494 101376 net.cpp:287] dropout5 needs backward computation.
I1031 14:58:23.354054 101376 net.cpp:287] fc1 needs backward computation.
I1031 14:58:23.369058 101376 net.cpp:287] pool4 needs backward computation.
I1031 14:58:23.371721 101376 net.cpp:287] dropout4 needs backward computation.
I1031 14:58:23.377979 101376 net.cpp:287] relu4 needs backward computation.
I1031 14:58:23.393229 101376 net.cpp:287] conv4 needs backward computation.
I1031 14:58:23.409955 101376 net.cpp:287] pool3 needs backward computation.
I1031 14:58:23.423408 101376 net.cpp:287] dropout3 needs backward computation.
I1031 14:58:23.428179 101376 net.cpp:287] relu3 needs backward computation.
I1031 14:58:23.428503 101376 net.cpp:287] conv3 needs backward computation.
I1031 14:58:23.428829 101376 net.cpp:287] pool2 needs backward computation.
I1031 14:58:23.429097 101376 net.cpp:287] dropout2 needs backward computation.
I1031 14:58:23.429292 101376 net.cpp:287] relu2 needs backward computation.
I1031 14:58:23.429479 101376 net.cpp:287] conv2 needs backward computation.
I1031 14:58:23.429669 101376 net.cpp:287] pool1 needs backward computation.
I1031 14:58:23.429857 101376 net.cpp:287] dropout1 needs backward computation.
I1031 14:58:23.430047 101376 net.cpp:287] relu1 needs backward computation.
I1031 14:58:23.430230 101376 net.cpp:287] conv1 needs backward computation.
I1031 14:58:23.450098 101376 net.cpp:289] data does not need backward computation.
I1031 14:58:23.506652 101376 net.cpp:345] Network initialization done.
I1031 14:58:23.685497 101376 caffe.cpp:452] Performing Forward
I1031 14:58:35.497951 101376 caffe.cpp:457] Initial loss: 37.1846
I1031 14:58:35.618932 101376 caffe.cpp:459] Performing Backward
I1031 14:58:39.033478 101376 caffe.cpp:468] *** Benchmark begins ***
I1031 14:58:39.051527 101376 caffe.cpp:469] Testing for 1 iterations.
I1031 14:58:39.210258 101376 caffe.cpp:485] Profiling Layer: conv4 backward
I1031 14:58:40.866598 101376 caffe.cpp:512] Iteration: 1 forward-backward time: 1653 ms.
I1031 14:58:40.960269 101376 caffe.cpp:519] Average time per layer: 
I1031 14:58:40.986093 101376 caffe.cpp:522]       data	forward: 47.576 ms.
I1031 14:58:41.048254 101376 caffe.cpp:526]       data	backward: 7.514 ms.
I1031 14:58:41.074074 101376 caffe.cpp:522]      conv1	forward: 95.067 ms.
I1031 14:58:41.101292 101376 caffe.cpp:526]      conv1	backward: 42.591 ms.
I1031 14:58:41.115939 101376 caffe.cpp:522]      relu1	forward: 22.439 ms.
I1031 14:58:41.120146 101376 caffe.cpp:526]      relu1	backward: 59.237 ms.
I1031 14:58:41.124850 101376 caffe.cpp:522]   dropout1	forward: 86.473 ms.
I1031 14:58:41.131402 101376 caffe.cpp:526]   dropout1	backward: 66.757 ms.
I1031 14:58:41.135810 101376 caffe.cpp:522]      pool1	forward: 125.67 ms.
I1031 14:58:41.141026 101376 caffe.cpp:526]      pool1	backward: 109.713 ms.
I1031 14:58:41.149907 101376 caffe.cpp:522]      conv2	forward: 68.257 ms.
I1031 14:58:41.155501 101376 caffe.cpp:526]      conv2	backward: 14.305 ms.
I1031 14:58:41.156922 101376 caffe.cpp:522]      relu2	forward: 16.528 ms.
I1031 14:58:41.157302 101376 caffe.cpp:526]      relu2	backward: 6.873 ms.
I1031 14:58:41.157516 101376 caffe.cpp:522]   dropout2	forward: 53.934 ms.
I1031 14:58:41.157748 101376 caffe.cpp:526]   dropout2	backward: 7.807 ms.
I1031 14:58:41.157944 101376 caffe.cpp:522]      pool2	forward: 29.368 ms.
I1031 14:58:41.158179 101376 caffe.cpp:526]      pool2	backward: 25.756 ms.
I1031 14:58:41.158395 101376 caffe.cpp:522]      conv3	forward: 69.315 ms.
I1031 14:58:41.158658 101376 caffe.cpp:526]      conv3	backward: 2.566 ms.
I1031 14:58:41.158958 101376 caffe.cpp:522]      relu3	forward: 21.51 ms.
I1031 14:58:41.159199 101376 caffe.cpp:526]      relu3	backward: 1.343 ms.
I1031 14:58:41.159477 101376 caffe.cpp:522]   dropout3	forward: 50.937 ms.
I1031 14:58:41.159716 101376 caffe.cpp:526]   dropout3	backward: 1.916 ms.
I1031 14:58:41.159912 101376 caffe.cpp:522]      pool3	forward: 21.262 ms.
I1031 14:58:41.160112 101376 caffe.cpp:526]      pool3	backward: 5.997 ms.
I1031 14:58:41.160307 101376 caffe.cpp:522]      conv4	forward: 61.424 ms.
I1031 14:58:41.160508 101376 caffe.cpp:526]      conv4	backward: 21.161 ms.
I1031 14:58:41.160711 101376 caffe.cpp:522]      relu4	forward: 21.2 ms.
I1031 14:58:41.160910 101376 caffe.cpp:526]      relu4	backward: 8.126 ms.
I1031 14:58:41.161936 101376 caffe.cpp:522]   dropout4	forward: 48.622 ms.
I1031 14:58:41.162252 101376 caffe.cpp:526]   dropout4	backward: 12.501 ms.
I1031 14:58:41.162461 101376 caffe.cpp:522]      pool4	forward: 20.902 ms.
I1031 14:58:41.162664 101376 caffe.cpp:526]      pool4	backward: 1.157 ms.
I1031 14:58:41.162861 101376 caffe.cpp:522]        fc1	forward: 43.071 ms.
I1031 14:58:41.163064 101376 caffe.cpp:526]        fc1	backward: 38.393 ms.
I1031 14:58:41.163265 101376 caffe.cpp:522]   dropout5	forward: 37.675 ms.
I1031 14:58:41.163501 101376 caffe.cpp:526]   dropout5	backward: 22.156 ms.
I1031 14:58:41.163703 101376 caffe.cpp:522]        fc2	forward: 27.017 ms.
I1031 14:58:41.163904 101376 caffe.cpp:526]        fc2	backward: 0.207 ms.
I1031 14:58:41.164103 101376 caffe.cpp:522]       loss	forward: 88.445 ms.
I1031 14:58:41.164304 101376 caffe.cpp:526]       loss	backward: 42.946 ms.
I1031 14:58:41.170045 101376 caffe.cpp:532] Average Forward pass: 1116.57 ms.
I1031 14:58:41.183256 101376 caffe.cpp:535] Average Backward pass: 508.366 ms.
I1031 14:58:41.194782 101376 caffe.cpp:537] Average Forward-Backward: 2070 ms.
I1031 14:58:41.210332 101376 caffe.cpp:540] Total Time: 2070 ms.
I1031 14:58:41.223126 101376 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 0
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 5309568
elements_fp_double_1 = 0
elements_fp_double_2 = 0
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 84953088
--->Total double-precision FLOPs = 0
--->Total FLOPs = 84953088
mem-read-1 = 904096
mem-read-2 = 138
mem-read-4 = 9908878
mem-read-8 = 10029468
mem-read-16 = 0
mem-read-32 = 140
mem-read-64 = 374669
mem-write-1 = 202
mem-write-2 = 68
mem-write-4 = 3524
mem-write-8 = 961942
mem-write-16 = 4
mem-write-32 = 4
mem-write-64 = 173765
--->Total Bytes read = 144758924
--->Total Bytes written = 18831122
--->Total Bytes = 163590046
