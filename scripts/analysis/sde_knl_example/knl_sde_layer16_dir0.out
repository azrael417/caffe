sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer16_dir0.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=16 -prof_forward_direction=0
I1031 15:14:44.083922 102008 caffe.cpp:444] Use CPU.
I1031 15:15:02.089108 102008 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 15:15:02.157099 102008 cpu_info.cpp:455] Total number of sockets: 1
I1031 15:15:02.169531 102008 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 15:15:02.182025 102008 cpu_info.cpp:461] Total number of processors: 256
I1031 15:15:02.199918 102008 cpu_info.cpp:464] GPU is used: no
I1031 15:15:02.209784 102008 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 15:15:02.218948 102008 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 15:15:02.231477 102008 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 15:15:11.454629 102008 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 15:15:12.187538 102008 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 15:15:14.733016 102008 layer_factory.hpp:114] Creating layer data
I1031 15:15:14.895195 102008 net.cpp:160] Creating Layer data
I1031 15:15:14.947602 102008 net.cpp:570] data -> data
I1031 15:15:15.448581 102008 net.cpp:570] data -> label
I1031 15:15:22.914615 102008 net.cpp:210] Setting up data
I1031 15:15:22.999184 102008 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 15:15:23.106761 102008 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 15:15:23.116159 102008 net.cpp:225] Memory required for data: 184516
I1031 15:15:23.197904 102008 layer_factory.hpp:114] Creating layer conv1
I1031 15:15:23.555275 102008 net.cpp:160] Creating Layer conv1
I1031 15:15:23.609069 102008 net.cpp:596] conv1 <- data
I1031 15:15:23.740968 102008 net.cpp:570] conv1 -> conv1
I1031 15:16:01.526234 102008 net.cpp:210] Setting up conv1
I1031 15:16:01.604133 102008 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 15:16:01.615200 102008 net.cpp:225] Memory required for data: 7805124
I1031 15:16:01.940932 102008 layer_factory.hpp:114] Creating layer relu1
I1031 15:16:02.099268 102008 net.cpp:160] Creating Layer relu1
I1031 15:16:02.105335 102008 net.cpp:596] relu1 <- conv1
I1031 15:16:02.140229 102008 net.cpp:557] relu1 -> conv1 (in-place)
I1031 15:16:02.370435 102008 net.cpp:210] Setting up relu1
I1031 15:16:02.374071 102008 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 15:16:02.374528 102008 net.cpp:225] Memory required for data: 15425732
I1031 15:16:02.374788 102008 layer_factory.hpp:114] Creating layer dropout1
I1031 15:16:02.410616 102008 net.cpp:160] Creating Layer dropout1
I1031 15:16:02.410987 102008 net.cpp:596] dropout1 <- conv1
I1031 15:16:02.414036 102008 net.cpp:570] dropout1 -> drop1
I1031 15:16:02.530169 102008 net.cpp:210] Setting up dropout1
I1031 15:16:02.544104 102008 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 15:16:02.544504 102008 net.cpp:225] Memory required for data: 23046340
I1031 15:16:02.544842 102008 layer_factory.hpp:114] Creating layer pool1
I1031 15:16:02.654209 102008 net.cpp:160] Creating Layer pool1
I1031 15:16:02.655024 102008 net.cpp:596] pool1 <- drop1
I1031 15:16:02.655424 102008 net.cpp:570] pool1 -> pool1
I1031 15:16:03.078155 102008 net.cpp:210] Setting up pool1
I1031 15:16:03.083719 102008 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 15:16:03.084133 102008 net.cpp:225] Memory required for data: 24951492
I1031 15:16:03.084492 102008 layer_factory.hpp:114] Creating layer conv2
I1031 15:16:03.149651 102008 net.cpp:160] Creating Layer conv2
I1031 15:16:03.154247 102008 net.cpp:596] conv2 <- pool1
I1031 15:16:03.170315 102008 net.cpp:570] conv2 -> conv2
I1031 15:16:10.070019 102008 net.cpp:210] Setting up conv2
I1031 15:16:10.074867 102008 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 15:16:10.092895 102008 net.cpp:225] Memory required for data: 26733764
I1031 15:16:10.160396 102008 layer_factory.hpp:114] Creating layer relu2
I1031 15:16:10.173954 102008 net.cpp:160] Creating Layer relu2
I1031 15:16:10.182373 102008 net.cpp:596] relu2 <- conv2
I1031 15:16:10.190907 102008 net.cpp:557] relu2 -> conv2 (in-place)
I1031 15:16:10.205579 102008 net.cpp:210] Setting up relu2
I1031 15:16:10.212549 102008 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 15:16:10.219040 102008 net.cpp:225] Memory required for data: 28516036
I1031 15:16:10.227284 102008 layer_factory.hpp:114] Creating layer dropout2
I1031 15:16:10.239231 102008 net.cpp:160] Creating Layer dropout2
I1031 15:16:10.249523 102008 net.cpp:596] dropout2 <- conv2
I1031 15:16:10.251567 102008 net.cpp:570] dropout2 -> drop2
I1031 15:16:10.262486 102008 net.cpp:210] Setting up dropout2
I1031 15:16:10.264750 102008 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 15:16:10.273890 102008 net.cpp:225] Memory required for data: 30298308
I1031 15:16:10.278301 102008 layer_factory.hpp:114] Creating layer pool2
I1031 15:16:10.280719 102008 net.cpp:160] Creating Layer pool2
I1031 15:16:10.281005 102008 net.cpp:596] pool2 <- drop2
I1031 15:16:10.281276 102008 net.cpp:570] pool2 -> pool2
I1031 15:16:10.290011 102008 net.cpp:210] Setting up pool2
I1031 15:16:10.294430 102008 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 15:16:10.294822 102008 net.cpp:225] Memory required for data: 30759108
I1031 15:16:10.311461 102008 layer_factory.hpp:114] Creating layer conv3
I1031 15:16:10.317963 102008 net.cpp:160] Creating Layer conv3
I1031 15:16:10.322031 102008 net.cpp:596] conv3 <- pool2
I1031 15:16:10.328696 102008 net.cpp:570] conv3 -> conv3
I1031 15:16:10.984073 102008 net.cpp:210] Setting up conv3
I1031 15:16:10.990892 102008 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 15:16:11.001341 102008 net.cpp:225] Memory required for data: 31160516
I1031 15:16:11.016444 102008 layer_factory.hpp:114] Creating layer relu3
I1031 15:16:11.022899 102008 net.cpp:160] Creating Layer relu3
I1031 15:16:11.030899 102008 net.cpp:596] relu3 <- conv3
I1031 15:16:11.037878 102008 net.cpp:557] relu3 -> conv3 (in-place)
I1031 15:16:11.041082 102008 net.cpp:210] Setting up relu3
I1031 15:16:11.049679 102008 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 15:16:11.057767 102008 net.cpp:225] Memory required for data: 31561924
I1031 15:16:11.064949 102008 layer_factory.hpp:114] Creating layer dropout3
I1031 15:16:11.066814 102008 net.cpp:160] Creating Layer dropout3
I1031 15:16:11.067122 102008 net.cpp:596] dropout3 <- conv3
I1031 15:16:11.067447 102008 net.cpp:570] dropout3 -> drop3
I1031 15:16:11.070660 102008 net.cpp:210] Setting up dropout3
I1031 15:16:11.070946 102008 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 15:16:11.075062 102008 net.cpp:225] Memory required for data: 31963332
I1031 15:16:11.081590 102008 layer_factory.hpp:114] Creating layer pool3
I1031 15:16:11.094002 102008 net.cpp:160] Creating Layer pool3
I1031 15:16:11.095960 102008 net.cpp:596] pool3 <- drop3
I1031 15:16:11.103273 102008 net.cpp:570] pool3 -> pool3
I1031 15:16:11.112157 102008 net.cpp:210] Setting up pool3
I1031 15:16:11.118146 102008 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 15:16:11.122668 102008 net.cpp:225] Memory required for data: 32063684
I1031 15:16:11.125063 102008 layer_factory.hpp:114] Creating layer conv4
I1031 15:16:11.127563 102008 net.cpp:160] Creating Layer conv4
I1031 15:16:11.127863 102008 net.cpp:596] conv4 <- pool3
I1031 15:16:11.128152 102008 net.cpp:570] conv4 -> conv4
I1031 15:16:11.498716 102008 net.cpp:210] Setting up conv4
I1031 15:16:11.501819 102008 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 15:16:11.505924 102008 net.cpp:225] Memory required for data: 32137412
I1031 15:16:11.514714 102008 layer_factory.hpp:114] Creating layer relu4
I1031 15:16:11.517232 102008 net.cpp:160] Creating Layer relu4
I1031 15:16:11.521623 102008 net.cpp:596] relu4 <- conv4
I1031 15:16:11.530429 102008 net.cpp:557] relu4 -> conv4 (in-place)
I1031 15:16:11.534888 102008 net.cpp:210] Setting up relu4
I1031 15:16:11.543114 102008 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 15:16:11.545511 102008 net.cpp:225] Memory required for data: 32211140
I1031 15:16:11.551342 102008 layer_factory.hpp:114] Creating layer dropout4
I1031 15:16:11.554383 102008 net.cpp:160] Creating Layer dropout4
I1031 15:16:11.560813 102008 net.cpp:596] dropout4 <- conv4
I1031 15:16:11.566890 102008 net.cpp:570] dropout4 -> drop4
I1031 15:16:11.575620 102008 net.cpp:210] Setting up dropout4
I1031 15:16:11.579958 102008 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 15:16:11.588400 102008 net.cpp:225] Memory required for data: 32284868
I1031 15:16:11.590306 102008 layer_factory.hpp:114] Creating layer pool4
I1031 15:16:11.593175 102008 net.cpp:160] Creating Layer pool4
I1031 15:16:11.595669 102008 net.cpp:596] pool4 <- drop4
I1031 15:16:11.600435 102008 net.cpp:570] pool4 -> pool4
I1031 15:16:11.622838 102008 net.cpp:210] Setting up pool4
I1031 15:16:11.629783 102008 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 15:16:11.638489 102008 net.cpp:225] Memory required for data: 32303300
I1031 15:16:11.643085 102008 layer_factory.hpp:114] Creating layer fc1
I1031 15:16:11.720679 102008 net.cpp:160] Creating Layer fc1
I1031 15:16:11.725142 102008 net.cpp:596] fc1 <- pool4
I1031 15:16:11.731261 102008 net.cpp:570] fc1 -> fc1
I1031 15:16:12.582224 102008 net.cpp:210] Setting up fc1
I1031 15:16:12.585306 102008 net.cpp:217] Top shape: 1 1024 (1024)
I1031 15:16:12.587842 102008 net.cpp:225] Memory required for data: 32307396
I1031 15:16:12.602432 102008 layer_factory.hpp:114] Creating layer dropout5
I1031 15:16:12.615226 102008 net.cpp:160] Creating Layer dropout5
I1031 15:16:12.619487 102008 net.cpp:596] dropout5 <- fc1
I1031 15:16:12.635746 102008 net.cpp:570] dropout5 -> drop5
I1031 15:16:12.644068 102008 net.cpp:210] Setting up dropout5
I1031 15:16:12.646270 102008 net.cpp:217] Top shape: 1 1024 (1024)
I1031 15:16:12.648799 102008 net.cpp:225] Memory required for data: 32311492
I1031 15:16:12.652809 102008 layer_factory.hpp:114] Creating layer fc2
I1031 15:16:12.659749 102008 net.cpp:160] Creating Layer fc2
I1031 15:16:12.665419 102008 net.cpp:596] fc2 <- drop5
I1031 15:16:12.674409 102008 net.cpp:570] fc2 -> fc2
I1031 15:16:12.700099 102008 net.cpp:210] Setting up fc2
I1031 15:16:12.707489 102008 net.cpp:217] Top shape: 1 2 (2)
I1031 15:16:12.713768 102008 net.cpp:225] Memory required for data: 32311500
I1031 15:16:12.720510 102008 layer_factory.hpp:114] Creating layer loss
I1031 15:16:12.756929 102008 net.cpp:160] Creating Layer loss
I1031 15:16:12.763501 102008 net.cpp:596] loss <- fc2
I1031 15:16:12.772125 102008 net.cpp:596] loss <- label
I1031 15:16:12.840598 102008 net.cpp:570] loss -> (automatic)
I1031 15:16:12.896226 102008 layer_factory.hpp:114] Creating layer loss
I1031 15:16:13.323009 102008 net.cpp:210] Setting up loss
I1031 15:16:13.331769 102008 net.cpp:217] Top shape: (1)
I1031 15:16:13.348372 102008 net.cpp:220]     with loss weight 1
I1031 15:16:13.491830 102008 net.cpp:225] Memory required for data: 32311504
I1031 15:16:13.547690 102008 net.cpp:287] loss needs backward computation.
I1031 15:16:13.651940 102008 net.cpp:287] fc2 needs backward computation.
I1031 15:16:13.667695 102008 net.cpp:287] dropout5 needs backward computation.
I1031 15:16:13.670444 102008 net.cpp:287] fc1 needs backward computation.
I1031 15:16:13.671267 102008 net.cpp:287] pool4 needs backward computation.
I1031 15:16:13.671613 102008 net.cpp:287] dropout4 needs backward computation.
I1031 15:16:13.671814 102008 net.cpp:287] relu4 needs backward computation.
I1031 15:16:13.681936 102008 net.cpp:287] conv4 needs backward computation.
I1031 15:16:13.699352 102008 net.cpp:287] pool3 needs backward computation.
I1031 15:16:13.713208 102008 net.cpp:287] dropout3 needs backward computation.
I1031 15:16:13.718073 102008 net.cpp:287] relu3 needs backward computation.
I1031 15:16:13.718403 102008 net.cpp:287] conv3 needs backward computation.
I1031 15:16:13.718724 102008 net.cpp:287] pool2 needs backward computation.
I1031 15:16:13.719029 102008 net.cpp:287] dropout2 needs backward computation.
I1031 15:16:13.719324 102008 net.cpp:287] relu2 needs backward computation.
I1031 15:16:13.719563 102008 net.cpp:287] conv2 needs backward computation.
I1031 15:16:13.719759 102008 net.cpp:287] pool1 needs backward computation.
I1031 15:16:13.719952 102008 net.cpp:287] dropout1 needs backward computation.
I1031 15:16:13.720144 102008 net.cpp:287] relu1 needs backward computation.
I1031 15:16:13.720329 102008 net.cpp:287] conv1 needs backward computation.
I1031 15:16:13.741482 102008 net.cpp:289] data does not need backward computation.
I1031 15:16:13.801102 102008 net.cpp:345] Network initialization done.
I1031 15:16:13.997740 102008 caffe.cpp:452] Performing Forward
I1031 15:16:25.999935 102008 caffe.cpp:457] Initial loss: 87.3365
I1031 15:16:26.134961 102008 caffe.cpp:459] Performing Backward
I1031 15:16:29.597476 102008 caffe.cpp:468] *** Benchmark begins ***
I1031 15:16:29.609136 102008 caffe.cpp:469] Testing for 1 iterations.
I1031 15:16:29.762734 102008 caffe.cpp:485] Profiling Layer: pool4 backward
I1031 15:16:31.420083 102008 caffe.cpp:512] Iteration: 1 forward-backward time: 1646 ms.
I1031 15:16:31.518851 102008 caffe.cpp:519] Average time per layer: 
I1031 15:16:31.534564 102008 caffe.cpp:522]       data	forward: 47.406 ms.
I1031 15:16:31.614907 102008 caffe.cpp:526]       data	backward: 22.386 ms.
I1031 15:16:31.640468 102008 caffe.cpp:522]      conv1	forward: 71.81 ms.
I1031 15:16:31.647121 102008 caffe.cpp:526]      conv1	backward: 34.136 ms.
I1031 15:16:31.650205 102008 caffe.cpp:522]      relu1	forward: 30.219 ms.
I1031 15:16:31.651118 102008 caffe.cpp:526]      relu1	backward: 61.419 ms.
I1031 15:16:31.651484 102008 caffe.cpp:522]   dropout1	forward: 86.109 ms.
I1031 15:16:31.651708 102008 caffe.cpp:526]   dropout1	backward: 63.579 ms.
I1031 15:16:31.651921 102008 caffe.cpp:522]      pool1	forward: 129.918 ms.
I1031 15:16:31.652720 102008 caffe.cpp:526]      pool1	backward: 112.814 ms.
I1031 15:16:31.655442 102008 caffe.cpp:522]      conv2	forward: 66.459 ms.
I1031 15:16:31.655724 102008 caffe.cpp:526]      conv2	backward: 14.226 ms.
I1031 15:16:31.655941 102008 caffe.cpp:522]      relu2	forward: 22.724 ms.
I1031 15:16:31.656152 102008 caffe.cpp:526]      relu2	backward: 7.058 ms.
I1031 15:16:31.656358 102008 caffe.cpp:522]   dropout2	forward: 52.14 ms.
I1031 15:16:31.656565 102008 caffe.cpp:526]   dropout2	backward: 7.768 ms.
I1031 15:16:31.656767 102008 caffe.cpp:522]      pool2	forward: 31.66 ms.
I1031 15:16:31.656972 102008 caffe.cpp:526]      pool2	backward: 27.063 ms.
I1031 15:16:31.657178 102008 caffe.cpp:522]      conv3	forward: 46.416 ms.
I1031 15:16:31.657383 102008 caffe.cpp:526]      conv3	backward: 2.531 ms.
I1031 15:16:31.657583 102008 caffe.cpp:522]      relu3	forward: 26.514 ms.
I1031 15:16:31.657788 102008 caffe.cpp:526]      relu3	backward: 1.37 ms.
I1031 15:16:31.657986 102008 caffe.cpp:522]   dropout3	forward: 59.587 ms.
I1031 15:16:31.658190 102008 caffe.cpp:526]   dropout3	backward: 1.898 ms.
I1031 15:16:31.658432 102008 caffe.cpp:522]      pool3	forward: 11.762 ms.
I1031 15:16:31.658673 102008 caffe.cpp:526]      pool3	backward: 5.887 ms.
I1031 15:16:31.659029 102008 caffe.cpp:522]      conv4	forward: 53.553 ms.
I1031 15:16:31.659317 102008 caffe.cpp:526]      conv4	backward: 10.498 ms.
I1031 15:16:31.661842 102008 caffe.cpp:522]      relu4	forward: 20.794 ms.
I1031 15:16:31.662129 102008 caffe.cpp:526]      relu4	backward: 5.343 ms.
I1031 15:16:31.662482 102008 caffe.cpp:522]   dropout4	forward: 60.02 ms.
I1031 15:16:31.662829 102008 caffe.cpp:526]   dropout4	backward: 16.142 ms.
I1031 15:16:31.663116 102008 caffe.cpp:522]      pool4	forward: 17.66 ms.
I1031 15:16:31.663353 102008 caffe.cpp:526]      pool4	backward: 12.375 ms.
I1031 15:16:31.663604 102008 caffe.cpp:522]        fc1	forward: 47.219 ms.
I1031 15:16:31.663815 102008 caffe.cpp:526]        fc1	backward: 15.631 ms.
I1031 15:16:31.664022 102008 caffe.cpp:522]   dropout5	forward: 41.369 ms.
I1031 15:16:31.664227 102008 caffe.cpp:526]   dropout5	backward: 21.465 ms.
I1031 15:16:31.664433 102008 caffe.cpp:522]        fc2	forward: 18.596 ms.
I1031 15:16:31.664636 102008 caffe.cpp:526]        fc2	backward: 0.212 ms.
I1031 15:16:31.664839 102008 caffe.cpp:522]       loss	forward: 110.387 ms.
I1031 15:16:31.665045 102008 caffe.cpp:526]       loss	backward: 51.444 ms.
I1031 15:16:31.670866 102008 caffe.cpp:532] Average Forward pass: 1114.17 ms.
I1031 15:16:31.684669 102008 caffe.cpp:535] Average Backward pass: 504.797 ms.
I1031 15:16:31.696274 102008 caffe.cpp:537] Average Forward-Backward: 2019 ms.
I1031 15:16:31.712105 102008 caffe.cpp:540] Total Time: 2019 ms.
I1031 15:16:31.725703 102008 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 4608
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 0
elements_fp_double_1 = 0
elements_fp_double_2 = 0
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 4608
--->Total double-precision FLOPs = 0
--->Total FLOPs = 4608
mem-read-1 = 473861
mem-read-2 = 71
mem-read-4 = 3807479
mem-read-8 = 5271166
mem-read-16 = 0
mem-read-32 = 2
mem-read-64 = 3
mem-write-1 = 234
mem-write-2 = 34
mem-write-4 = 25094
mem-write-8 = 486071
mem-write-16 = 0
mem-write-32 = 1
mem-write-64 = 1153
--->Total Bytes read = 57873503
--->Total Bytes written = 4063070
--->Total Bytes = 61936573
