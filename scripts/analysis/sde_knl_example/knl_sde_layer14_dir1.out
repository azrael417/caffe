sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer14_dir1.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=14 -prof_forward_direction=1
I1031 12:54:06.769574 96932 caffe.cpp:444] Use CPU.
I1031 12:54:24.641516 96932 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 12:54:24.704478 96932 cpu_info.cpp:455] Total number of sockets: 1
I1031 12:54:24.717445 96932 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 12:54:24.729490 96932 cpu_info.cpp:461] Total number of processors: 256
I1031 12:54:24.746518 96932 cpu_info.cpp:464] GPU is used: no
I1031 12:54:24.755868 96932 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 12:54:24.764632 96932 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 12:54:24.777401 96932 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 12:54:33.893867 96932 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 12:54:34.575825 96932 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 12:54:37.000377 96932 layer_factory.hpp:114] Creating layer data
I1031 12:54:37.161368 96932 net.cpp:160] Creating Layer data
I1031 12:54:37.212646 96932 net.cpp:570] data -> data
I1031 12:54:37.709089 96932 net.cpp:570] data -> label
I1031 12:54:45.286192 96932 net.cpp:210] Setting up data
I1031 12:54:45.373564 96932 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 12:54:45.480911 96932 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 12:54:45.488325 96932 net.cpp:225] Memory required for data: 184516
I1031 12:54:45.566931 96932 layer_factory.hpp:114] Creating layer conv1
I1031 12:54:45.918678 96932 net.cpp:160] Creating Layer conv1
I1031 12:54:45.972486 96932 net.cpp:596] conv1 <- data
I1031 12:54:46.098714 96932 net.cpp:570] conv1 -> conv1
I1031 12:55:23.512228 96932 net.cpp:210] Setting up conv1
I1031 12:55:23.581804 96932 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:55:23.597920 96932 net.cpp:225] Memory required for data: 7805124
I1031 12:55:23.923473 96932 layer_factory.hpp:114] Creating layer relu1
I1031 12:55:24.058939 96932 net.cpp:160] Creating Layer relu1
I1031 12:55:24.064126 96932 net.cpp:596] relu1 <- conv1
I1031 12:55:24.099231 96932 net.cpp:557] relu1 -> conv1 (in-place)
I1031 12:55:24.314775 96932 net.cpp:210] Setting up relu1
I1031 12:55:24.317423 96932 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:55:24.317792 96932 net.cpp:225] Memory required for data: 15425732
I1031 12:55:24.318004 96932 layer_factory.hpp:114] Creating layer dropout1
I1031 12:55:24.351143 96932 net.cpp:160] Creating Layer dropout1
I1031 12:55:24.351516 96932 net.cpp:596] dropout1 <- conv1
I1031 12:55:24.354187 96932 net.cpp:570] dropout1 -> drop1
I1031 12:55:24.470686 96932 net.cpp:210] Setting up dropout1
I1031 12:55:24.484848 96932 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:55:24.485265 96932 net.cpp:225] Memory required for data: 23046340
I1031 12:55:24.485647 96932 layer_factory.hpp:114] Creating layer pool1
I1031 12:55:24.591017 96932 net.cpp:160] Creating Layer pool1
I1031 12:55:24.591341 96932 net.cpp:596] pool1 <- drop1
I1031 12:55:24.591743 96932 net.cpp:570] pool1 -> pool1
I1031 12:55:24.999717 96932 net.cpp:210] Setting up pool1
I1031 12:55:25.004621 96932 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 12:55:25.005026 96932 net.cpp:225] Memory required for data: 24951492
I1031 12:55:25.005331 96932 layer_factory.hpp:114] Creating layer conv2
I1031 12:55:25.066663 96932 net.cpp:160] Creating Layer conv2
I1031 12:55:25.071066 96932 net.cpp:596] conv2 <- pool1
I1031 12:55:25.086335 96932 net.cpp:570] conv2 -> conv2
I1031 12:55:31.827698 96932 net.cpp:210] Setting up conv2
I1031 12:55:31.828142 96932 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:55:31.834594 96932 net.cpp:225] Memory required for data: 26733764
I1031 12:55:31.896741 96932 layer_factory.hpp:114] Creating layer relu2
I1031 12:55:31.897797 96932 net.cpp:160] Creating Layer relu2
I1031 12:55:31.898133 96932 net.cpp:596] relu2 <- conv2
I1031 12:55:31.898427 96932 net.cpp:557] relu2 -> conv2 (in-place)
I1031 12:55:31.898921 96932 net.cpp:210] Setting up relu2
I1031 12:55:31.899225 96932 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:55:31.899554 96932 net.cpp:225] Memory required for data: 28516036
I1031 12:55:31.899776 96932 layer_factory.hpp:114] Creating layer dropout2
I1031 12:55:31.900027 96932 net.cpp:160] Creating Layer dropout2
I1031 12:55:31.900244 96932 net.cpp:596] dropout2 <- conv2
I1031 12:55:31.900593 96932 net.cpp:570] dropout2 -> drop2
I1031 12:55:31.900986 96932 net.cpp:210] Setting up dropout2
I1031 12:55:31.901229 96932 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:55:31.901481 96932 net.cpp:225] Memory required for data: 30298308
I1031 12:55:31.901705 96932 layer_factory.hpp:114] Creating layer pool2
I1031 12:55:31.902005 96932 net.cpp:160] Creating Layer pool2
I1031 12:55:31.902233 96932 net.cpp:596] pool2 <- drop2
I1031 12:55:31.902480 96932 net.cpp:570] pool2 -> pool2
I1031 12:55:31.902909 96932 net.cpp:210] Setting up pool2
I1031 12:55:31.903167 96932 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 12:55:31.903473 96932 net.cpp:225] Memory required for data: 30759108
I1031 12:55:31.903774 96932 layer_factory.hpp:114] Creating layer conv3
I1031 12:55:31.904199 96932 net.cpp:160] Creating Layer conv3
I1031 12:55:31.904443 96932 net.cpp:596] conv3 <- pool2
I1031 12:55:31.904712 96932 net.cpp:570] conv3 -> conv3
I1031 12:55:32.541702 96932 net.cpp:210] Setting up conv3
I1031 12:55:32.550784 96932 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:55:32.555727 96932 net.cpp:225] Memory required for data: 31160516
I1031 12:55:32.576175 96932 layer_factory.hpp:114] Creating layer relu3
I1031 12:55:32.581790 96932 net.cpp:160] Creating Layer relu3
I1031 12:55:32.587792 96932 net.cpp:596] relu3 <- conv3
I1031 12:55:32.594471 96932 net.cpp:557] relu3 -> conv3 (in-place)
I1031 12:55:32.601441 96932 net.cpp:210] Setting up relu3
I1031 12:55:32.604233 96932 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:55:32.614768 96932 net.cpp:225] Memory required for data: 31561924
I1031 12:55:32.623710 96932 layer_factory.hpp:114] Creating layer dropout3
I1031 12:55:32.629413 96932 net.cpp:160] Creating Layer dropout3
I1031 12:55:32.632051 96932 net.cpp:596] dropout3 <- conv3
I1031 12:55:32.636442 96932 net.cpp:570] dropout3 -> drop3
I1031 12:55:32.642671 96932 net.cpp:210] Setting up dropout3
I1031 12:55:32.644953 96932 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:55:32.653291 96932 net.cpp:225] Memory required for data: 31963332
I1031 12:55:32.655843 96932 layer_factory.hpp:114] Creating layer pool3
I1031 12:55:32.666092 96932 net.cpp:160] Creating Layer pool3
I1031 12:55:32.670254 96932 net.cpp:596] pool3 <- drop3
I1031 12:55:32.674862 96932 net.cpp:570] pool3 -> pool3
I1031 12:55:32.681298 96932 net.cpp:210] Setting up pool3
I1031 12:55:32.687875 96932 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 12:55:32.693665 96932 net.cpp:225] Memory required for data: 32063684
I1031 12:55:32.698513 96932 layer_factory.hpp:114] Creating layer conv4
I1031 12:55:32.700857 96932 net.cpp:160] Creating Layer conv4
I1031 12:55:32.703616 96932 net.cpp:596] conv4 <- pool3
I1031 12:55:32.710209 96932 net.cpp:570] conv4 -> conv4
I1031 12:55:33.083758 96932 net.cpp:210] Setting up conv4
I1031 12:55:33.090708 96932 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:55:33.097831 96932 net.cpp:225] Memory required for data: 32137412
I1031 12:55:33.105888 96932 layer_factory.hpp:114] Creating layer relu4
I1031 12:55:33.113312 96932 net.cpp:160] Creating Layer relu4
I1031 12:55:33.123922 96932 net.cpp:596] relu4 <- conv4
I1031 12:55:33.132616 96932 net.cpp:557] relu4 -> conv4 (in-place)
I1031 12:55:33.139024 96932 net.cpp:210] Setting up relu4
I1031 12:55:33.145841 96932 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:55:33.148257 96932 net.cpp:225] Memory required for data: 32211140
I1031 12:55:33.162955 96932 layer_factory.hpp:114] Creating layer dropout4
I1031 12:55:33.167441 96932 net.cpp:160] Creating Layer dropout4
I1031 12:55:33.174003 96932 net.cpp:596] dropout4 <- conv4
I1031 12:55:33.180512 96932 net.cpp:570] dropout4 -> drop4
I1031 12:55:33.182993 96932 net.cpp:210] Setting up dropout4
I1031 12:55:33.183275 96932 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:55:33.187816 96932 net.cpp:225] Memory required for data: 32284868
I1031 12:55:33.190326 96932 layer_factory.hpp:114] Creating layer pool4
I1031 12:55:33.194718 96932 net.cpp:160] Creating Layer pool4
I1031 12:55:33.204905 96932 net.cpp:596] pool4 <- drop4
I1031 12:55:33.213412 96932 net.cpp:570] pool4 -> pool4
I1031 12:55:33.230531 96932 net.cpp:210] Setting up pool4
I1031 12:55:33.235258 96932 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 12:55:33.247880 96932 net.cpp:225] Memory required for data: 32303300
I1031 12:55:33.254470 96932 layer_factory.hpp:114] Creating layer fc1
I1031 12:55:33.323946 96932 net.cpp:160] Creating Layer fc1
I1031 12:55:33.333082 96932 net.cpp:596] fc1 <- pool4
I1031 12:55:33.343961 96932 net.cpp:570] fc1 -> fc1
I1031 12:55:34.220697 96932 net.cpp:210] Setting up fc1
I1031 12:55:34.221024 96932 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:55:34.221385 96932 net.cpp:225] Memory required for data: 32307396
I1031 12:55:34.226635 96932 layer_factory.hpp:114] Creating layer dropout5
I1031 12:55:34.227039 96932 net.cpp:160] Creating Layer dropout5
I1031 12:55:34.227273 96932 net.cpp:596] dropout5 <- fc1
I1031 12:55:34.227622 96932 net.cpp:570] dropout5 -> drop5
I1031 12:55:34.228073 96932 net.cpp:210] Setting up dropout5
I1031 12:55:34.228329 96932 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:55:34.228584 96932 net.cpp:225] Memory required for data: 32311492
I1031 12:55:34.228793 96932 layer_factory.hpp:114] Creating layer fc2
I1031 12:55:34.229095 96932 net.cpp:160] Creating Layer fc2
I1031 12:55:34.229321 96932 net.cpp:596] fc2 <- drop5
I1031 12:55:34.229589 96932 net.cpp:570] fc2 -> fc2
I1031 12:55:34.240972 96932 net.cpp:210] Setting up fc2
I1031 12:55:34.242429 96932 net.cpp:217] Top shape: 1 2 (2)
I1031 12:55:34.244971 96932 net.cpp:225] Memory required for data: 32311500
I1031 12:55:34.245429 96932 layer_factory.hpp:114] Creating layer loss
I1031 12:55:34.274756 96932 net.cpp:160] Creating Layer loss
I1031 12:55:34.275102 96932 net.cpp:596] loss <- fc2
I1031 12:55:34.276114 96932 net.cpp:596] loss <- label
I1031 12:55:34.331701 96932 net.cpp:570] loss -> (automatic)
I1031 12:55:34.376368 96932 layer_factory.hpp:114] Creating layer loss
I1031 12:55:34.792299 96932 net.cpp:210] Setting up loss
I1031 12:55:34.801286 96932 net.cpp:217] Top shape: (1)
I1031 12:55:34.813796 96932 net.cpp:220]     with loss weight 1
I1031 12:55:34.965559 96932 net.cpp:225] Memory required for data: 32311504
I1031 12:55:35.020887 96932 net.cpp:287] loss needs backward computation.
I1031 12:55:35.130340 96932 net.cpp:287] fc2 needs backward computation.
I1031 12:55:35.144613 96932 net.cpp:287] dropout5 needs backward computation.
I1031 12:55:35.158934 96932 net.cpp:287] fc1 needs backward computation.
I1031 12:55:35.168031 96932 net.cpp:287] pool4 needs backward computation.
I1031 12:55:35.168546 96932 net.cpp:287] dropout4 needs backward computation.
I1031 12:55:35.168747 96932 net.cpp:287] relu4 needs backward computation.
I1031 12:55:35.178539 96932 net.cpp:287] conv4 needs backward computation.
I1031 12:55:35.195660 96932 net.cpp:287] pool3 needs backward computation.
I1031 12:55:35.209414 96932 net.cpp:287] dropout3 needs backward computation.
I1031 12:55:35.214308 96932 net.cpp:287] relu3 needs backward computation.
I1031 12:55:35.214648 96932 net.cpp:287] conv3 needs backward computation.
I1031 12:55:35.214987 96932 net.cpp:287] pool2 needs backward computation.
I1031 12:55:35.215261 96932 net.cpp:287] dropout2 needs backward computation.
I1031 12:55:35.215504 96932 net.cpp:287] relu2 needs backward computation.
I1031 12:55:35.215697 96932 net.cpp:287] conv2 needs backward computation.
I1031 12:55:35.215893 96932 net.cpp:287] pool1 needs backward computation.
I1031 12:55:35.216084 96932 net.cpp:287] dropout1 needs backward computation.
I1031 12:55:35.216275 96932 net.cpp:287] relu1 needs backward computation.
I1031 12:55:35.216461 96932 net.cpp:287] conv1 needs backward computation.
I1031 12:55:35.236754 96932 net.cpp:289] data does not need backward computation.
I1031 12:55:35.296003 96932 net.cpp:345] Network initialization done.
I1031 12:55:35.480512 96932 caffe.cpp:452] Performing Forward
I1031 12:55:47.503744 96932 caffe.cpp:457] Initial loss: 87.3365
I1031 12:55:47.632238 96932 caffe.cpp:459] Performing Backward
I1031 12:55:51.049446 96932 caffe.cpp:468] *** Benchmark begins ***
I1031 12:55:51.067570 96932 caffe.cpp:469] Testing for 1 iterations.
I1031 12:55:51.222779 96932 caffe.cpp:482] Profiling Layer: relu4 forward
I1031 12:55:52.856180 96932 caffe.cpp:512] Iteration: 1 forward-backward time: 1628 ms.
I1031 12:55:52.948314 96932 caffe.cpp:519] Average time per layer: 
I1031 12:55:52.968472 96932 caffe.cpp:522]       data	forward: 49.065 ms.
I1031 12:55:53.050037 96932 caffe.cpp:526]       data	backward: 5.731 ms.
I1031 12:55:53.071213 96932 caffe.cpp:522]      conv1	forward: 60.973 ms.
I1031 12:55:53.074126 96932 caffe.cpp:526]      conv1	backward: 39.284 ms.
I1031 12:55:53.074602 96932 caffe.cpp:522]      relu1	forward: 20.122 ms.
I1031 12:55:53.077852 96932 caffe.cpp:526]      relu1	backward: 57.506 ms.
I1031 12:55:53.080734 96932 caffe.cpp:522]   dropout1	forward: 86.782 ms.
I1031 12:55:53.081481 96932 caffe.cpp:526]   dropout1	backward: 86.105 ms.
I1031 12:55:53.082202 96932 caffe.cpp:522]      pool1	forward: 130.596 ms.
I1031 12:55:53.083544 96932 caffe.cpp:526]      pool1	backward: 108.289 ms.
I1031 12:55:53.084012 96932 caffe.cpp:522]      conv2	forward: 72.819 ms.
I1031 12:55:53.084308 96932 caffe.cpp:526]      conv2	backward: 14.105 ms.
I1031 12:55:53.084630 96932 caffe.cpp:522]      relu2	forward: 15.132 ms.
I1031 12:55:53.084938 96932 caffe.cpp:526]      relu2	backward: 7.06 ms.
I1031 12:55:53.085247 96932 caffe.cpp:522]   dropout2	forward: 48.601 ms.
I1031 12:55:53.086494 96932 caffe.cpp:526]   dropout2	backward: 7.793 ms.
I1031 12:55:53.086801 96932 caffe.cpp:522]      pool2	forward: 29.955 ms.
I1031 12:55:53.087147 96932 caffe.cpp:526]      pool2	backward: 25.466 ms.
I1031 12:55:53.087499 96932 caffe.cpp:522]      conv3	forward: 54.401 ms.
I1031 12:55:53.087831 96932 caffe.cpp:526]      conv3	backward: 2.567 ms.
I1031 12:55:53.088138 96932 caffe.cpp:522]      relu3	forward: 13.755 ms.
I1031 12:55:53.088444 96932 caffe.cpp:526]      relu3	backward: 1.334 ms.
I1031 12:55:53.088752 96932 caffe.cpp:522]   dropout3	forward: 63.85 ms.
I1031 12:55:53.089059 96932 caffe.cpp:526]   dropout3	backward: 1.913 ms.
I1031 12:55:53.089366 96932 caffe.cpp:522]      pool3	forward: 13.1 ms.
I1031 12:55:53.089673 96932 caffe.cpp:526]      pool3	backward: 5.872 ms.
I1031 12:55:53.089980 96932 caffe.cpp:522]      conv4	forward: 70.674 ms.
I1031 12:55:53.090302 96932 caffe.cpp:526]      conv4	backward: 9.986 ms.
I1031 12:55:53.091434 96932 caffe.cpp:522]      relu4	forward: 38.44 ms.
I1031 12:55:53.091753 96932 caffe.cpp:526]      relu4	backward: 8.63 ms.
I1031 12:55:53.092078 96932 caffe.cpp:522]   dropout4	forward: 63.744 ms.
I1031 12:55:53.092368 96932 caffe.cpp:526]   dropout4	backward: 13.19 ms.
I1031 12:55:53.092645 96932 caffe.cpp:522]      pool4	forward: 22.018 ms.
I1031 12:55:53.092957 96932 caffe.cpp:526]      pool4	backward: 1.251 ms.
I1031 12:55:53.093267 96932 caffe.cpp:522]        fc1	forward: 45.043 ms.
I1031 12:55:53.093597 96932 caffe.cpp:526]        fc1	backward: 13.228 ms.
I1031 12:55:53.093933 96932 caffe.cpp:522]   dropout5	forward: 48.659 ms.
I1031 12:55:53.094182 96932 caffe.cpp:526]   dropout5	backward: 8.435 ms.
I1031 12:55:53.094466 96932 caffe.cpp:522]        fc2	forward: 25.531 ms.
I1031 12:55:53.094754 96932 caffe.cpp:526]        fc2	backward: 0.296 ms.
I1031 12:55:53.095059 96932 caffe.cpp:522]       loss	forward: 85.614 ms.
I1031 12:55:53.095396 96932 caffe.cpp:526]       loss	backward: 53.597 ms.
I1031 12:55:53.102057 96932 caffe.cpp:532] Average Forward pass: 1119.59 ms.
I1031 12:55:53.119639 96932 caffe.cpp:535] Average Backward pass: 481.302 ms.
I1031 12:55:53.132505 96932 caffe.cpp:537] Average Forward-Backward: 1987 ms.
I1031 12:55:53.150116 96932 caffe.cpp:540] Total Time: 1987 ms.
I1031 12:55:53.164531 96932 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 1
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 1152
elements_fp_double_1 = 0
elements_fp_double_2 = 1152
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 18433
--->Total double-precision FLOPs = 2304
--->Total FLOPs = 20737
mem-read-1 = 28236
mem-read-2 = 34
mem-read-4 = 226756
mem-read-8 = 315229
mem-read-16 = 0
mem-read-32 = 1
mem-read-64 = 2321
mem-write-1 = 50
mem-write-2 = 17
mem-write-4 = 560
mem-write-8 = 30643
mem-write-16 = 0
mem-write-32 = 1
mem-write-64 = 1153
--->Total Bytes read = 3605736
--->Total Bytes written = 321292
--->Total Bytes = 3927028
