sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer10_dir1.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=10 -prof_forward_direction=1
I1031 12:29:05.510684 96059 caffe.cpp:444] Use CPU.
I1031 12:29:23.327724 96059 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 12:29:23.390275 96059 cpu_info.cpp:455] Total number of sockets: 1
I1031 12:29:23.403247 96059 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 12:29:23.415128 96059 cpu_info.cpp:461] Total number of processors: 256
I1031 12:29:23.432046 96059 cpu_info.cpp:464] GPU is used: no
I1031 12:29:23.441509 96059 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 12:29:23.450428 96059 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 12:29:23.463002 96059 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 12:29:32.630746 96059 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 12:29:33.323695 96059 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 12:29:35.733803 96059 layer_factory.hpp:114] Creating layer data
I1031 12:29:35.896157 96059 net.cpp:160] Creating Layer data
I1031 12:29:35.947662 96059 net.cpp:570] data -> data
I1031 12:29:36.442315 96059 net.cpp:570] data -> label
I1031 12:29:43.915163 96059 net.cpp:210] Setting up data
I1031 12:29:44.001735 96059 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 12:29:44.112929 96059 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 12:29:44.120396 96059 net.cpp:225] Memory required for data: 184516
I1031 12:29:44.199198 96059 layer_factory.hpp:114] Creating layer conv1
I1031 12:29:44.546730 96059 net.cpp:160] Creating Layer conv1
I1031 12:29:44.600625 96059 net.cpp:596] conv1 <- data
I1031 12:29:44.727502 96059 net.cpp:570] conv1 -> conv1
I1031 12:30:22.098532 96059 net.cpp:210] Setting up conv1
I1031 12:30:22.165252 96059 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:30:22.182636 96059 net.cpp:225] Memory required for data: 7805124
I1031 12:30:22.508291 96059 layer_factory.hpp:114] Creating layer relu1
I1031 12:30:22.652312 96059 net.cpp:160] Creating Layer relu1
I1031 12:30:22.657946 96059 net.cpp:596] relu1 <- conv1
I1031 12:30:22.692994 96059 net.cpp:557] relu1 -> conv1 (in-place)
I1031 12:30:22.911489 96059 net.cpp:210] Setting up relu1
I1031 12:30:22.914126 96059 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:30:22.914494 96059 net.cpp:225] Memory required for data: 15425732
I1031 12:30:22.914710 96059 layer_factory.hpp:114] Creating layer dropout1
I1031 12:30:22.947091 96059 net.cpp:160] Creating Layer dropout1
I1031 12:30:22.947489 96059 net.cpp:596] dropout1 <- conv1
I1031 12:30:22.950347 96059 net.cpp:570] dropout1 -> drop1
I1031 12:30:23.068990 96059 net.cpp:210] Setting up dropout1
I1031 12:30:23.082510 96059 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:30:23.082904 96059 net.cpp:225] Memory required for data: 23046340
I1031 12:30:23.083241 96059 layer_factory.hpp:114] Creating layer pool1
I1031 12:30:23.183452 96059 net.cpp:160] Creating Layer pool1
I1031 12:30:23.183881 96059 net.cpp:596] pool1 <- drop1
I1031 12:30:23.184232 96059 net.cpp:570] pool1 -> pool1
I1031 12:30:23.590853 96059 net.cpp:210] Setting up pool1
I1031 12:30:23.595906 96059 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 12:30:23.596293 96059 net.cpp:225] Memory required for data: 24951492
I1031 12:30:23.596612 96059 layer_factory.hpp:114] Creating layer conv2
I1031 12:30:23.658094 96059 net.cpp:160] Creating Layer conv2
I1031 12:30:23.662502 96059 net.cpp:596] conv2 <- pool1
I1031 12:30:23.679468 96059 net.cpp:570] conv2 -> conv2
I1031 12:30:30.538462 96059 net.cpp:210] Setting up conv2
I1031 12:30:30.547432 96059 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:30:30.556524 96059 net.cpp:225] Memory required for data: 26733764
I1031 12:30:30.624794 96059 layer_factory.hpp:114] Creating layer relu2
I1031 12:30:30.636085 96059 net.cpp:160] Creating Layer relu2
I1031 12:30:30.646667 96059 net.cpp:596] relu2 <- conv2
I1031 12:30:30.655014 96059 net.cpp:557] relu2 -> conv2 (in-place)
I1031 12:30:30.667201 96059 net.cpp:210] Setting up relu2
I1031 12:30:30.669512 96059 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:30:30.671663 96059 net.cpp:225] Memory required for data: 28516036
I1031 12:30:30.688328 96059 layer_factory.hpp:114] Creating layer dropout2
I1031 12:30:30.693774 96059 net.cpp:160] Creating Layer dropout2
I1031 12:30:30.706554 96059 net.cpp:596] dropout2 <- conv2
I1031 12:30:30.715148 96059 net.cpp:570] dropout2 -> drop2
I1031 12:30:30.723923 96059 net.cpp:210] Setting up dropout2
I1031 12:30:30.725733 96059 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:30:30.731271 96059 net.cpp:225] Memory required for data: 30298308
I1031 12:30:30.739730 96059 layer_factory.hpp:114] Creating layer pool2
I1031 12:30:30.743847 96059 net.cpp:160] Creating Layer pool2
I1031 12:30:30.745990 96059 net.cpp:596] pool2 <- drop2
I1031 12:30:30.750879 96059 net.cpp:570] pool2 -> pool2
I1031 12:30:30.759273 96059 net.cpp:210] Setting up pool2
I1031 12:30:30.761502 96059 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 12:30:30.763931 96059 net.cpp:225] Memory required for data: 30759108
I1031 12:30:30.770540 96059 layer_factory.hpp:114] Creating layer conv3
I1031 12:30:30.779129 96059 net.cpp:160] Creating Layer conv3
I1031 12:30:30.785715 96059 net.cpp:596] conv3 <- pool2
I1031 12:30:30.790089 96059 net.cpp:570] conv3 -> conv3
I1031 12:30:31.405583 96059 net.cpp:210] Setting up conv3
I1031 12:30:31.416404 96059 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:30:31.422893 96059 net.cpp:225] Memory required for data: 31160516
I1031 12:30:31.443168 96059 layer_factory.hpp:114] Creating layer relu3
I1031 12:30:31.445493 96059 net.cpp:160] Creating Layer relu3
I1031 12:30:31.447868 96059 net.cpp:596] relu3 <- conv3
I1031 12:30:31.458426 96059 net.cpp:557] relu3 -> conv3 (in-place)
I1031 12:30:31.462965 96059 net.cpp:210] Setting up relu3
I1031 12:30:31.473865 96059 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:30:31.482379 96059 net.cpp:225] Memory required for data: 31561924
I1031 12:30:31.486773 96059 layer_factory.hpp:114] Creating layer dropout3
I1031 12:30:31.499178 96059 net.cpp:160] Creating Layer dropout3
I1031 12:30:31.507848 96059 net.cpp:596] dropout3 <- conv3
I1031 12:30:31.518465 96059 net.cpp:570] dropout3 -> drop3
I1031 12:30:31.528996 96059 net.cpp:210] Setting up dropout3
I1031 12:30:31.531297 96059 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:30:31.537751 96059 net.cpp:225] Memory required for data: 31963332
I1031 12:30:31.546383 96059 layer_factory.hpp:114] Creating layer pool3
I1031 12:30:31.548960 96059 net.cpp:160] Creating Layer pool3
I1031 12:30:31.549247 96059 net.cpp:596] pool3 <- drop3
I1031 12:30:31.549525 96059 net.cpp:570] pool3 -> pool3
I1031 12:30:31.558184 96059 net.cpp:210] Setting up pool3
I1031 12:30:31.562707 96059 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 12:30:31.568760 96059 net.cpp:225] Memory required for data: 32063684
I1031 12:30:31.577443 96059 layer_factory.hpp:114] Creating layer conv4
I1031 12:30:31.584133 96059 net.cpp:160] Creating Layer conv4
I1031 12:30:31.586835 96059 net.cpp:596] conv4 <- pool3
I1031 12:30:31.589438 96059 net.cpp:570] conv4 -> conv4
I1031 12:30:31.947516 96059 net.cpp:210] Setting up conv4
I1031 12:30:31.955715 96059 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:30:31.966332 96059 net.cpp:225] Memory required for data: 32137412
I1031 12:30:31.974874 96059 layer_factory.hpp:114] Creating layer relu4
I1031 12:30:31.981310 96059 net.cpp:160] Creating Layer relu4
I1031 12:30:31.995565 96059 net.cpp:596] relu4 <- conv4
I1031 12:30:32.006042 96059 net.cpp:557] relu4 -> conv4 (in-place)
I1031 12:30:32.014802 96059 net.cpp:210] Setting up relu4
I1031 12:30:32.021037 96059 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:30:32.029433 96059 net.cpp:225] Memory required for data: 32211140
I1031 12:30:32.031065 96059 layer_factory.hpp:114] Creating layer dropout4
I1031 12:30:32.036046 96059 net.cpp:160] Creating Layer dropout4
I1031 12:30:32.044482 96059 net.cpp:596] dropout4 <- conv4
I1031 12:30:32.053288 96059 net.cpp:570] dropout4 -> drop4
I1031 12:30:32.063738 96059 net.cpp:210] Setting up dropout4
I1031 12:30:32.066047 96059 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:30:32.072494 96059 net.cpp:225] Memory required for data: 32284868
I1031 12:30:32.093780 96059 layer_factory.hpp:114] Creating layer pool4
I1031 12:30:32.100311 96059 net.cpp:160] Creating Layer pool4
I1031 12:30:32.100682 96059 net.cpp:596] pool4 <- drop4
I1031 12:30:32.108991 96059 net.cpp:570] pool4 -> pool4
I1031 12:30:32.125879 96059 net.cpp:210] Setting up pool4
I1031 12:30:32.130338 96059 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 12:30:32.140924 96059 net.cpp:225] Memory required for data: 32303300
I1031 12:30:32.147721 96059 layer_factory.hpp:114] Creating layer fc1
I1031 12:30:32.215690 96059 net.cpp:160] Creating Layer fc1
I1031 12:30:32.220095 96059 net.cpp:596] fc1 <- pool4
I1031 12:30:32.224624 96059 net.cpp:570] fc1 -> fc1
I1031 12:30:33.099344 96059 net.cpp:210] Setting up fc1
I1031 12:30:33.103731 96059 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:30:33.111726 96059 net.cpp:225] Memory required for data: 32307396
I1031 12:30:33.121137 96059 layer_factory.hpp:114] Creating layer dropout5
I1031 12:30:33.127895 96059 net.cpp:160] Creating Layer dropout5
I1031 12:30:33.140702 96059 net.cpp:596] dropout5 <- fc1
I1031 12:30:33.149466 96059 net.cpp:570] dropout5 -> drop5
I1031 12:30:33.158084 96059 net.cpp:210] Setting up dropout5
I1031 12:30:33.160374 96059 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:30:33.166921 96059 net.cpp:225] Memory required for data: 32311492
I1031 12:30:33.176061 96059 layer_factory.hpp:114] Creating layer fc2
I1031 12:30:33.182615 96059 net.cpp:160] Creating Layer fc2
I1031 12:30:33.184854 96059 net.cpp:596] fc2 <- drop5
I1031 12:30:33.194118 96059 net.cpp:570] fc2 -> fc2
I1031 12:30:33.224273 96059 net.cpp:210] Setting up fc2
I1031 12:30:33.235919 96059 net.cpp:217] Top shape: 1 2 (2)
I1031 12:30:33.246289 96059 net.cpp:225] Memory required for data: 32311500
I1031 12:30:33.254871 96059 layer_factory.hpp:114] Creating layer loss
I1031 12:30:33.290992 96059 net.cpp:160] Creating Layer loss
I1031 12:30:33.299469 96059 net.cpp:596] loss <- fc2
I1031 12:30:33.308619 96059 net.cpp:596] loss <- label
I1031 12:30:33.370385 96059 net.cpp:570] loss -> (automatic)
I1031 12:30:33.416255 96059 layer_factory.hpp:114] Creating layer loss
I1031 12:30:33.811869 96059 net.cpp:210] Setting up loss
I1031 12:30:33.820969 96059 net.cpp:217] Top shape: (1)
I1031 12:30:33.835278 96059 net.cpp:220]     with loss weight 1
I1031 12:30:33.974349 96059 net.cpp:225] Memory required for data: 32311504
I1031 12:30:34.028914 96059 net.cpp:287] loss needs backward computation.
I1031 12:30:34.133816 96059 net.cpp:287] fc2 needs backward computation.
I1031 12:30:34.149780 96059 net.cpp:287] dropout5 needs backward computation.
I1031 12:30:34.162076 96059 net.cpp:287] fc1 needs backward computation.
I1031 12:30:34.187526 96059 net.cpp:287] pool4 needs backward computation.
I1031 12:30:34.193616 96059 net.cpp:287] dropout4 needs backward computation.
I1031 12:30:34.193964 96059 net.cpp:287] relu4 needs backward computation.
I1031 12:30:34.212174 96059 net.cpp:287] conv4 needs backward computation.
I1031 12:30:34.230007 96059 net.cpp:287] pool3 needs backward computation.
I1031 12:30:34.243923 96059 net.cpp:287] dropout3 needs backward computation.
I1031 12:30:34.248796 96059 net.cpp:287] relu3 needs backward computation.
I1031 12:30:34.249125 96059 net.cpp:287] conv3 needs backward computation.
I1031 12:30:34.249444 96059 net.cpp:287] pool2 needs backward computation.
I1031 12:30:34.249717 96059 net.cpp:287] dropout2 needs backward computation.
I1031 12:30:34.249914 96059 net.cpp:287] relu2 needs backward computation.
I1031 12:30:34.250102 96059 net.cpp:287] conv2 needs backward computation.
I1031 12:30:34.250293 96059 net.cpp:287] pool1 needs backward computation.
I1031 12:30:34.250483 96059 net.cpp:287] dropout1 needs backward computation.
I1031 12:30:34.250671 96059 net.cpp:287] relu1 needs backward computation.
I1031 12:30:34.250856 96059 net.cpp:287] conv1 needs backward computation.
I1031 12:30:34.270972 96059 net.cpp:289] data does not need backward computation.
I1031 12:30:34.328313 96059 net.cpp:345] Network initialization done.
I1031 12:30:34.514235 96059 caffe.cpp:452] Performing Forward
I1031 12:30:46.442745 96059 caffe.cpp:457] Initial loss: 87.3365
I1031 12:30:46.565242 96059 caffe.cpp:459] Performing Backward
I1031 12:30:49.709534 96059 caffe.cpp:468] *** Benchmark begins ***
I1031 12:30:49.725472 96059 caffe.cpp:469] Testing for 1 iterations.
I1031 12:30:49.916931 96059 caffe.cpp:482] Profiling Layer: relu3 forward
I1031 12:30:51.588798 96059 caffe.cpp:512] Iteration: 1 forward-backward time: 1666 ms.
I1031 12:30:51.684921 96059 caffe.cpp:519] Average time per layer: 
I1031 12:30:51.703176 96059 caffe.cpp:522]       data	forward: 51.734 ms.
I1031 12:30:51.770859 96059 caffe.cpp:526]       data	backward: 7.687 ms.
I1031 12:30:51.795123 96059 caffe.cpp:522]      conv1	forward: 65.513 ms.
I1031 12:30:51.799227 96059 caffe.cpp:526]      conv1	backward: 38.228 ms.
I1031 12:30:51.804612 96059 caffe.cpp:522]      relu1	forward: 28.786 ms.
I1031 12:30:51.809710 96059 caffe.cpp:526]      relu1	backward: 59.863 ms.
I1031 12:30:51.827193 96059 caffe.cpp:522]   dropout1	forward: 79.131 ms.
I1031 12:30:51.834105 96059 caffe.cpp:526]   dropout1	backward: 62.978 ms.
I1031 12:30:51.841722 96059 caffe.cpp:522]      pool1	forward: 128.688 ms.
I1031 12:30:51.848831 96059 caffe.cpp:526]      pool1	backward: 109.76 ms.
I1031 12:30:51.853997 96059 caffe.cpp:522]      conv2	forward: 78.128 ms.
I1031 12:30:51.859067 96059 caffe.cpp:526]      conv2	backward: 14.338 ms.
I1031 12:30:51.865617 96059 caffe.cpp:522]      relu2	forward: 15.091 ms.
I1031 12:30:51.869699 96059 caffe.cpp:526]      relu2	backward: 6.842 ms.
I1031 12:30:51.870121 96059 caffe.cpp:522]   dropout2	forward: 65.812 ms.
I1031 12:30:51.877511 96059 caffe.cpp:526]   dropout2	backward: 7.784 ms.
I1031 12:30:51.879274 96059 caffe.cpp:522]      pool2	forward: 29.984 ms.
I1031 12:30:51.885582 96059 caffe.cpp:526]      pool2	backward: 25.798 ms.
I1031 12:30:51.892349 96059 caffe.cpp:522]      conv3	forward: 69.841 ms.
I1031 12:30:51.897706 96059 caffe.cpp:526]      conv3	backward: 2.515 ms.
I1031 12:30:51.901705 96059 caffe.cpp:522]      relu3	forward: 33.72 ms.
I1031 12:30:51.907227 96059 caffe.cpp:526]      relu3	backward: 1.36 ms.
I1031 12:30:51.911685 96059 caffe.cpp:522]   dropout3	forward: 66.73 ms.
I1031 12:30:51.918268 96059 caffe.cpp:526]   dropout3	backward: 1.885 ms.
I1031 12:30:51.924948 96059 caffe.cpp:522]      pool3	forward: 16.857 ms.
I1031 12:30:51.929095 96059 caffe.cpp:526]      pool3	backward: 5.972 ms.
I1031 12:30:51.931658 96059 caffe.cpp:522]      conv4	forward: 60.562 ms.
I1031 12:30:51.937201 96059 caffe.cpp:526]      conv4	backward: 10.08 ms.
I1031 12:30:51.947456 96059 caffe.cpp:522]      relu4	forward: 15.743 ms.
I1031 12:30:51.947898 96059 caffe.cpp:526]      relu4	backward: 5.843 ms.
I1031 12:30:51.953258 96059 caffe.cpp:522]   dropout4	forward: 74.466 ms.
I1031 12:30:51.957240 96059 caffe.cpp:526]   dropout4	backward: 16.061 ms.
I1031 12:30:51.958520 96059 caffe.cpp:522]      pool4	forward: 23.177 ms.
I1031 12:30:51.964079 96059 caffe.cpp:526]      pool4	backward: 1.233 ms.
I1031 12:30:51.970644 96059 caffe.cpp:522]        fc1	forward: 53.542 ms.
I1031 12:30:51.975234 96059 caffe.cpp:526]        fc1	backward: 11.63 ms.
I1031 12:30:51.976866 96059 caffe.cpp:522]   dropout5	forward: 35.58 ms.
I1031 12:30:51.977126 96059 caffe.cpp:526]   dropout5	backward: 0.099 ms.
I1031 12:30:51.994608 96059 caffe.cpp:522]        fc2	forward: 20.091 ms.
I1031 12:30:51.995040 96059 caffe.cpp:526]        fc2	backward: 0.238 ms.
I1031 12:30:51.995448 96059 caffe.cpp:522]       loss	forward: 119.314 ms.
I1031 12:30:51.995702 96059 caffe.cpp:526]       loss	backward: 44.703 ms.
I1031 12:30:52.001587 96059 caffe.cpp:532] Average Forward pass: 1192.46 ms.
I1031 12:30:52.015436 96059 caffe.cpp:535] Average Backward pass: 444.48 ms.
I1031 12:30:52.026947 96059 caffe.cpp:537] Average Forward-Backward: 2197 ms.
I1031 12:30:52.042791 96059 caffe.cpp:540] Total Time: 2197 ms.
I1031 12:30:52.055690 96059 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 1
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 6272
elements_fp_double_1 = 0
elements_fp_double_2 = 6272
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 100353
--->Total double-precision FLOPs = 12544
--->Total FLOPs = 112897
mem-read-1 = 25932
mem-read-2 = 34
mem-read-4 = 208315
mem-read-8 = 289856
mem-read-16 = 0
mem-read-32 = 1
mem-read-64 = 12561
mem-write-1 = 50
mem-write-2 = 17
mem-write-4 = 564
mem-write-8 = 28332
mem-write-16 = 0
mem-write-32 = 1
mem-write-64 = 6273
--->Total Bytes read = 3982044
--->Total Bytes written = 630500
--->Total Bytes = 4612544
