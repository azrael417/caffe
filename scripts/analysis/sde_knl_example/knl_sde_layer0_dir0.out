sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer0_dir0.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=0 -prof_forward_direction=0
I1031 13:40:57.920464 98616 caffe.cpp:444] Use CPU.
I1031 13:41:15.880149 98616 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 13:41:15.943614 98616 cpu_info.cpp:455] Total number of sockets: 1
I1031 13:41:15.956758 98616 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 13:41:15.968825 98616 cpu_info.cpp:461] Total number of processors: 256
I1031 13:41:15.986184 98616 cpu_info.cpp:464] GPU is used: no
I1031 13:41:15.995659 98616 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 13:41:16.006419 98616 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 13:41:16.038379 98616 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 13:41:25.219074 98616 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 13:41:25.910341 98616 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 13:41:28.361403 98616 layer_factory.hpp:114] Creating layer data
I1031 13:41:28.521009 98616 net.cpp:160] Creating Layer data
I1031 13:41:28.572820 98616 net.cpp:570] data -> data
I1031 13:41:29.083703 98616 net.cpp:570] data -> label
I1031 13:41:36.624538 98616 net.cpp:210] Setting up data
I1031 13:41:36.709466 98616 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 13:41:36.817061 98616 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 13:41:36.824512 98616 net.cpp:225] Memory required for data: 184516
I1031 13:41:36.907692 98616 layer_factory.hpp:114] Creating layer conv1
I1031 13:41:37.261109 98616 net.cpp:160] Creating Layer conv1
I1031 13:41:37.314703 98616 net.cpp:596] conv1 <- data
I1031 13:41:37.442982 98616 net.cpp:570] conv1 -> conv1
I1031 13:42:14.974726 98616 net.cpp:210] Setting up conv1
I1031 13:42:15.046888 98616 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:42:15.056995 98616 net.cpp:225] Memory required for data: 7805124
I1031 13:42:15.379072 98616 layer_factory.hpp:114] Creating layer relu1
I1031 13:42:15.509325 98616 net.cpp:160] Creating Layer relu1
I1031 13:42:15.514281 98616 net.cpp:596] relu1 <- conv1
I1031 13:42:15.549422 98616 net.cpp:557] relu1 -> conv1 (in-place)
I1031 13:42:15.766487 98616 net.cpp:210] Setting up relu1
I1031 13:42:15.769153 98616 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:42:15.769520 98616 net.cpp:225] Memory required for data: 15425732
I1031 13:42:15.769742 98616 layer_factory.hpp:114] Creating layer dropout1
I1031 13:42:15.802497 98616 net.cpp:160] Creating Layer dropout1
I1031 13:42:15.802830 98616 net.cpp:596] dropout1 <- conv1
I1031 13:42:15.805652 98616 net.cpp:570] dropout1 -> drop1
I1031 13:42:15.930178 98616 net.cpp:210] Setting up dropout1
I1031 13:42:15.944720 98616 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:42:15.945152 98616 net.cpp:225] Memory required for data: 23046340
I1031 13:42:15.945533 98616 layer_factory.hpp:114] Creating layer pool1
I1031 13:42:16.051915 98616 net.cpp:160] Creating Layer pool1
I1031 13:42:16.056540 98616 net.cpp:596] pool1 <- drop1
I1031 13:42:16.056938 98616 net.cpp:570] pool1 -> pool1
I1031 13:42:16.474892 98616 net.cpp:210] Setting up pool1
I1031 13:42:16.480474 98616 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 13:42:16.480981 98616 net.cpp:225] Memory required for data: 24951492
I1031 13:42:16.481287 98616 layer_factory.hpp:114] Creating layer conv2
I1031 13:42:16.544319 98616 net.cpp:160] Creating Layer conv2
I1031 13:42:16.548776 98616 net.cpp:596] conv2 <- pool1
I1031 13:42:16.565913 98616 net.cpp:570] conv2 -> conv2
I1031 13:42:23.436710 98616 net.cpp:210] Setting up conv2
I1031 13:42:23.439172 98616 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:42:23.453171 98616 net.cpp:225] Memory required for data: 26733764
I1031 13:42:23.520426 98616 layer_factory.hpp:114] Creating layer relu2
I1031 13:42:23.527319 98616 net.cpp:160] Creating Layer relu2
I1031 13:42:23.539788 98616 net.cpp:596] relu2 <- conv2
I1031 13:42:23.546721 98616 net.cpp:557] relu2 -> conv2 (in-place)
I1031 13:42:23.549482 98616 net.cpp:210] Setting up relu2
I1031 13:42:23.558503 98616 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:42:23.564659 98616 net.cpp:225] Memory required for data: 28516036
I1031 13:42:23.569245 98616 layer_factory.hpp:114] Creating layer dropout2
I1031 13:42:23.573498 98616 net.cpp:160] Creating Layer dropout2
I1031 13:42:23.577797 98616 net.cpp:596] dropout2 <- conv2
I1031 13:42:23.580535 98616 net.cpp:570] dropout2 -> drop2
I1031 13:42:23.582898 98616 net.cpp:210] Setting up dropout2
I1031 13:42:23.587353 98616 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:42:23.591955 98616 net.cpp:225] Memory required for data: 30298308
I1031 13:42:23.596385 98616 layer_factory.hpp:114] Creating layer pool2
I1031 13:42:23.606886 98616 net.cpp:160] Creating Layer pool2
I1031 13:42:23.611723 98616 net.cpp:596] pool2 <- drop2
I1031 13:42:23.615732 98616 net.cpp:570] pool2 -> pool2
I1031 13:42:23.621137 98616 net.cpp:210] Setting up pool2
I1031 13:42:23.622973 98616 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 13:42:23.625883 98616 net.cpp:225] Memory required for data: 30759108
I1031 13:42:23.630131 98616 layer_factory.hpp:114] Creating layer conv3
I1031 13:42:23.637266 98616 net.cpp:160] Creating Layer conv3
I1031 13:42:23.642982 98616 net.cpp:596] conv3 <- pool2
I1031 13:42:23.647799 98616 net.cpp:570] conv3 -> conv3
I1031 13:42:24.246539 98616 net.cpp:210] Setting up conv3
I1031 13:42:24.256119 98616 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:42:24.260494 98616 net.cpp:225] Memory required for data: 31160516
I1031 13:42:24.285166 98616 layer_factory.hpp:114] Creating layer relu3
I1031 13:42:24.289655 98616 net.cpp:160] Creating Layer relu3
I1031 13:42:24.296249 98616 net.cpp:596] relu3 <- conv3
I1031 13:42:24.308598 98616 net.cpp:557] relu3 -> conv3 (in-place)
I1031 13:42:24.319334 98616 net.cpp:210] Setting up relu3
I1031 13:42:24.321656 98616 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:42:24.328598 98616 net.cpp:225] Memory required for data: 31561924
I1031 13:42:24.333112 98616 layer_factory.hpp:114] Creating layer dropout3
I1031 13:42:24.335119 98616 net.cpp:160] Creating Layer dropout3
I1031 13:42:24.341131 98616 net.cpp:596] dropout3 <- conv3
I1031 13:42:24.350275 98616 net.cpp:570] dropout3 -> drop3
I1031 13:42:24.360522 98616 net.cpp:210] Setting up dropout3
I1031 13:42:24.365113 98616 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:42:24.365525 98616 net.cpp:225] Memory required for data: 31963332
I1031 13:42:24.375275 98616 layer_factory.hpp:114] Creating layer pool3
I1031 13:42:24.379809 98616 net.cpp:160] Creating Layer pool3
I1031 13:42:24.382349 98616 net.cpp:596] pool3 <- drop3
I1031 13:42:24.390944 98616 net.cpp:570] pool3 -> pool3
I1031 13:42:24.395416 98616 net.cpp:210] Setting up pool3
I1031 13:42:24.403831 98616 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 13:42:24.408360 98616 net.cpp:225] Memory required for data: 32063684
I1031 13:42:24.412694 98616 layer_factory.hpp:114] Creating layer conv4
I1031 13:42:24.417637 98616 net.cpp:160] Creating Layer conv4
I1031 13:42:24.421468 98616 net.cpp:596] conv4 <- pool3
I1031 13:42:24.424157 98616 net.cpp:570] conv4 -> conv4
I1031 13:42:24.773655 98616 net.cpp:210] Setting up conv4
I1031 13:42:24.773938 98616 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:42:24.774301 98616 net.cpp:225] Memory required for data: 32137412
I1031 13:42:24.774617 98616 layer_factory.hpp:114] Creating layer relu4
I1031 13:42:24.774899 98616 net.cpp:160] Creating Layer relu4
I1031 13:42:24.775112 98616 net.cpp:596] relu4 <- conv4
I1031 13:42:24.775408 98616 net.cpp:557] relu4 -> conv4 (in-place)
I1031 13:42:24.775871 98616 net.cpp:210] Setting up relu4
I1031 13:42:24.776134 98616 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:42:24.776384 98616 net.cpp:225] Memory required for data: 32211140
I1031 13:42:24.776592 98616 layer_factory.hpp:114] Creating layer dropout4
I1031 13:42:24.776830 98616 net.cpp:160] Creating Layer dropout4
I1031 13:42:24.777035 98616 net.cpp:596] dropout4 <- conv4
I1031 13:42:24.777285 98616 net.cpp:570] dropout4 -> drop4
I1031 13:42:24.777606 98616 net.cpp:210] Setting up dropout4
I1031 13:42:24.777920 98616 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:42:24.778194 98616 net.cpp:225] Memory required for data: 32284868
I1031 13:42:24.778401 98616 layer_factory.hpp:114] Creating layer pool4
I1031 13:42:24.778704 98616 net.cpp:160] Creating Layer pool4
I1031 13:42:24.778929 98616 net.cpp:596] pool4 <- drop4
I1031 13:42:24.779173 98616 net.cpp:570] pool4 -> pool4
I1031 13:42:24.793354 98616 net.cpp:210] Setting up pool4
I1031 13:42:24.793717 98616 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 13:42:24.796285 98616 net.cpp:225] Memory required for data: 32303300
I1031 13:42:24.796542 98616 layer_factory.hpp:114] Creating layer fc1
I1031 13:42:24.863116 98616 net.cpp:160] Creating Layer fc1
I1031 13:42:24.867565 98616 net.cpp:596] fc1 <- pool4
I1031 13:42:24.875826 98616 net.cpp:570] fc1 -> fc1
I1031 13:42:25.738801 98616 net.cpp:210] Setting up fc1
I1031 13:42:25.747479 98616 net.cpp:217] Top shape: 1 1024 (1024)
I1031 13:42:25.750023 98616 net.cpp:225] Memory required for data: 32307396
I1031 13:42:25.767138 98616 layer_factory.hpp:114] Creating layer dropout5
I1031 13:42:25.773273 98616 net.cpp:160] Creating Layer dropout5
I1031 13:42:25.775722 98616 net.cpp:596] dropout5 <- fc1
I1031 13:42:25.782241 98616 net.cpp:570] dropout5 -> drop5
I1031 13:42:25.786119 98616 net.cpp:210] Setting up dropout5
I1031 13:42:25.790891 98616 net.cpp:217] Top shape: 1 1024 (1024)
I1031 13:42:25.793911 98616 net.cpp:225] Memory required for data: 32311492
I1031 13:42:25.798430 98616 layer_factory.hpp:114] Creating layer fc2
I1031 13:42:25.803228 98616 net.cpp:160] Creating Layer fc2
I1031 13:42:25.811353 98616 net.cpp:596] fc2 <- drop5
I1031 13:42:25.819983 98616 net.cpp:570] fc2 -> fc2
I1031 13:42:25.853657 98616 net.cpp:210] Setting up fc2
I1031 13:42:25.862763 98616 net.cpp:217] Top shape: 1 2 (2)
I1031 13:42:25.871965 98616 net.cpp:225] Memory required for data: 32311500
I1031 13:42:25.876605 98616 layer_factory.hpp:114] Creating layer loss
I1031 13:42:25.906831 98616 net.cpp:160] Creating Layer loss
I1031 13:42:25.914911 98616 net.cpp:596] loss <- fc2
I1031 13:42:25.922134 98616 net.cpp:596] loss <- label
I1031 13:42:25.986140 98616 net.cpp:570] loss -> (automatic)
I1031 13:42:26.030432 98616 layer_factory.hpp:114] Creating layer loss
I1031 13:42:26.444589 98616 net.cpp:210] Setting up loss
I1031 13:42:26.449558 98616 net.cpp:217] Top shape: (1)
I1031 13:42:26.464586 98616 net.cpp:220]     with loss weight 1
I1031 13:42:26.611393 98616 net.cpp:225] Memory required for data: 32311504
I1031 13:42:26.668393 98616 net.cpp:287] loss needs backward computation.
I1031 13:42:26.786264 98616 net.cpp:287] fc2 needs backward computation.
I1031 13:42:26.801308 98616 net.cpp:287] dropout5 needs backward computation.
I1031 13:42:26.804890 98616 net.cpp:287] fc1 needs backward computation.
I1031 13:42:26.806032 98616 net.cpp:287] pool4 needs backward computation.
I1031 13:42:26.806404 98616 net.cpp:287] dropout4 needs backward computation.
I1031 13:42:26.806715 98616 net.cpp:287] relu4 needs backward computation.
I1031 13:42:26.816671 98616 net.cpp:287] conv4 needs backward computation.
I1031 13:42:26.833739 98616 net.cpp:287] pool3 needs backward computation.
I1031 13:42:26.847503 98616 net.cpp:287] dropout3 needs backward computation.
I1031 13:42:26.852416 98616 net.cpp:287] relu3 needs backward computation.
I1031 13:42:26.852747 98616 net.cpp:287] conv3 needs backward computation.
I1031 13:42:26.853096 98616 net.cpp:287] pool2 needs backward computation.
I1031 13:42:26.853368 98616 net.cpp:287] dropout2 needs backward computation.
I1031 13:42:26.853562 98616 net.cpp:287] relu2 needs backward computation.
I1031 13:42:26.853751 98616 net.cpp:287] conv2 needs backward computation.
I1031 13:42:26.853943 98616 net.cpp:287] pool1 needs backward computation.
I1031 13:42:26.854133 98616 net.cpp:287] dropout1 needs backward computation.
I1031 13:42:26.854322 98616 net.cpp:287] relu1 needs backward computation.
I1031 13:42:26.854507 98616 net.cpp:287] conv1 needs backward computation.
I1031 13:42:26.874675 98616 net.cpp:289] data does not need backward computation.
I1031 13:42:26.931921 98616 net.cpp:345] Network initialization done.
I1031 13:42:27.118314 98616 caffe.cpp:452] Performing Forward
I1031 13:42:39.158504 98616 caffe.cpp:457] Initial loss: 0
I1031 13:42:39.193364 98616 caffe.cpp:459] Performing Backward
I1031 13:42:42.703001 98616 caffe.cpp:468] *** Benchmark begins ***
I1031 13:42:42.726261 98616 caffe.cpp:469] Testing for 1 iterations.
I1031 13:42:42.882829 98616 caffe.cpp:485] Profiling Layer: data backward
I1031 13:42:44.934329 98616 caffe.cpp:512] Iteration: 1 forward-backward time: 2049 ms.
I1031 13:42:45.118299 98616 caffe.cpp:519] Average time per layer: 
I1031 13:42:45.133663 98616 caffe.cpp:522]       data	forward: 48.086 ms.
I1031 13:42:45.218113 98616 caffe.cpp:526]       data	backward: 16.597 ms.
I1031 13:42:45.240305 98616 caffe.cpp:522]      conv1	forward: 68.466 ms.
I1031 13:42:45.244976 98616 caffe.cpp:526]      conv1	backward: 37.343 ms.
I1031 13:42:45.247979 98616 caffe.cpp:522]      relu1	forward: 8.855 ms.
I1031 13:42:45.253872 98616 caffe.cpp:526]      relu1	backward: 65.243 ms.
I1031 13:42:45.254956 98616 caffe.cpp:522]   dropout1	forward: 35.907 ms.
I1031 13:42:45.256554 98616 caffe.cpp:526]   dropout1	backward: 61.373 ms.
I1031 13:42:45.264410 98616 caffe.cpp:522]      pool1	forward: 126.317 ms.
I1031 13:42:45.270166 98616 caffe.cpp:526]      pool1	backward: 134.587 ms.
I1031 13:42:45.275707 98616 caffe.cpp:522]      conv2	forward: 27.436 ms.
I1031 13:42:45.279800 98616 caffe.cpp:526]      conv2	backward: 76.015 ms.
I1031 13:42:45.284276 98616 caffe.cpp:522]      relu2	forward: 0.144 ms.
I1031 13:42:45.286082 98616 caffe.cpp:526]      relu2	backward: 38.882 ms.
I1031 13:42:45.286835 98616 caffe.cpp:522]   dropout2	forward: 52.95 ms.
I1031 13:42:45.288089 98616 caffe.cpp:526]   dropout2	backward: 28.896 ms.
I1031 13:42:45.289232 98616 caffe.cpp:522]      pool2	forward: 29.904 ms.
I1031 13:42:45.290352 98616 caffe.cpp:526]      pool2	backward: 69.501 ms.
I1031 13:42:45.292400 98616 caffe.cpp:522]      conv3	forward: 50.722 ms.
I1031 13:42:45.293283 98616 caffe.cpp:526]      conv3	backward: 66.719 ms.
I1031 13:42:45.294121 98616 caffe.cpp:522]      relu3	forward: 23.181 ms.
I1031 13:42:45.294924 98616 caffe.cpp:526]      relu3	backward: 45.886 ms.
I1031 13:42:45.295858 98616 caffe.cpp:522]   dropout3	forward: 54.095 ms.
I1031 13:42:45.296389 98616 caffe.cpp:526]   dropout3	backward: 41.081 ms.
I1031 13:42:45.297185 98616 caffe.cpp:522]      pool3	forward: 25.246 ms.
I1031 13:42:45.297979 98616 caffe.cpp:526]      pool3	backward: 73.884 ms.
I1031 13:42:45.298774 98616 caffe.cpp:522]      conv4	forward: 54.419 ms.
I1031 13:42:45.299806 98616 caffe.cpp:526]      conv4	backward: 80.355 ms.
I1031 13:42:45.300230 98616 caffe.cpp:522]      relu4	forward: 18.353 ms.
I1031 13:42:45.301033 98616 caffe.cpp:526]      relu4	backward: 46.053 ms.
I1031 13:42:45.301831 98616 caffe.cpp:522]   dropout4	forward: 50.581 ms.
I1031 13:42:45.302883 98616 caffe.cpp:526]   dropout4	backward: 49.591 ms.
I1031 13:42:45.303557 98616 caffe.cpp:522]      pool4	forward: 10.575 ms.
I1031 13:42:45.307170 98616 caffe.cpp:526]      pool4	backward: 31.307 ms.
I1031 13:42:45.308456 98616 caffe.cpp:522]        fc1	forward: 45.399 ms.
I1031 13:42:45.310015 98616 caffe.cpp:526]        fc1	backward: 50.882 ms.
I1031 13:42:45.310994 98616 caffe.cpp:522]   dropout5	forward: 32.508 ms.
I1031 13:42:45.311692 98616 caffe.cpp:526]   dropout5	backward: 14.768 ms.
I1031 13:42:45.312283 98616 caffe.cpp:522]        fc2	forward: 14.879 ms.
I1031 13:42:45.312731 98616 caffe.cpp:526]        fc2	backward: 0.209 ms.
I1031 13:42:45.313125 98616 caffe.cpp:522]       loss	forward: 86.176 ms.
I1031 13:42:45.313814 98616 caffe.cpp:526]       loss	backward: 54.038 ms.
I1031 13:42:45.321177 98616 caffe.cpp:532] Average Forward pass: 925.49 ms.
I1031 13:42:45.338522 98616 caffe.cpp:535] Average Backward pass: 1092.86 ms.
I1031 13:42:45.350848 98616 caffe.cpp:537] Average Forward-Backward: 2547 ms.
I1031 13:42:45.366583 98616 caffe.cpp:540] Total Time: 2547 ms.
I1031 13:42:45.379566 98616 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 0
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 0
elements_fp_double_1 = 0
elements_fp_double_2 = 0
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 0
--->Total double-precision FLOPs = 0
--->Total FLOPs = 0
mem-read-1 = 1
mem-read-2 = 0
mem-read-4 = 6
mem-read-8 = 64
mem-read-16 = 0
mem-read-32 = 0
mem-read-64 = 0
mem-write-1 = 0
mem-write-2 = 0
mem-write-4 = 0
mem-write-8 = 26
mem-write-16 = 0
mem-write-32 = 0
mem-write-64 = 0
--->Total Bytes read = 537
--->Total Bytes written = 208
--->Total Bytes = 745
