sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer11_dir1.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=11 -prof_forward_direction=1
I1031 12:34:43.067814 96294 caffe.cpp:444] Use CPU.
I1031 12:35:00.984649 96294 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 12:35:01.057539 96294 cpu_info.cpp:455] Total number of sockets: 1
I1031 12:35:01.070718 96294 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 12:35:01.083400 96294 cpu_info.cpp:461] Total number of processors: 256
I1031 12:35:01.100636 96294 cpu_info.cpp:464] GPU is used: no
I1031 12:35:01.110661 96294 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 12:35:01.120190 96294 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 12:35:01.132781 96294 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 12:35:10.378029 96294 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 12:35:11.076691 96294 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 12:35:13.522752 96294 layer_factory.hpp:114] Creating layer data
I1031 12:35:13.684649 96294 net.cpp:160] Creating Layer data
I1031 12:35:13.736873 96294 net.cpp:570] data -> data
I1031 12:35:14.239701 96294 net.cpp:570] data -> label
I1031 12:35:21.868430 96294 net.cpp:210] Setting up data
I1031 12:35:21.953085 96294 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 12:35:22.062703 96294 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 12:35:22.070276 96294 net.cpp:225] Memory required for data: 184516
I1031 12:35:22.150208 96294 layer_factory.hpp:114] Creating layer conv1
I1031 12:35:22.507099 96294 net.cpp:160] Creating Layer conv1
I1031 12:35:22.561493 96294 net.cpp:596] conv1 <- data
I1031 12:35:22.692347 96294 net.cpp:570] conv1 -> conv1
I1031 12:36:00.016726 96294 net.cpp:210] Setting up conv1
I1031 12:36:00.092911 96294 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:36:00.103049 96294 net.cpp:225] Memory required for data: 7805124
I1031 12:36:00.427712 96294 layer_factory.hpp:114] Creating layer relu1
I1031 12:36:00.564920 96294 net.cpp:160] Creating Layer relu1
I1031 12:36:00.570129 96294 net.cpp:596] relu1 <- conv1
I1031 12:36:00.606087 96294 net.cpp:557] relu1 -> conv1 (in-place)
I1031 12:36:00.824053 96294 net.cpp:210] Setting up relu1
I1031 12:36:00.826643 96294 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:36:00.827004 96294 net.cpp:225] Memory required for data: 15425732
I1031 12:36:00.827214 96294 layer_factory.hpp:114] Creating layer dropout1
I1031 12:36:00.862669 96294 net.cpp:160] Creating Layer dropout1
I1031 12:36:00.863492 96294 net.cpp:596] dropout1 <- conv1
I1031 12:36:00.866339 96294 net.cpp:570] dropout1 -> drop1
I1031 12:36:00.979765 96294 net.cpp:210] Setting up dropout1
I1031 12:36:00.993477 96294 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:36:00.993878 96294 net.cpp:225] Memory required for data: 23046340
I1031 12:36:00.994230 96294 layer_factory.hpp:114] Creating layer pool1
I1031 12:36:01.103839 96294 net.cpp:160] Creating Layer pool1
I1031 12:36:01.104846 96294 net.cpp:596] pool1 <- drop1
I1031 12:36:01.105167 96294 net.cpp:570] pool1 -> pool1
I1031 12:36:01.564031 96294 net.cpp:210] Setting up pool1
I1031 12:36:01.569345 96294 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 12:36:01.569767 96294 net.cpp:225] Memory required for data: 24951492
I1031 12:36:01.570140 96294 layer_factory.hpp:114] Creating layer conv2
I1031 12:36:01.638279 96294 net.cpp:160] Creating Layer conv2
I1031 12:36:01.643622 96294 net.cpp:596] conv2 <- pool1
I1031 12:36:01.660012 96294 net.cpp:570] conv2 -> conv2
I1031 12:36:08.594527 96294 net.cpp:210] Setting up conv2
I1031 12:36:08.596478 96294 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:36:08.606744 96294 net.cpp:225] Memory required for data: 26733764
I1031 12:36:08.677083 96294 layer_factory.hpp:114] Creating layer relu2
I1031 12:36:08.687904 96294 net.cpp:160] Creating Layer relu2
I1031 12:36:08.698812 96294 net.cpp:596] relu2 <- conv2
I1031 12:36:08.703294 96294 net.cpp:557] relu2 -> conv2 (in-place)
I1031 12:36:08.712224 96294 net.cpp:210] Setting up relu2
I1031 12:36:08.718322 96294 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:36:08.718730 96294 net.cpp:225] Memory required for data: 28516036
I1031 12:36:08.725339 96294 layer_factory.hpp:114] Creating layer dropout2
I1031 12:36:08.731765 96294 net.cpp:160] Creating Layer dropout2
I1031 12:36:08.732125 96294 net.cpp:596] dropout2 <- conv2
I1031 12:36:08.732414 96294 net.cpp:570] dropout2 -> drop2
I1031 12:36:08.736646 96294 net.cpp:210] Setting up dropout2
I1031 12:36:08.746815 96294 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:36:08.755312 96294 net.cpp:225] Memory required for data: 30298308
I1031 12:36:08.761579 96294 layer_factory.hpp:114] Creating layer pool2
I1031 12:36:08.772541 96294 net.cpp:160] Creating Layer pool2
I1031 12:36:08.776923 96294 net.cpp:596] pool2 <- drop2
I1031 12:36:08.781337 96294 net.cpp:570] pool2 -> pool2
I1031 12:36:08.786273 96294 net.cpp:210] Setting up pool2
I1031 12:36:08.792933 96294 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 12:36:08.801389 96294 net.cpp:225] Memory required for data: 30759108
I1031 12:36:08.805433 96294 layer_factory.hpp:114] Creating layer conv3
I1031 12:36:08.812336 96294 net.cpp:160] Creating Layer conv3
I1031 12:36:08.818128 96294 net.cpp:596] conv3 <- pool2
I1031 12:36:08.821234 96294 net.cpp:570] conv3 -> conv3
I1031 12:36:09.396868 96294 net.cpp:210] Setting up conv3
I1031 12:36:09.407198 96294 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:36:09.413617 96294 net.cpp:225] Memory required for data: 31160516
I1031 12:36:09.437747 96294 layer_factory.hpp:114] Creating layer relu3
I1031 12:36:09.448119 96294 net.cpp:160] Creating Layer relu3
I1031 12:36:09.450692 96294 net.cpp:596] relu3 <- conv3
I1031 12:36:09.459143 96294 net.cpp:557] relu3 -> conv3 (in-place)
I1031 12:36:09.465502 96294 net.cpp:210] Setting up relu3
I1031 12:36:09.480301 96294 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:36:09.494663 96294 net.cpp:225] Memory required for data: 31561924
I1031 12:36:09.496829 96294 layer_factory.hpp:114] Creating layer dropout3
I1031 12:36:09.505456 96294 net.cpp:160] Creating Layer dropout3
I1031 12:36:09.511716 96294 net.cpp:596] dropout3 <- conv3
I1031 12:36:09.523610 96294 net.cpp:570] dropout3 -> drop3
I1031 12:36:09.539721 96294 net.cpp:210] Setting up dropout3
I1031 12:36:09.545858 96294 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:36:09.550475 96294 net.cpp:225] Memory required for data: 31963332
I1031 12:36:09.560582 96294 layer_factory.hpp:114] Creating layer pool3
I1031 12:36:09.564858 96294 net.cpp:160] Creating Layer pool3
I1031 12:36:09.571261 96294 net.cpp:596] pool3 <- drop3
I1031 12:36:09.575839 96294 net.cpp:570] pool3 -> pool3
I1031 12:36:09.578418 96294 net.cpp:210] Setting up pool3
I1031 12:36:09.583166 96294 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 12:36:09.588871 96294 net.cpp:225] Memory required for data: 32063684
I1031 12:36:09.591922 96294 layer_factory.hpp:114] Creating layer conv4
I1031 12:36:09.593040 96294 net.cpp:160] Creating Layer conv4
I1031 12:36:09.593919 96294 net.cpp:596] conv4 <- pool3
I1031 12:36:09.594425 96294 net.cpp:570] conv4 -> conv4
I1031 12:36:09.957044 96294 net.cpp:210] Setting up conv4
I1031 12:36:09.959259 96294 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:36:09.964129 96294 net.cpp:225] Memory required for data: 32137412
I1031 12:36:09.974241 96294 layer_factory.hpp:114] Creating layer relu4
I1031 12:36:09.983183 96294 net.cpp:160] Creating Layer relu4
I1031 12:36:09.987912 96294 net.cpp:596] relu4 <- conv4
I1031 12:36:09.991675 96294 net.cpp:557] relu4 -> conv4 (in-place)
I1031 12:36:09.992489 96294 net.cpp:210] Setting up relu4
I1031 12:36:09.992760 96294 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:36:09.993011 96294 net.cpp:225] Memory required for data: 32211140
I1031 12:36:09.993218 96294 layer_factory.hpp:114] Creating layer dropout4
I1031 12:36:09.993458 96294 net.cpp:160] Creating Layer dropout4
I1031 12:36:09.993660 96294 net.cpp:596] dropout4 <- conv4
I1031 12:36:09.993914 96294 net.cpp:570] dropout4 -> drop4
I1031 12:36:09.994279 96294 net.cpp:210] Setting up dropout4
I1031 12:36:09.994551 96294 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:36:09.994796 96294 net.cpp:225] Memory required for data: 32284868
I1031 12:36:09.995002 96294 layer_factory.hpp:114] Creating layer pool4
I1031 12:36:09.995312 96294 net.cpp:160] Creating Layer pool4
I1031 12:36:09.995576 96294 net.cpp:596] pool4 <- drop4
I1031 12:36:09.995816 96294 net.cpp:570] pool4 -> pool4
I1031 12:36:10.009723 96294 net.cpp:210] Setting up pool4
I1031 12:36:10.012975 96294 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 12:36:10.015518 96294 net.cpp:225] Memory required for data: 32303300
I1031 12:36:10.015777 96294 layer_factory.hpp:114] Creating layer fc1
I1031 12:36:10.081583 96294 net.cpp:160] Creating Layer fc1
I1031 12:36:10.081933 96294 net.cpp:596] fc1 <- pool4
I1031 12:36:10.082345 96294 net.cpp:570] fc1 -> fc1
I1031 12:36:10.894224 96294 net.cpp:210] Setting up fc1
I1031 12:36:10.901311 96294 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:36:10.917420 96294 net.cpp:225] Memory required for data: 32307396
I1031 12:36:10.928583 96294 layer_factory.hpp:114] Creating layer dropout5
I1031 12:36:10.937291 96294 net.cpp:160] Creating Layer dropout5
I1031 12:36:10.947865 96294 net.cpp:596] dropout5 <- fc1
I1031 12:36:10.953963 96294 net.cpp:570] dropout5 -> drop5
I1031 12:36:10.956531 96294 net.cpp:210] Setting up dropout5
I1031 12:36:10.962644 96294 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:36:10.967134 96294 net.cpp:225] Memory required for data: 32311492
I1031 12:36:10.974056 96294 layer_factory.hpp:114] Creating layer fc2
I1031 12:36:10.978605 96294 net.cpp:160] Creating Layer fc2
I1031 12:36:10.980731 96294 net.cpp:596] fc2 <- drop5
I1031 12:36:10.989632 96294 net.cpp:570] fc2 -> fc2
I1031 12:36:11.031435 96294 net.cpp:210] Setting up fc2
I1031 12:36:11.050660 96294 net.cpp:217] Top shape: 1 2 (2)
I1031 12:36:11.059083 96294 net.cpp:225] Memory required for data: 32311500
I1031 12:36:11.078161 96294 layer_factory.hpp:114] Creating layer loss
I1031 12:36:11.108824 96294 net.cpp:160] Creating Layer loss
I1031 12:36:11.111073 96294 net.cpp:596] loss <- fc2
I1031 12:36:11.120059 96294 net.cpp:596] loss <- label
I1031 12:36:11.183269 96294 net.cpp:570] loss -> (automatic)
I1031 12:36:11.234593 96294 layer_factory.hpp:114] Creating layer loss
I1031 12:36:11.646702 96294 net.cpp:210] Setting up loss
I1031 12:36:11.652971 96294 net.cpp:217] Top shape: (1)
I1031 12:36:11.666473 96294 net.cpp:220]     with loss weight 1
I1031 12:36:11.802335 96294 net.cpp:225] Memory required for data: 32311504
I1031 12:36:11.849459 96294 net.cpp:287] loss needs backward computation.
I1031 12:36:11.952193 96294 net.cpp:287] fc2 needs backward computation.
I1031 12:36:11.970427 96294 net.cpp:287] dropout5 needs backward computation.
I1031 12:36:11.980903 96294 net.cpp:287] fc1 needs backward computation.
I1031 12:36:12.002521 96294 net.cpp:287] pool4 needs backward computation.
I1031 12:36:12.007050 96294 net.cpp:287] dropout4 needs backward computation.
I1031 12:36:12.009104 96294 net.cpp:287] relu4 needs backward computation.
I1031 12:36:12.023412 96294 net.cpp:287] conv4 needs backward computation.
I1031 12:36:12.041930 96294 net.cpp:287] pool3 needs backward computation.
I1031 12:36:12.055601 96294 net.cpp:287] dropout3 needs backward computation.
I1031 12:36:12.060394 96294 net.cpp:287] relu3 needs backward computation.
I1031 12:36:12.060724 96294 net.cpp:287] conv3 needs backward computation.
I1031 12:36:12.061041 96294 net.cpp:287] pool2 needs backward computation.
I1031 12:36:12.061312 96294 net.cpp:287] dropout2 needs backward computation.
I1031 12:36:12.061511 96294 net.cpp:287] relu2 needs backward computation.
I1031 12:36:12.061698 96294 net.cpp:287] conv2 needs backward computation.
I1031 12:36:12.061889 96294 net.cpp:287] pool1 needs backward computation.
I1031 12:36:12.062078 96294 net.cpp:287] dropout1 needs backward computation.
I1031 12:36:12.062266 96294 net.cpp:287] relu1 needs backward computation.
I1031 12:36:12.062450 96294 net.cpp:287] conv1 needs backward computation.
I1031 12:36:12.083418 96294 net.cpp:289] data does not need backward computation.
I1031 12:36:12.142606 96294 net.cpp:345] Network initialization done.
I1031 12:36:12.336580 96294 caffe.cpp:452] Performing Forward
I1031 12:36:24.197464 96294 caffe.cpp:457] Initial loss: 0
I1031 12:36:24.231711 96294 caffe.cpp:459] Performing Backward
I1031 12:36:27.650319 96294 caffe.cpp:468] *** Benchmark begins ***
I1031 12:36:27.666344 96294 caffe.cpp:469] Testing for 1 iterations.
I1031 12:36:27.826599 96294 caffe.cpp:482] Profiling Layer: dropout3 forward
I1031 12:36:29.879700 96294 caffe.cpp:512] Iteration: 1 forward-backward time: 2038 ms.
I1031 12:36:30.053644 96294 caffe.cpp:519] Average time per layer: 
I1031 12:36:30.075899 96294 caffe.cpp:522]       data	forward: 48.071 ms.
I1031 12:36:30.169502 96294 caffe.cpp:526]       data	backward: 7.322 ms.
I1031 12:36:30.193503 96294 caffe.cpp:522]      conv1	forward: 55.225 ms.
I1031 12:36:30.200901 96294 caffe.cpp:526]      conv1	backward: 37.143 ms.
I1031 12:36:30.213713 96294 caffe.cpp:522]      relu1	forward: 21.182 ms.
I1031 12:36:30.221694 96294 caffe.cpp:526]      relu1	backward: 59.086 ms.
I1031 12:36:30.232594 96294 caffe.cpp:522]   dropout1	forward: 75.668 ms.
I1031 12:36:30.237948 96294 caffe.cpp:526]   dropout1	backward: 72.883 ms.
I1031 12:36:30.238332 96294 caffe.cpp:522]      pool1	forward: 126.041 ms.
I1031 12:36:30.243433 96294 caffe.cpp:526]      pool1	backward: 112.87 ms.
I1031 12:36:30.243741 96294 caffe.cpp:522]      conv2	forward: 72.982 ms.
I1031 12:36:30.244103 96294 caffe.cpp:526]      conv2	backward: 14.217 ms.
I1031 12:36:30.244377 96294 caffe.cpp:522]      relu2	forward: 19.197 ms.
I1031 12:36:30.244585 96294 caffe.cpp:526]      relu2	backward: 6.909 ms.
I1031 12:36:30.244786 96294 caffe.cpp:522]   dropout2	forward: 61.98 ms.
I1031 12:36:30.244989 96294 caffe.cpp:526]   dropout2	backward: 7.716 ms.
I1031 12:36:30.245187 96294 caffe.cpp:522]      pool2	forward: 39.931 ms.
I1031 12:36:30.245388 96294 caffe.cpp:526]      pool2	backward: 28.225 ms.
I1031 12:36:30.245590 96294 caffe.cpp:522]      conv3	forward: 62.494 ms.
I1031 12:36:30.245791 96294 caffe.cpp:526]      conv3	backward: 85.27 ms.
I1031 12:36:30.245992 96294 caffe.cpp:522]      relu3	forward: 20.945 ms.
I1031 12:36:30.246192 96294 caffe.cpp:526]      relu3	backward: 38.3 ms.
I1031 12:36:30.246433 96294 caffe.cpp:522]   dropout3	forward: 62.097 ms.
I1031 12:36:30.246652 96294 caffe.cpp:526]   dropout3	backward: 40.94 ms.
I1031 12:36:30.246989 96294 caffe.cpp:522]      pool3	forward: 25.475 ms.
I1031 12:36:30.247303 96294 caffe.cpp:526]      pool3	backward: 63.255 ms.
I1031 12:36:30.247586 96294 caffe.cpp:522]      conv4	forward: 64.033 ms.
I1031 12:36:30.247788 96294 caffe.cpp:526]      conv4	backward: 92.162 ms.
I1031 12:36:30.247989 96294 caffe.cpp:522]      relu4	forward: 16.673 ms.
I1031 12:36:30.248190 96294 caffe.cpp:526]      relu4	backward: 49.952 ms.
I1031 12:36:30.248390 96294 caffe.cpp:522]   dropout4	forward: 61.454 ms.
I1031 12:36:30.248590 96294 caffe.cpp:526]   dropout4	backward: 49.117 ms.
I1031 12:36:30.248791 96294 caffe.cpp:522]      pool4	forward: 14.525 ms.
I1031 12:36:30.248992 96294 caffe.cpp:526]      pool4	backward: 48.983 ms.
I1031 12:36:30.249193 96294 caffe.cpp:522]        fc1	forward: 26.969 ms.
I1031 12:36:30.249436 96294 caffe.cpp:526]        fc1	backward: 41.84 ms.
I1031 12:36:30.249662 96294 caffe.cpp:522]   dropout5	forward: 32.381 ms.
I1031 12:36:30.250001 96294 caffe.cpp:526]   dropout5	backward: 18.485 ms.
I1031 12:36:30.250259 96294 caffe.cpp:522]        fc2	forward: 15.906 ms.
I1031 12:36:30.250464 96294 caffe.cpp:526]        fc2	backward: 0.274 ms.
I1031 12:36:30.250665 96294 caffe.cpp:522]       loss	forward: 82.917 ms.
I1031 12:36:30.250867 96294 caffe.cpp:526]       loss	backward: 53.079 ms.
I1031 12:36:30.256738 96294 caffe.cpp:532] Average Forward pass: 1071.23 ms.
I1031 12:36:30.270328 96294 caffe.cpp:535] Average Backward pass: 939.494 ms.
I1031 12:36:30.281733 96294 caffe.cpp:537] Average Forward-Backward: 2539 ms.
I1031 12:36:30.297194 96294 caffe.cpp:540] Total Time: 2539 ms.
I1031 12:36:30.310433 96294 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 3
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 12544
elements_fp_double_1 = 33
elements_fp_double_2 = 150752
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 200707
--->Total double-precision FLOPs = 301537
--->Total FLOPs = 502244
mem-read-1 = 67863
mem-read-2 = 105
mem-read-4 = 565824
mem-read-8 = 804688
mem-read-16 = 501776
mem-read-32 = 1
mem-read-64 = 30225
mem-write-1 = 188
mem-write-2 = 67
mem-write-4 = 2456
mem-write-8 = 130815
mem-write-16 = 32
mem-write-32 = 1
mem-write-64 = 19713
--->Total Bytes read = 18731721
--->Total Bytes written = 2318842
--->Total Bytes = 21050563
