sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer12_dir1.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=12 -prof_forward_direction=1
I1031 12:42:18.380316 96505 caffe.cpp:444] Use CPU.
I1031 12:42:36.249498 96505 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 12:42:36.312813 96505 cpu_info.cpp:455] Total number of sockets: 1
I1031 12:42:36.325878 96505 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 12:42:36.338171 96505 cpu_info.cpp:461] Total number of processors: 256
I1031 12:42:36.355154 96505 cpu_info.cpp:464] GPU is used: no
I1031 12:42:36.364610 96505 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 12:42:36.373427 96505 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 12:42:36.386039 96505 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 12:42:45.512867 96505 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 12:42:46.194919 96505 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 12:42:48.625880 96505 layer_factory.hpp:114] Creating layer data
I1031 12:42:48.784713 96505 net.cpp:160] Creating Layer data
I1031 12:42:48.835564 96505 net.cpp:570] data -> data
I1031 12:42:49.329066 96505 net.cpp:570] data -> label
I1031 12:42:56.778909 96505 net.cpp:210] Setting up data
I1031 12:42:56.864934 96505 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 12:42:56.974791 96505 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 12:42:56.983062 96505 net.cpp:225] Memory required for data: 184516
I1031 12:42:57.064122 96505 layer_factory.hpp:114] Creating layer conv1
I1031 12:42:57.428413 96505 net.cpp:160] Creating Layer conv1
I1031 12:42:57.481241 96505 net.cpp:596] conv1 <- data
I1031 12:42:57.607501 96505 net.cpp:570] conv1 -> conv1
I1031 12:43:35.071832 96505 net.cpp:210] Setting up conv1
I1031 12:43:35.136843 96505 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:43:35.144328 96505 net.cpp:225] Memory required for data: 7805124
I1031 12:43:35.462713 96505 layer_factory.hpp:114] Creating layer relu1
I1031 12:43:35.604526 96505 net.cpp:160] Creating Layer relu1
I1031 12:43:35.609899 96505 net.cpp:596] relu1 <- conv1
I1031 12:43:35.644327 96505 net.cpp:557] relu1 -> conv1 (in-place)
I1031 12:43:35.862373 96505 net.cpp:210] Setting up relu1
I1031 12:43:35.865150 96505 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:43:35.865517 96505 net.cpp:225] Memory required for data: 15425732
I1031 12:43:35.865731 96505 layer_factory.hpp:114] Creating layer dropout1
I1031 12:43:35.898428 96505 net.cpp:160] Creating Layer dropout1
I1031 12:43:35.898763 96505 net.cpp:596] dropout1 <- conv1
I1031 12:43:35.901656 96505 net.cpp:570] dropout1 -> drop1
I1031 12:43:36.021942 96505 net.cpp:210] Setting up dropout1
I1031 12:43:36.036239 96505 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:43:36.036664 96505 net.cpp:225] Memory required for data: 23046340
I1031 12:43:36.037016 96505 layer_factory.hpp:114] Creating layer pool1
I1031 12:43:36.141716 96505 net.cpp:160] Creating Layer pool1
I1031 12:43:36.142457 96505 net.cpp:596] pool1 <- drop1
I1031 12:43:36.142866 96505 net.cpp:570] pool1 -> pool1
I1031 12:43:36.565385 96505 net.cpp:210] Setting up pool1
I1031 12:43:36.574990 96505 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 12:43:36.575496 96505 net.cpp:225] Memory required for data: 24951492
I1031 12:43:36.575822 96505 layer_factory.hpp:114] Creating layer conv2
I1031 12:43:36.642562 96505 net.cpp:160] Creating Layer conv2
I1031 12:43:36.647126 96505 net.cpp:596] conv2 <- pool1
I1031 12:43:36.663235 96505 net.cpp:570] conv2 -> conv2
I1031 12:43:43.495100 96505 net.cpp:210] Setting up conv2
I1031 12:43:43.498309 96505 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:43:43.510144 96505 net.cpp:225] Memory required for data: 26733764
I1031 12:43:43.582947 96505 layer_factory.hpp:114] Creating layer relu2
I1031 12:43:43.593619 96505 net.cpp:160] Creating Layer relu2
I1031 12:43:43.603117 96505 net.cpp:596] relu2 <- conv2
I1031 12:43:43.611697 96505 net.cpp:557] relu2 -> conv2 (in-place)
I1031 12:43:43.624799 96505 net.cpp:210] Setting up relu2
I1031 12:43:43.630918 96505 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:43:43.637527 96505 net.cpp:225] Memory required for data: 28516036
I1031 12:43:43.647955 96505 layer_factory.hpp:114] Creating layer dropout2
I1031 12:43:43.652458 96505 net.cpp:160] Creating Layer dropout2
I1031 12:43:43.654276 96505 net.cpp:596] dropout2 <- conv2
I1031 12:43:43.667410 96505 net.cpp:570] dropout2 -> drop2
I1031 12:43:43.669684 96505 net.cpp:210] Setting up dropout2
I1031 12:43:43.674136 96505 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:43:43.682582 96505 net.cpp:225] Memory required for data: 30298308
I1031 12:43:43.689070 96505 layer_factory.hpp:114] Creating layer pool2
I1031 12:43:43.693629 96505 net.cpp:160] Creating Layer pool2
I1031 12:43:43.700223 96505 net.cpp:596] pool2 <- drop2
I1031 12:43:43.708838 96505 net.cpp:570] pool2 -> pool2
I1031 12:43:43.711536 96505 net.cpp:210] Setting up pool2
I1031 12:43:43.717319 96505 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 12:43:43.729997 96505 net.cpp:225] Memory required for data: 30759108
I1031 12:43:43.735347 96505 layer_factory.hpp:114] Creating layer conv3
I1031 12:43:43.743325 96505 net.cpp:160] Creating Layer conv3
I1031 12:43:43.752048 96505 net.cpp:596] conv3 <- pool2
I1031 12:43:43.762508 96505 net.cpp:570] conv3 -> conv3
I1031 12:43:44.358662 96505 net.cpp:210] Setting up conv3
I1031 12:43:44.365177 96505 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:43:44.367686 96505 net.cpp:225] Memory required for data: 31160516
I1031 12:43:44.383563 96505 layer_factory.hpp:114] Creating layer relu3
I1031 12:43:44.395704 96505 net.cpp:160] Creating Layer relu3
I1031 12:43:44.398067 96505 net.cpp:596] relu3 <- conv3
I1031 12:43:44.405014 96505 net.cpp:557] relu3 -> conv3 (in-place)
I1031 12:43:44.411664 96505 net.cpp:210] Setting up relu3
I1031 12:43:44.418145 96505 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:43:44.424752 96505 net.cpp:225] Memory required for data: 31561924
I1031 12:43:44.435125 96505 layer_factory.hpp:114] Creating layer dropout3
I1031 12:43:44.441982 96505 net.cpp:160] Creating Layer dropout3
I1031 12:43:44.450412 96505 net.cpp:596] dropout3 <- conv3
I1031 12:43:44.456794 96505 net.cpp:570] dropout3 -> drop3
I1031 12:43:44.459602 96505 net.cpp:210] Setting up dropout3
I1031 12:43:44.461172 96505 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:43:44.464278 96505 net.cpp:225] Memory required for data: 31963332
I1031 12:43:44.474730 96505 layer_factory.hpp:114] Creating layer pool3
I1031 12:43:44.481823 96505 net.cpp:160] Creating Layer pool3
I1031 12:43:44.484113 96505 net.cpp:596] pool3 <- drop3
I1031 12:43:44.494799 96505 net.cpp:570] pool3 -> pool3
I1031 12:43:44.501778 96505 net.cpp:210] Setting up pool3
I1031 12:43:44.503964 96505 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 12:43:44.508842 96505 net.cpp:225] Memory required for data: 32063684
I1031 12:43:44.519351 96505 layer_factory.hpp:114] Creating layer conv4
I1031 12:43:44.527998 96505 net.cpp:160] Creating Layer conv4
I1031 12:43:44.536361 96505 net.cpp:596] conv4 <- pool3
I1031 12:43:44.548749 96505 net.cpp:570] conv4 -> conv4
I1031 12:43:44.924342 96505 net.cpp:210] Setting up conv4
I1031 12:43:44.926226 96505 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:43:44.933843 96505 net.cpp:225] Memory required for data: 32137412
I1031 12:43:44.940554 96505 layer_factory.hpp:114] Creating layer relu4
I1031 12:43:44.950858 96505 net.cpp:160] Creating Layer relu4
I1031 12:43:44.954973 96505 net.cpp:596] relu4 <- conv4
I1031 12:43:44.965198 96505 net.cpp:557] relu4 -> conv4 (in-place)
I1031 12:43:44.978808 96505 net.cpp:210] Setting up relu4
I1031 12:43:44.983180 96505 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:43:44.987833 96505 net.cpp:225] Memory required for data: 32211140
I1031 12:43:44.994515 96505 layer_factory.hpp:114] Creating layer dropout4
I1031 12:43:45.000980 96505 net.cpp:160] Creating Layer dropout4
I1031 12:43:45.009032 96505 net.cpp:596] dropout4 <- conv4
I1031 12:43:45.013463 96505 net.cpp:570] dropout4 -> drop4
I1031 12:43:45.025998 96505 net.cpp:210] Setting up dropout4
I1031 12:43:45.028376 96505 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:43:45.034550 96505 net.cpp:225] Memory required for data: 32284868
I1031 12:43:45.036718 96505 layer_factory.hpp:114] Creating layer pool4
I1031 12:43:45.043551 96505 net.cpp:160] Creating Layer pool4
I1031 12:43:45.053903 96505 net.cpp:596] pool4 <- drop4
I1031 12:43:45.057842 96505 net.cpp:570] pool4 -> pool4
I1031 12:43:45.080641 96505 net.cpp:210] Setting up pool4
I1031 12:43:45.087427 96505 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 12:43:45.093822 96505 net.cpp:225] Memory required for data: 32303300
I1031 12:43:45.095876 96505 layer_factory.hpp:114] Creating layer fc1
I1031 12:43:45.164402 96505 net.cpp:160] Creating Layer fc1
I1031 12:43:45.169134 96505 net.cpp:596] fc1 <- pool4
I1031 12:43:45.176342 96505 net.cpp:570] fc1 -> fc1
I1031 12:43:46.070089 96505 net.cpp:210] Setting up fc1
I1031 12:43:46.078531 96505 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:43:46.086941 96505 net.cpp:225] Memory required for data: 32307396
I1031 12:43:46.096053 96505 layer_factory.hpp:114] Creating layer dropout5
I1031 12:43:46.106565 96505 net.cpp:160] Creating Layer dropout5
I1031 12:43:46.115175 96505 net.cpp:596] dropout5 <- fc1
I1031 12:43:46.126128 96505 net.cpp:570] dropout5 -> drop5
I1031 12:43:46.132225 96505 net.cpp:210] Setting up dropout5
I1031 12:43:46.144989 96505 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:43:46.155474 96505 net.cpp:225] Memory required for data: 32311492
I1031 12:43:46.159942 96505 layer_factory.hpp:114] Creating layer fc2
I1031 12:43:46.166280 96505 net.cpp:160] Creating Layer fc2
I1031 12:43:46.171860 96505 net.cpp:596] fc2 <- drop5
I1031 12:43:46.178697 96505 net.cpp:570] fc2 -> fc2
I1031 12:43:46.204926 96505 net.cpp:210] Setting up fc2
I1031 12:43:46.218052 96505 net.cpp:217] Top shape: 1 2 (2)
I1031 12:43:46.227288 96505 net.cpp:225] Memory required for data: 32311500
I1031 12:43:46.234194 96505 layer_factory.hpp:114] Creating layer loss
I1031 12:43:46.266661 96505 net.cpp:160] Creating Layer loss
I1031 12:43:46.275118 96505 net.cpp:596] loss <- fc2
I1031 12:43:46.285823 96505 net.cpp:596] loss <- label
I1031 12:43:46.350528 96505 net.cpp:570] loss -> (automatic)
I1031 12:43:46.404225 96505 layer_factory.hpp:114] Creating layer loss
I1031 12:43:46.835605 96505 net.cpp:210] Setting up loss
I1031 12:43:46.848500 96505 net.cpp:217] Top shape: (1)
I1031 12:43:46.863152 96505 net.cpp:220]     with loss weight 1
I1031 12:43:47.000733 96505 net.cpp:225] Memory required for data: 32311504
I1031 12:43:47.053594 96505 net.cpp:287] loss needs backward computation.
I1031 12:43:47.156234 96505 net.cpp:287] fc2 needs backward computation.
I1031 12:43:47.169755 96505 net.cpp:287] dropout5 needs backward computation.
I1031 12:43:47.178630 96505 net.cpp:287] fc1 needs backward computation.
I1031 12:43:47.186100 96505 net.cpp:287] pool4 needs backward computation.
I1031 12:43:47.194371 96505 net.cpp:287] dropout4 needs backward computation.
I1031 12:43:47.200929 96505 net.cpp:287] relu4 needs backward computation.
I1031 12:43:47.213333 96505 net.cpp:287] conv4 needs backward computation.
I1031 12:43:47.230535 96505 net.cpp:287] pool3 needs backward computation.
I1031 12:43:47.244318 96505 net.cpp:287] dropout3 needs backward computation.
I1031 12:43:47.249227 96505 net.cpp:287] relu3 needs backward computation.
I1031 12:43:47.249557 96505 net.cpp:287] conv3 needs backward computation.
I1031 12:43:47.249883 96505 net.cpp:287] pool2 needs backward computation.
I1031 12:43:47.250154 96505 net.cpp:287] dropout2 needs backward computation.
I1031 12:43:47.250349 96505 net.cpp:287] relu2 needs backward computation.
I1031 12:43:47.250535 96505 net.cpp:287] conv2 needs backward computation.
I1031 12:43:47.250730 96505 net.cpp:287] pool1 needs backward computation.
I1031 12:43:47.250919 96505 net.cpp:287] dropout1 needs backward computation.
I1031 12:43:47.251108 96505 net.cpp:287] relu1 needs backward computation.
I1031 12:43:47.251293 96505 net.cpp:287] conv1 needs backward computation.
I1031 12:43:47.271461 96505 net.cpp:289] data does not need backward computation.
I1031 12:43:47.328809 96505 net.cpp:345] Network initialization done.
I1031 12:43:47.513592 96505 caffe.cpp:452] Performing Forward
I1031 12:43:59.463467 96505 caffe.cpp:457] Initial loss: 16.15
I1031 12:43:59.602017 96505 caffe.cpp:459] Performing Backward
I1031 12:44:02.978945 96505 caffe.cpp:468] *** Benchmark begins ***
I1031 12:44:02.994638 96505 caffe.cpp:469] Testing for 1 iterations.
I1031 12:44:03.149812 96505 caffe.cpp:482] Profiling Layer: pool3 forward
I1031 12:44:05.010403 96505 caffe.cpp:512] Iteration: 1 forward-backward time: 1854 ms.
I1031 12:44:05.101610 96505 caffe.cpp:519] Average time per layer: 
I1031 12:44:05.117771 96505 caffe.cpp:522]       data	forward: 52.145 ms.
I1031 12:44:05.183346 96505 caffe.cpp:526]       data	backward: 5.62 ms.
I1031 12:44:05.204159 96505 caffe.cpp:522]      conv1	forward: 69.282 ms.
I1031 12:44:05.227208 96505 caffe.cpp:526]      conv1	backward: 49.438 ms.
I1031 12:44:05.230309 96505 caffe.cpp:522]      relu1	forward: 18.324 ms.
I1031 12:44:05.240140 96505 caffe.cpp:526]      relu1	backward: 55.692 ms.
I1031 12:44:05.244349 96505 caffe.cpp:522]   dropout1	forward: 106.194 ms.
I1031 12:44:05.248802 96505 caffe.cpp:526]   dropout1	backward: 74.388 ms.
I1031 12:44:05.254451 96505 caffe.cpp:522]      pool1	forward: 136.111 ms.
I1031 12:44:05.259584 96505 caffe.cpp:526]      pool1	backward: 146.905 ms.
I1031 12:44:05.263533 96505 caffe.cpp:522]      conv2	forward: 27.506 ms.
I1031 12:44:05.269196 96505 caffe.cpp:526]      conv2	backward: 66.827 ms.
I1031 12:44:05.271477 96505 caffe.cpp:522]      relu2	forward: 0.138 ms.
I1031 12:44:05.271860 96505 caffe.cpp:526]      relu2	backward: 32.01 ms.
I1031 12:44:05.275586 96505 caffe.cpp:522]   dropout2	forward: 8.216 ms.
I1031 12:44:05.279736 96505 caffe.cpp:526]   dropout2	backward: 52.394 ms.
I1031 12:44:05.283704 96505 caffe.cpp:522]      pool2	forward: 33.55 ms.
I1031 12:44:05.286175 96505 caffe.cpp:526]      pool2	backward: 65.478 ms.
I1031 12:44:05.287884 96505 caffe.cpp:522]      conv3	forward: 2.187 ms.
I1031 12:44:05.289885 96505 caffe.cpp:526]      conv3	backward: 75.453 ms.
I1031 12:44:05.293267 96505 caffe.cpp:522]      relu3	forward: 0.092 ms.
I1031 12:44:05.317416 96505 caffe.cpp:526]      relu3	backward: 33.662 ms.
I1031 12:44:05.323650 96505 caffe.cpp:522]   dropout3	forward: 2.087 ms.
I1031 12:44:05.344403 96505 caffe.cpp:526]   dropout3	backward: 69.806 ms.
I1031 12:44:05.348919 96505 caffe.cpp:522]      pool3	forward: 19.192 ms.
I1031 12:44:05.351430 96505 caffe.cpp:526]      pool3	backward: 64.548 ms.
I1031 12:44:05.357075 96505 caffe.cpp:522]      conv4	forward: 0.79 ms.
I1031 12:44:05.359205 96505 caffe.cpp:526]      conv4	backward: 86.723 ms.
I1031 12:44:05.364259 96505 caffe.cpp:522]      relu4	forward: 0.072 ms.
I1031 12:44:05.374513 96505 caffe.cpp:526]      relu4	backward: 61.238 ms.
I1031 12:44:05.380234 96505 caffe.cpp:522]   dropout4	forward: 0.558 ms.
I1031 12:44:05.380709 96505 caffe.cpp:526]   dropout4	backward: 49.772 ms.
I1031 12:44:05.380942 96505 caffe.cpp:522]      pool4	forward: 1.503 ms.
I1031 12:44:05.381158 96505 caffe.cpp:526]      pool4	backward: 36.64 ms.
I1031 12:44:05.381378 96505 caffe.cpp:522]        fc1	forward: 1.193 ms.
I1031 12:44:05.381592 96505 caffe.cpp:526]        fc1	backward: 58.536 ms.
I1031 12:44:05.381810 96505 caffe.cpp:522]   dropout5	forward: 0.193 ms.
I1031 12:44:05.382025 96505 caffe.cpp:526]   dropout5	backward: 22.275 ms.
I1031 12:44:05.382243 96505 caffe.cpp:522]        fc2	forward: 0.13 ms.
I1031 12:44:05.382458 96505 caffe.cpp:526]        fc2	backward: 0.21 ms.
I1031 12:44:05.382673 96505 caffe.cpp:522]       loss	forward: 115.984 ms.
I1031 12:44:05.382894 96505 caffe.cpp:526]       loss	backward: 52.373 ms.
I1031 12:44:05.388653 96505 caffe.cpp:532] Average Forward pass: 655.088 ms.
I1031 12:44:05.402405 96505 caffe.cpp:535] Average Backward pass: 1169.36 ms.
I1031 12:44:05.414764 96505 caffe.cpp:537] Average Forward-Backward: 2346 ms.
I1031 12:44:05.430717 96505 caffe.cpp:540] Total Time: 2346 ms.
I1031 12:44:05.443804 96505 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 100377
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 0
elements_fp_double_1 = 0
elements_fp_double_2 = 0
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 100377
--->Total double-precision FLOPs = 0
--->Total FLOPs = 100377
mem-read-1 = 839897
mem-read-2 = 37
mem-read-4 = 7029263
mem-read-8 = 9905551
mem-read-16 = 0
mem-read-32 = 1
mem-read-64 = 1
mem-write-1 = 56
mem-write-2 = 17
mem-write-4 = 45300
mem-write-8 = 1131645
mem-write-16 = 0
mem-write-32 = 1
mem-write-64 = 1
--->Total Bytes read = 108201527
--->Total Bytes written = 9234546
--->Total Bytes = 117436073
