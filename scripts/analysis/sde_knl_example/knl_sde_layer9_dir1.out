sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer9_dir1.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=9 -prof_forward_direction=1
I1031 12:22:57.791282 95855 caffe.cpp:444] Use CPU.
I1031 12:23:15.644651 95855 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 12:23:15.707334 95855 cpu_info.cpp:455] Total number of sockets: 1
I1031 12:23:15.720899 95855 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 12:23:15.734390 95855 cpu_info.cpp:461] Total number of processors: 256
I1031 12:23:15.751560 95855 cpu_info.cpp:464] GPU is used: no
I1031 12:23:15.761443 95855 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 12:23:15.770779 95855 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 12:23:15.783293 95855 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 12:23:24.968394 95855 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 12:23:25.653014 95855 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 12:23:28.061833 95855 layer_factory.hpp:114] Creating layer data
I1031 12:23:28.238276 95855 net.cpp:160] Creating Layer data
I1031 12:23:28.289507 95855 net.cpp:570] data -> data
I1031 12:23:28.792115 95855 net.cpp:570] data -> label
I1031 12:23:36.272208 95855 net.cpp:210] Setting up data
I1031 12:23:36.356447 95855 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 12:23:36.465674 95855 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 12:23:36.473402 95855 net.cpp:225] Memory required for data: 184516
I1031 12:23:36.555089 95855 layer_factory.hpp:114] Creating layer conv1
I1031 12:23:36.906713 95855 net.cpp:160] Creating Layer conv1
I1031 12:23:36.960412 95855 net.cpp:596] conv1 <- data
I1031 12:23:37.087867 95855 net.cpp:570] conv1 -> conv1
I1031 12:24:14.517169 95855 net.cpp:210] Setting up conv1
I1031 12:24:14.597326 95855 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:24:14.620836 95855 net.cpp:225] Memory required for data: 7805124
I1031 12:24:14.941555 95855 layer_factory.hpp:114] Creating layer relu1
I1031 12:24:15.077147 95855 net.cpp:160] Creating Layer relu1
I1031 12:24:15.082171 95855 net.cpp:596] relu1 <- conv1
I1031 12:24:15.117238 95855 net.cpp:557] relu1 -> conv1 (in-place)
I1031 12:24:15.337159 95855 net.cpp:210] Setting up relu1
I1031 12:24:15.340001 95855 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:24:15.340389 95855 net.cpp:225] Memory required for data: 15425732
I1031 12:24:15.340653 95855 layer_factory.hpp:114] Creating layer dropout1
I1031 12:24:15.374240 95855 net.cpp:160] Creating Layer dropout1
I1031 12:24:15.374579 95855 net.cpp:596] dropout1 <- conv1
I1031 12:24:15.377624 95855 net.cpp:570] dropout1 -> drop1
I1031 12:24:15.492022 95855 net.cpp:210] Setting up dropout1
I1031 12:24:15.505676 95855 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 12:24:15.506078 95855 net.cpp:225] Memory required for data: 23046340
I1031 12:24:15.506429 95855 layer_factory.hpp:114] Creating layer pool1
I1031 12:24:15.609848 95855 net.cpp:160] Creating Layer pool1
I1031 12:24:15.610333 95855 net.cpp:596] pool1 <- drop1
I1031 12:24:15.610729 95855 net.cpp:570] pool1 -> pool1
I1031 12:24:16.039160 95855 net.cpp:210] Setting up pool1
I1031 12:24:16.044502 95855 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 12:24:16.044953 95855 net.cpp:225] Memory required for data: 24951492
I1031 12:24:16.045341 95855 layer_factory.hpp:114] Creating layer conv2
I1031 12:24:16.109658 95855 net.cpp:160] Creating Layer conv2
I1031 12:24:16.114213 95855 net.cpp:596] conv2 <- pool1
I1031 12:24:16.130228 95855 net.cpp:570] conv2 -> conv2
I1031 12:24:23.055837 95855 net.cpp:210] Setting up conv2
I1031 12:24:23.056314 95855 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:24:23.062933 95855 net.cpp:225] Memory required for data: 26733764
I1031 12:24:23.125493 95855 layer_factory.hpp:114] Creating layer relu2
I1031 12:24:23.125982 95855 net.cpp:160] Creating Layer relu2
I1031 12:24:23.126349 95855 net.cpp:596] relu2 <- conv2
I1031 12:24:23.126653 95855 net.cpp:557] relu2 -> conv2 (in-place)
I1031 12:24:23.127339 95855 net.cpp:210] Setting up relu2
I1031 12:24:23.127706 95855 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:24:23.127985 95855 net.cpp:225] Memory required for data: 28516036
I1031 12:24:23.128202 95855 layer_factory.hpp:114] Creating layer dropout2
I1031 12:24:23.128474 95855 net.cpp:160] Creating Layer dropout2
I1031 12:24:23.128722 95855 net.cpp:596] dropout2 <- conv2
I1031 12:24:23.129093 95855 net.cpp:570] dropout2 -> drop2
I1031 12:24:23.129467 95855 net.cpp:210] Setting up dropout2
I1031 12:24:23.129703 95855 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 12:24:23.129956 95855 net.cpp:225] Memory required for data: 30298308
I1031 12:24:23.130170 95855 layer_factory.hpp:114] Creating layer pool2
I1031 12:24:23.130484 95855 net.cpp:160] Creating Layer pool2
I1031 12:24:23.130717 95855 net.cpp:596] pool2 <- drop2
I1031 12:24:23.130959 95855 net.cpp:570] pool2 -> pool2
I1031 12:24:23.131448 95855 net.cpp:210] Setting up pool2
I1031 12:24:23.131824 95855 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 12:24:23.132128 95855 net.cpp:225] Memory required for data: 30759108
I1031 12:24:23.132339 95855 layer_factory.hpp:114] Creating layer conv3
I1031 12:24:23.132716 95855 net.cpp:160] Creating Layer conv3
I1031 12:24:23.132967 95855 net.cpp:596] conv3 <- pool2
I1031 12:24:23.133240 95855 net.cpp:570] conv3 -> conv3
I1031 12:24:23.762351 95855 net.cpp:210] Setting up conv3
I1031 12:24:23.768965 95855 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:24:23.773654 95855 net.cpp:225] Memory required for data: 31160516
I1031 12:24:23.793836 95855 layer_factory.hpp:114] Creating layer relu3
I1031 12:24:23.798475 95855 net.cpp:160] Creating Layer relu3
I1031 12:24:23.808663 95855 net.cpp:596] relu3 <- conv3
I1031 12:24:23.822787 95855 net.cpp:557] relu3 -> conv3 (in-place)
I1031 12:24:23.831455 95855 net.cpp:210] Setting up relu3
I1031 12:24:23.837276 95855 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:24:23.842536 95855 net.cpp:225] Memory required for data: 31561924
I1031 12:24:23.853169 95855 layer_factory.hpp:114] Creating layer dropout3
I1031 12:24:23.861711 95855 net.cpp:160] Creating Layer dropout3
I1031 12:24:23.870028 95855 net.cpp:596] dropout3 <- conv3
I1031 12:24:23.872545 95855 net.cpp:570] dropout3 -> drop3
I1031 12:24:23.872911 95855 net.cpp:210] Setting up dropout3
I1031 12:24:23.885291 95855 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 12:24:23.897575 95855 net.cpp:225] Memory required for data: 31963332
I1031 12:24:23.906435 95855 layer_factory.hpp:114] Creating layer pool3
I1031 12:24:23.911026 95855 net.cpp:160] Creating Layer pool3
I1031 12:24:23.911437 95855 net.cpp:596] pool3 <- drop3
I1031 12:24:23.923882 95855 net.cpp:570] pool3 -> pool3
I1031 12:24:23.928396 95855 net.cpp:210] Setting up pool3
I1031 12:24:23.935072 95855 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 12:24:23.943341 95855 net.cpp:225] Memory required for data: 32063684
I1031 12:24:23.945518 95855 layer_factory.hpp:114] Creating layer conv4
I1031 12:24:23.949996 95855 net.cpp:160] Creating Layer conv4
I1031 12:24:23.954324 95855 net.cpp:596] conv4 <- pool3
I1031 12:24:23.965085 95855 net.cpp:570] conv4 -> conv4
I1031 12:24:24.343528 95855 net.cpp:210] Setting up conv4
I1031 12:24:24.347565 95855 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:24:24.352532 95855 net.cpp:225] Memory required for data: 32137412
I1031 12:24:24.361006 95855 layer_factory.hpp:114] Creating layer relu4
I1031 12:24:24.363415 95855 net.cpp:160] Creating Layer relu4
I1031 12:24:24.369773 95855 net.cpp:596] relu4 <- conv4
I1031 12:24:24.374405 95855 net.cpp:557] relu4 -> conv4 (in-place)
I1031 12:24:24.376612 95855 net.cpp:210] Setting up relu4
I1031 12:24:24.376916 95855 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:24:24.379407 95855 net.cpp:225] Memory required for data: 32211140
I1031 12:24:24.383987 95855 layer_factory.hpp:114] Creating layer dropout4
I1031 12:24:24.386559 95855 net.cpp:160] Creating Layer dropout4
I1031 12:24:24.398627 95855 net.cpp:596] dropout4 <- conv4
I1031 12:24:24.405266 95855 net.cpp:570] dropout4 -> drop4
I1031 12:24:24.415582 95855 net.cpp:210] Setting up dropout4
I1031 12:24:24.420047 95855 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 12:24:24.420469 95855 net.cpp:225] Memory required for data: 32284868
I1031 12:24:24.422260 95855 layer_factory.hpp:114] Creating layer pool4
I1031 12:24:24.429062 95855 net.cpp:160] Creating Layer pool4
I1031 12:24:24.433398 95855 net.cpp:596] pool4 <- drop4
I1031 12:24:24.440143 95855 net.cpp:570] pool4 -> pool4
I1031 12:24:24.466909 95855 net.cpp:210] Setting up pool4
I1031 12:24:24.471932 95855 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 12:24:24.482571 95855 net.cpp:225] Memory required for data: 32303300
I1031 12:24:24.489433 95855 layer_factory.hpp:114] Creating layer fc1
I1031 12:24:24.556072 95855 net.cpp:160] Creating Layer fc1
I1031 12:24:24.560732 95855 net.cpp:596] fc1 <- pool4
I1031 12:24:24.567348 95855 net.cpp:570] fc1 -> fc1
I1031 12:24:25.436053 95855 net.cpp:210] Setting up fc1
I1031 12:24:25.436374 95855 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:24:25.436743 95855 net.cpp:225] Memory required for data: 32307396
I1031 12:24:25.441962 95855 layer_factory.hpp:114] Creating layer dropout5
I1031 12:24:25.442378 95855 net.cpp:160] Creating Layer dropout5
I1031 12:24:25.442708 95855 net.cpp:596] dropout5 <- fc1
I1031 12:24:25.443009 95855 net.cpp:570] dropout5 -> drop5
I1031 12:24:25.443339 95855 net.cpp:210] Setting up dropout5
I1031 12:24:25.443622 95855 net.cpp:217] Top shape: 1 1024 (1024)
I1031 12:24:25.443882 95855 net.cpp:225] Memory required for data: 32311492
I1031 12:24:25.444095 95855 layer_factory.hpp:114] Creating layer fc2
I1031 12:24:25.444387 95855 net.cpp:160] Creating Layer fc2
I1031 12:24:25.444609 95855 net.cpp:596] fc2 <- drop5
I1031 12:24:25.444875 95855 net.cpp:570] fc2 -> fc2
I1031 12:24:25.454974 95855 net.cpp:210] Setting up fc2
I1031 12:24:25.456495 95855 net.cpp:217] Top shape: 1 2 (2)
I1031 12:24:25.459014 95855 net.cpp:225] Memory required for data: 32311500
I1031 12:24:25.459512 95855 layer_factory.hpp:114] Creating layer loss
I1031 12:24:25.488800 95855 net.cpp:160] Creating Layer loss
I1031 12:24:25.489140 95855 net.cpp:596] loss <- fc2
I1031 12:24:25.490108 95855 net.cpp:596] loss <- label
I1031 12:24:25.545851 95855 net.cpp:570] loss -> (automatic)
I1031 12:24:25.585183 95855 layer_factory.hpp:114] Creating layer loss
I1031 12:24:25.983455 95855 net.cpp:210] Setting up loss
I1031 12:24:25.992099 95855 net.cpp:217] Top shape: (1)
I1031 12:24:26.006490 95855 net.cpp:220]     with loss weight 1
I1031 12:24:26.147661 95855 net.cpp:225] Memory required for data: 32311504
I1031 12:24:26.200536 95855 net.cpp:287] loss needs backward computation.
I1031 12:24:26.308578 95855 net.cpp:287] fc2 needs backward computation.
I1031 12:24:26.324492 95855 net.cpp:287] dropout5 needs backward computation.
I1031 12:24:26.333012 95855 net.cpp:287] fc1 needs backward computation.
I1031 12:24:26.346010 95855 net.cpp:287] pool4 needs backward computation.
I1031 12:24:26.351480 95855 net.cpp:287] dropout4 needs backward computation.
I1031 12:24:26.352514 95855 net.cpp:287] relu4 needs backward computation.
I1031 12:24:26.362848 95855 net.cpp:287] conv4 needs backward computation.
I1031 12:24:26.380053 95855 net.cpp:287] pool3 needs backward computation.
I1031 12:24:26.393775 95855 net.cpp:287] dropout3 needs backward computation.
I1031 12:24:26.398680 95855 net.cpp:287] relu3 needs backward computation.
I1031 12:24:26.399009 95855 net.cpp:287] conv3 needs backward computation.
I1031 12:24:26.399338 95855 net.cpp:287] pool2 needs backward computation.
I1031 12:24:26.399651 95855 net.cpp:287] dropout2 needs backward computation.
I1031 12:24:26.399850 95855 net.cpp:287] relu2 needs backward computation.
I1031 12:24:26.400040 95855 net.cpp:287] conv2 needs backward computation.
I1031 12:24:26.400235 95855 net.cpp:287] pool1 needs backward computation.
I1031 12:24:26.400425 95855 net.cpp:287] dropout1 needs backward computation.
I1031 12:24:26.400617 95855 net.cpp:287] relu1 needs backward computation.
I1031 12:24:26.400802 95855 net.cpp:287] conv1 needs backward computation.
I1031 12:24:26.420953 95855 net.cpp:289] data does not need backward computation.
I1031 12:24:26.478415 95855 net.cpp:345] Network initialization done.
I1031 12:24:26.664194 95855 caffe.cpp:452] Performing Forward
I1031 12:24:38.521805 95855 caffe.cpp:457] Initial loss: 51.5269
I1031 12:24:38.652298 95855 caffe.cpp:459] Performing Backward
I1031 12:24:41.892302 95855 caffe.cpp:468] *** Benchmark begins ***
I1031 12:24:41.908344 95855 caffe.cpp:469] Testing for 1 iterations.
I1031 12:24:42.063148 95855 caffe.cpp:482] Profiling Layer: conv3 forward
I1031 12:24:44.101344 95855 caffe.cpp:512] Iteration: 1 forward-backward time: 2038 ms.
I1031 12:24:44.203107 95855 caffe.cpp:519] Average time per layer: 
I1031 12:24:44.217144 95855 caffe.cpp:522]       data	forward: 54.815 ms.
I1031 12:24:44.288717 95855 caffe.cpp:526]       data	backward: 5.606 ms.
I1031 12:24:44.316973 95855 caffe.cpp:522]      conv1	forward: 54.751 ms.
I1031 12:24:44.325601 95855 caffe.cpp:526]      conv1	backward: 36.766 ms.
I1031 12:24:44.330201 95855 caffe.cpp:522]      relu1	forward: 22.307 ms.
I1031 12:24:44.338891 95855 caffe.cpp:526]      relu1	backward: 61.77 ms.
I1031 12:24:44.349901 95855 caffe.cpp:522]   dropout1	forward: 87.049 ms.
I1031 12:24:44.356986 95855 caffe.cpp:526]   dropout1	backward: 67.834 ms.
I1031 12:24:44.361939 95855 caffe.cpp:522]      pool1	forward: 127.715 ms.
I1031 12:24:44.367136 95855 caffe.cpp:526]      pool1	backward: 135.599 ms.
I1031 12:24:44.374119 95855 caffe.cpp:522]      conv2	forward: 64.185 ms.
I1031 12:24:44.380522 95855 caffe.cpp:526]      conv2	backward: 70.405 ms.
I1031 12:24:44.391000 95855 caffe.cpp:522]      relu2	forward: 18.877 ms.
I1031 12:24:44.394933 95855 caffe.cpp:526]      relu2	backward: 34.602 ms.
I1031 12:24:44.400137 95855 caffe.cpp:522]   dropout2	forward: 57.415 ms.
I1031 12:24:44.412135 95855 caffe.cpp:526]   dropout2	backward: 28.302 ms.
I1031 12:24:44.412559 95855 caffe.cpp:522]      pool2	forward: 42.707 ms.
I1031 12:24:44.415567 95855 caffe.cpp:526]      pool2	backward: 58.257 ms.
I1031 12:24:44.416395 95855 caffe.cpp:522]      conv3	forward: 89.393 ms.
I1031 12:24:44.416642 95855 caffe.cpp:526]      conv3	backward: 70.68 ms.
I1031 12:24:44.416848 95855 caffe.cpp:522]      relu3	forward: 20.853 ms.
I1031 12:24:44.417052 95855 caffe.cpp:526]      relu3	backward: 42.65 ms.
I1031 12:24:44.417253 95855 caffe.cpp:522]   dropout3	forward: 60.062 ms.
I1031 12:24:44.417456 95855 caffe.cpp:526]   dropout3	backward: 54.238 ms.
I1031 12:24:44.417657 95855 caffe.cpp:522]      pool3	forward: 12.901 ms.
I1031 12:24:44.417860 95855 caffe.cpp:526]      pool3	backward: 52.345 ms.
I1031 12:24:44.418061 95855 caffe.cpp:522]      conv4	forward: 59.108 ms.
I1031 12:24:44.418262 95855 caffe.cpp:526]      conv4	backward: 68.13 ms.
I1031 12:24:44.418503 95855 caffe.cpp:522]      relu4	forward: 16.183 ms.
I1031 12:24:44.418726 95855 caffe.cpp:526]      relu4	backward: 5.492 ms.
I1031 12:24:44.419061 95855 caffe.cpp:522]   dropout4	forward: 65.215 ms.
I1031 12:24:44.419319 95855 caffe.cpp:526]   dropout4	backward: 12.894 ms.
I1031 12:24:44.419562 95855 caffe.cpp:522]      pool4	forward: 18.575 ms.
I1031 12:24:44.419767 95855 caffe.cpp:526]      pool4	backward: 2.584 ms.
I1031 12:24:44.419965 95855 caffe.cpp:522]        fc1	forward: 49.653 ms.
I1031 12:24:44.420166 95855 caffe.cpp:526]        fc1	backward: 11.397 ms.
I1031 12:24:44.420368 95855 caffe.cpp:522]   dropout5	forward: 46.516 ms.
I1031 12:24:44.420570 95855 caffe.cpp:526]   dropout5	backward: 0.066 ms.
I1031 12:24:44.440022 95855 caffe.cpp:522]        fc2	forward: 0.474 ms.
I1031 12:24:44.440428 95855 caffe.cpp:526]        fc2	backward: 0.213 ms.
I1031 12:24:44.440704 95855 caffe.cpp:522]       loss	forward: 110.012 ms.
I1031 12:24:44.440930 95855 caffe.cpp:526]       loss	backward: 33.459 ms.
I1031 12:24:44.446835 95855 caffe.cpp:532] Average Forward pass: 1140.61 ms.
I1031 12:24:44.460479 95855 caffe.cpp:535] Average Backward pass: 863.067 ms.
I1031 12:24:44.471920 95855 caffe.cpp:537] Average Forward-Backward: 2492 ms.
I1031 12:24:44.487599 95855 caffe.cpp:540] Total Time: 2492 ms.
I1031 12:24:44.500469 95855 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 1
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 14500864
elements_fp_double_1 = 0
elements_fp_double_2 = 0
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 232013825
--->Total double-precision FLOPs = 0
--->Total FLOPs = 232013825
mem-read-1 = 94455
mem-read-2 = 110
mem-read-4 = 8006571
mem-read-8 = 1189228
mem-read-16 = 0
mem-read-32 = 19202
mem-read-64 = 845764
mem-write-1 = 166
mem-write-2 = 51
mem-write-4 = 119143
mem-write-8 = 154318
mem-write-16 = 0
mem-write-32 = 2
mem-write-64 = 56532
--->Total Bytes read = 96378143
--->Total Bytes written = 5329496
--->Total Bytes = 101707639
