sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer16_dir1.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=16 -prof_forward_direction=1
I1031 13:07:10.286350 97383 caffe.cpp:444] Use CPU.
I1031 13:07:28.151207 97383 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 13:07:28.214042 97383 cpu_info.cpp:455] Total number of sockets: 1
I1031 13:07:28.227036 97383 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 13:07:28.239035 97383 cpu_info.cpp:461] Total number of processors: 256
I1031 13:07:28.256268 97383 cpu_info.cpp:464] GPU is used: no
I1031 13:07:28.265676 97383 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 13:07:28.274644 97383 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 13:07:28.287294 97383 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 13:07:37.417757 97383 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 13:07:38.106003 97383 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 13:07:40.528172 97383 layer_factory.hpp:114] Creating layer data
I1031 13:07:40.688621 97383 net.cpp:160] Creating Layer data
I1031 13:07:40.739847 97383 net.cpp:570] data -> data
I1031 13:07:41.235746 97383 net.cpp:570] data -> label
I1031 13:07:48.746723 97383 net.cpp:210] Setting up data
I1031 13:07:48.831689 97383 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 13:07:48.938626 97383 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 13:07:48.946022 97383 net.cpp:225] Memory required for data: 184516
I1031 13:07:49.025094 97383 layer_factory.hpp:114] Creating layer conv1
I1031 13:07:49.373211 97383 net.cpp:160] Creating Layer conv1
I1031 13:07:49.426888 97383 net.cpp:596] conv1 <- data
I1031 13:07:49.555179 97383 net.cpp:570] conv1 -> conv1
I1031 13:08:26.866680 97383 net.cpp:210] Setting up conv1
I1031 13:08:26.954291 97383 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:08:26.960492 97383 net.cpp:225] Memory required for data: 7805124
I1031 13:08:27.283174 97383 layer_factory.hpp:114] Creating layer relu1
I1031 13:08:27.417088 97383 net.cpp:160] Creating Layer relu1
I1031 13:08:27.422389 97383 net.cpp:596] relu1 <- conv1
I1031 13:08:27.458194 97383 net.cpp:557] relu1 -> conv1 (in-place)
I1031 13:08:27.670658 97383 net.cpp:210] Setting up relu1
I1031 13:08:27.673293 97383 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:08:27.673655 97383 net.cpp:225] Memory required for data: 15425732
I1031 13:08:27.673903 97383 layer_factory.hpp:114] Creating layer dropout1
I1031 13:08:27.706152 97383 net.cpp:160] Creating Layer dropout1
I1031 13:08:27.706483 97383 net.cpp:596] dropout1 <- conv1
I1031 13:08:27.709389 97383 net.cpp:570] dropout1 -> drop1
I1031 13:08:27.821702 97383 net.cpp:210] Setting up dropout1
I1031 13:08:27.835211 97383 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:08:27.835659 97383 net.cpp:225] Memory required for data: 23046340
I1031 13:08:27.836002 97383 layer_factory.hpp:114] Creating layer pool1
I1031 13:08:27.942746 97383 net.cpp:160] Creating Layer pool1
I1031 13:08:27.943083 97383 net.cpp:596] pool1 <- drop1
I1031 13:08:27.943506 97383 net.cpp:570] pool1 -> pool1
I1031 13:08:28.368661 97383 net.cpp:210] Setting up pool1
I1031 13:08:28.373613 97383 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 13:08:28.374009 97383 net.cpp:225] Memory required for data: 24951492
I1031 13:08:28.374339 97383 layer_factory.hpp:114] Creating layer conv2
I1031 13:08:28.436367 97383 net.cpp:160] Creating Layer conv2
I1031 13:08:28.440847 97383 net.cpp:596] conv2 <- pool1
I1031 13:08:28.456260 97383 net.cpp:570] conv2 -> conv2
I1031 13:08:35.328825 97383 net.cpp:210] Setting up conv2
I1031 13:08:35.339526 97383 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:08:35.348772 97383 net.cpp:225] Memory required for data: 26733764
I1031 13:08:35.419167 97383 layer_factory.hpp:114] Creating layer relu2
I1031 13:08:35.430445 97383 net.cpp:160] Creating Layer relu2
I1031 13:08:35.430820 97383 net.cpp:596] relu2 <- conv2
I1031 13:08:35.441589 97383 net.cpp:557] relu2 -> conv2 (in-place)
I1031 13:08:35.450613 97383 net.cpp:210] Setting up relu2
I1031 13:08:35.455011 97383 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:08:35.465481 97383 net.cpp:225] Memory required for data: 28516036
I1031 13:08:35.474185 97383 layer_factory.hpp:114] Creating layer dropout2
I1031 13:08:35.476735 97383 net.cpp:160] Creating Layer dropout2
I1031 13:08:35.489336 97383 net.cpp:596] dropout2 <- conv2
I1031 13:08:35.498092 97383 net.cpp:570] dropout2 -> drop2
I1031 13:08:35.506609 97383 net.cpp:210] Setting up dropout2
I1031 13:08:35.506947 97383 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:08:35.513191 97383 net.cpp:225] Memory required for data: 30298308
I1031 13:08:35.517997 97383 layer_factory.hpp:114] Creating layer pool2
I1031 13:08:35.522565 97383 net.cpp:160] Creating Layer pool2
I1031 13:08:35.527695 97383 net.cpp:596] pool2 <- drop2
I1031 13:08:35.533637 97383 net.cpp:570] pool2 -> pool2
I1031 13:08:35.545054 97383 net.cpp:210] Setting up pool2
I1031 13:08:35.549859 97383 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 13:08:35.552403 97383 net.cpp:225] Memory required for data: 30759108
I1031 13:08:35.558786 97383 layer_factory.hpp:114] Creating layer conv3
I1031 13:08:35.561058 97383 net.cpp:160] Creating Layer conv3
I1031 13:08:35.568549 97383 net.cpp:596] conv3 <- pool2
I1031 13:08:35.568934 97383 net.cpp:570] conv3 -> conv3
I1031 13:08:36.221767 97383 net.cpp:210] Setting up conv3
I1031 13:08:36.236091 97383 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:08:36.242688 97383 net.cpp:225] Memory required for data: 31160516
I1031 13:08:36.259167 97383 layer_factory.hpp:114] Creating layer relu3
I1031 13:08:36.268177 97383 net.cpp:160] Creating Layer relu3
I1031 13:08:36.274518 97383 net.cpp:596] relu3 <- conv3
I1031 13:08:36.288439 97383 net.cpp:557] relu3 -> conv3 (in-place)
I1031 13:08:36.299408 97383 net.cpp:210] Setting up relu3
I1031 13:08:36.309741 97383 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:08:36.310166 97383 net.cpp:225] Memory required for data: 31561924
I1031 13:08:36.314108 97383 layer_factory.hpp:114] Creating layer dropout3
I1031 13:08:36.322340 97383 net.cpp:160] Creating Layer dropout3
I1031 13:08:36.324705 97383 net.cpp:596] dropout3 <- conv3
I1031 13:08:36.331061 97383 net.cpp:570] dropout3 -> drop3
I1031 13:08:36.331570 97383 net.cpp:210] Setting up dropout3
I1031 13:08:36.338027 97383 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:08:36.342607 97383 net.cpp:225] Memory required for data: 31963332
I1031 13:08:36.352964 97383 layer_factory.hpp:114] Creating layer pool3
I1031 13:08:36.359336 97383 net.cpp:160] Creating Layer pool3
I1031 13:08:36.363608 97383 net.cpp:596] pool3 <- drop3
I1031 13:08:36.376510 97383 net.cpp:570] pool3 -> pool3
I1031 13:08:36.378619 97383 net.cpp:210] Setting up pool3
I1031 13:08:36.378904 97383 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 13:08:36.381732 97383 net.cpp:225] Memory required for data: 32063684
I1031 13:08:36.390584 97383 layer_factory.hpp:114] Creating layer conv4
I1031 13:08:36.393175 97383 net.cpp:160] Creating Layer conv4
I1031 13:08:36.395879 97383 net.cpp:596] conv4 <- pool3
I1031 13:08:36.405462 97383 net.cpp:570] conv4 -> conv4
I1031 13:08:36.773027 97383 net.cpp:210] Setting up conv4
I1031 13:08:36.777792 97383 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:08:36.786173 97383 net.cpp:225] Memory required for data: 32137412
I1031 13:08:36.796576 97383 layer_factory.hpp:114] Creating layer relu4
I1031 13:08:36.801301 97383 net.cpp:160] Creating Layer relu4
I1031 13:08:36.805724 97383 net.cpp:596] relu4 <- conv4
I1031 13:08:36.809909 97383 net.cpp:557] relu4 -> conv4 (in-place)
I1031 13:08:36.816916 97383 net.cpp:210] Setting up relu4
I1031 13:08:36.819414 97383 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:08:36.825948 97383 net.cpp:225] Memory required for data: 32211140
I1031 13:08:36.834405 97383 layer_factory.hpp:114] Creating layer dropout4
I1031 13:08:36.842636 97383 net.cpp:160] Creating Layer dropout4
I1031 13:08:36.853119 97383 net.cpp:596] dropout4 <- conv4
I1031 13:08:36.859242 97383 net.cpp:570] dropout4 -> drop4
I1031 13:08:36.866909 97383 net.cpp:210] Setting up dropout4
I1031 13:08:36.877547 97383 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:08:36.877996 97383 net.cpp:225] Memory required for data: 32284868
I1031 13:08:36.879995 97383 layer_factory.hpp:114] Creating layer pool4
I1031 13:08:36.888698 97383 net.cpp:160] Creating Layer pool4
I1031 13:08:36.893548 97383 net.cpp:596] pool4 <- drop4
I1031 13:08:36.901577 97383 net.cpp:570] pool4 -> pool4
I1031 13:08:36.918431 97383 net.cpp:210] Setting up pool4
I1031 13:08:36.930366 97383 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 13:08:36.939410 97383 net.cpp:225] Memory required for data: 32303300
I1031 13:08:36.947862 97383 layer_factory.hpp:114] Creating layer fc1
I1031 13:08:37.012315 97383 net.cpp:160] Creating Layer fc1
I1031 13:08:37.016996 97383 net.cpp:596] fc1 <- pool4
I1031 13:08:37.020290 97383 net.cpp:570] fc1 -> fc1
I1031 13:08:37.838047 97383 net.cpp:210] Setting up fc1
I1031 13:08:37.840339 97383 net.cpp:217] Top shape: 1 1024 (1024)
I1031 13:08:37.843133 97383 net.cpp:225] Memory required for data: 32307396
I1031 13:08:37.856335 97383 layer_factory.hpp:114] Creating layer dropout5
I1031 13:08:37.866428 97383 net.cpp:160] Creating Layer dropout5
I1031 13:08:37.875335 97383 net.cpp:596] dropout5 <- fc1
I1031 13:08:37.880069 97383 net.cpp:570] dropout5 -> drop5
I1031 13:08:37.884567 97383 net.cpp:210] Setting up dropout5
I1031 13:08:37.892984 97383 net.cpp:217] Top shape: 1 1024 (1024)
I1031 13:08:37.897737 97383 net.cpp:225] Memory required for data: 32311492
I1031 13:08:37.904347 97383 layer_factory.hpp:114] Creating layer fc2
I1031 13:08:37.911257 97383 net.cpp:160] Creating Layer fc2
I1031 13:08:37.917819 97383 net.cpp:596] fc2 <- drop5
I1031 13:08:37.920532 97383 net.cpp:570] fc2 -> fc2
I1031 13:08:37.946075 97383 net.cpp:210] Setting up fc2
I1031 13:08:37.959553 97383 net.cpp:217] Top shape: 1 2 (2)
I1031 13:08:37.970376 97383 net.cpp:225] Memory required for data: 32311500
I1031 13:08:37.979038 97383 layer_factory.hpp:114] Creating layer loss
I1031 13:08:38.009083 97383 net.cpp:160] Creating Layer loss
I1031 13:08:38.012182 97383 net.cpp:596] loss <- fc2
I1031 13:08:38.021080 97383 net.cpp:596] loss <- label
I1031 13:08:38.083096 97383 net.cpp:570] loss -> (automatic)
I1031 13:08:38.131613 97383 layer_factory.hpp:114] Creating layer loss
I1031 13:08:38.574370 97383 net.cpp:210] Setting up loss
I1031 13:08:38.587177 97383 net.cpp:217] Top shape: (1)
I1031 13:08:38.609624 97383 net.cpp:220]     with loss weight 1
I1031 13:08:38.745712 97383 net.cpp:225] Memory required for data: 32311504
I1031 13:08:38.802415 97383 net.cpp:287] loss needs backward computation.
I1031 13:08:38.906167 97383 net.cpp:287] fc2 needs backward computation.
I1031 13:08:38.920171 97383 net.cpp:287] dropout5 needs backward computation.
I1031 13:08:38.932003 97383 net.cpp:287] fc1 needs backward computation.
I1031 13:08:38.941490 97383 net.cpp:287] pool4 needs backward computation.
I1031 13:08:38.958204 97383 net.cpp:287] dropout4 needs backward computation.
I1031 13:08:38.959460 97383 net.cpp:287] relu4 needs backward computation.
I1031 13:08:38.969307 97383 net.cpp:287] conv4 needs backward computation.
I1031 13:08:38.986210 97383 net.cpp:287] pool3 needs backward computation.
I1031 13:08:38.999814 97383 net.cpp:287] dropout3 needs backward computation.
I1031 13:08:39.004649 97383 net.cpp:287] relu3 needs backward computation.
I1031 13:08:39.005004 97383 net.cpp:287] conv3 needs backward computation.
I1031 13:08:39.005322 97383 net.cpp:287] pool2 needs backward computation.
I1031 13:08:39.005578 97383 net.cpp:287] dropout2 needs backward computation.
I1031 13:08:39.005794 97383 net.cpp:287] relu2 needs backward computation.
I1031 13:08:39.005998 97383 net.cpp:287] conv2 needs backward computation.
I1031 13:08:39.006209 97383 net.cpp:287] pool1 needs backward computation.
I1031 13:08:39.006415 97383 net.cpp:287] dropout1 needs backward computation.
I1031 13:08:39.006623 97383 net.cpp:287] relu1 needs backward computation.
I1031 13:08:39.006824 97383 net.cpp:287] conv1 needs backward computation.
I1031 13:08:39.026937 97383 net.cpp:289] data does not need backward computation.
I1031 13:08:39.084565 97383 net.cpp:345] Network initialization done.
I1031 13:08:39.269027 97383 caffe.cpp:452] Performing Forward
I1031 13:08:50.964150 97383 caffe.cpp:457] Initial loss: 0.000179724
I1031 13:08:51.032171 97383 caffe.cpp:459] Performing Backward
I1031 13:08:54.262253 97383 caffe.cpp:468] *** Benchmark begins ***
I1031 13:08:54.280846 97383 caffe.cpp:469] Testing for 1 iterations.
I1031 13:08:54.435236 97383 caffe.cpp:482] Profiling Layer: pool4 forward
I1031 13:08:56.045158 97383 caffe.cpp:512] Iteration: 1 forward-backward time: 1597 ms.
I1031 13:08:56.212523 97383 caffe.cpp:519] Average time per layer: 
I1031 13:08:56.230494 97383 caffe.cpp:522]       data	forward: 49.942 ms.
I1031 13:08:56.304399 97383 caffe.cpp:526]       data	backward: 7.57 ms.
I1031 13:08:56.335093 97383 caffe.cpp:522]      conv1	forward: 76.296 ms.
I1031 13:08:56.355010 97383 caffe.cpp:526]      conv1	backward: 41.774 ms.
I1031 13:08:56.361287 97383 caffe.cpp:522]      relu1	forward: 111.773 ms.
I1031 13:08:56.390089 97383 caffe.cpp:526]      relu1	backward: 60.416 ms.
I1031 13:08:56.402971 97383 caffe.cpp:522]   dropout1	forward: 83.411 ms.
I1031 13:08:56.408473 97383 caffe.cpp:526]   dropout1	backward: 70.967 ms.
I1031 13:08:56.410166 97383 caffe.cpp:522]      pool1	forward: 128.491 ms.
I1031 13:08:56.410392 97383 caffe.cpp:526]      pool1	backward: 109.835 ms.
I1031 13:08:56.410598 97383 caffe.cpp:522]      conv2	forward: 61.434 ms.
I1031 13:08:56.410804 97383 caffe.cpp:526]      conv2	backward: 14.088 ms.
I1031 13:08:56.411788 97383 caffe.cpp:522]      relu2	forward: 23.248 ms.
I1031 13:08:56.412073 97383 caffe.cpp:526]      relu2	backward: 6.955 ms.
I1031 13:08:56.412293 97383 caffe.cpp:522]   dropout2	forward: 61.912 ms.
I1031 13:08:56.412660 97383 caffe.cpp:526]   dropout2	backward: 7.972 ms.
I1031 13:08:56.412883 97383 caffe.cpp:522]      pool2	forward: 30.004 ms.
I1031 13:08:56.413105 97383 caffe.cpp:526]      pool2	backward: 25.998 ms.
I1031 13:08:56.413322 97383 caffe.cpp:522]      conv3	forward: 44.648 ms.
I1031 13:08:56.413568 97383 caffe.cpp:526]      conv3	backward: 2.5 ms.
I1031 13:08:56.413767 97383 caffe.cpp:522]      relu3	forward: 16.064 ms.
I1031 13:08:56.413972 97383 caffe.cpp:526]      relu3	backward: 1.351 ms.
I1031 13:08:56.414170 97383 caffe.cpp:522]   dropout3	forward: 63.069 ms.
I1031 13:08:56.414374 97383 caffe.cpp:526]   dropout3	backward: 1.878 ms.
I1031 13:08:56.414609 97383 caffe.cpp:522]      pool3	forward: 15.347 ms.
I1031 13:08:56.414830 97383 caffe.cpp:526]      pool3	backward: 8.503 ms.
I1031 13:08:56.416038 97383 caffe.cpp:522]      conv4	forward: 52.025 ms.
I1031 13:08:56.416293 97383 caffe.cpp:526]      conv4	backward: 9.809 ms.
I1031 13:08:56.416497 97383 caffe.cpp:522]      relu4	forward: 16.503 ms.
I1031 13:08:56.416700 97383 caffe.cpp:526]      relu4	backward: 5.44 ms.
I1031 13:08:56.416898 97383 caffe.cpp:522]   dropout4	forward: 52.603 ms.
I1031 13:08:56.417101 97383 caffe.cpp:526]   dropout4	backward: 12.628 ms.
I1031 13:08:56.417304 97383 caffe.cpp:522]      pool4	forward: 35.151 ms.
I1031 13:08:56.417506 97383 caffe.cpp:526]      pool4	backward: 1.21 ms.
I1031 13:08:56.417703 97383 caffe.cpp:522]        fc1	forward: 41.856 ms.
I1031 13:08:56.417906 97383 caffe.cpp:526]        fc1	backward: 11.5 ms.
I1031 13:08:56.418146 97383 caffe.cpp:522]   dropout5	forward: 30.481 ms.
I1031 13:08:56.418365 97383 caffe.cpp:526]   dropout5	backward: 0.071 ms.
I1031 13:08:56.419724 97383 caffe.cpp:522]        fc2	forward: 18.447 ms.
I1031 13:08:56.419987 97383 caffe.cpp:526]        fc2	backward: 0.244 ms.
I1031 13:08:56.420194 97383 caffe.cpp:522]       loss	forward: 49.21 ms.
I1031 13:08:56.420400 97383 caffe.cpp:526]       loss	backward: 33.762 ms.
I1031 13:08:56.426129 97383 caffe.cpp:532] Average Forward pass: 1122 ms.
I1031 13:08:56.453852 97383 caffe.cpp:535] Average Backward pass: 444.181 ms.
I1031 13:08:56.465889 97383 caffe.cpp:537] Average Forward-Backward: 2098 ms.
I1031 13:08:56.482038 97383 caffe.cpp:540] Total Time: 2098 ms.
I1031 13:08:56.495157 97383 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 18457
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 0
elements_fp_double_1 = 0
elements_fp_double_2 = 0
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 18457
--->Total double-precision FLOPs = 0
--->Total FLOPs = 18457
mem-read-1 = 27475
mem-read-2 = 37
mem-read-4 = 276801
mem-read-8 = 434202
mem-read-16 = 0
mem-read-32 = 1
mem-read-64 = 1
mem-write-1 = 56
mem-write-2 = 17
mem-write-4 = 8906
mem-write-8 = 87135
mem-write-16 = 0
mem-write-32 = 1
mem-write-64 = 1
--->Total Bytes read = 4608465
--->Total Bytes written = 732890
--->Total Bytes = 5341355
