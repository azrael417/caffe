sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer19_dir1.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=19 -prof_forward_direction=1
I1031 13:26:06.291551 98032 caffe.cpp:444] Use CPU.
I1031 13:26:23.987951 98032 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 13:26:24.049722 98032 cpu_info.cpp:455] Total number of sockets: 1
I1031 13:26:24.062724 98032 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 13:26:24.074719 98032 cpu_info.cpp:461] Total number of processors: 256
I1031 13:26:24.091795 98032 cpu_info.cpp:464] GPU is used: no
I1031 13:26:24.101090 98032 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 13:26:24.109879 98032 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 13:26:24.121718 98032 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 13:26:33.184900 98032 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 13:26:33.866019 98032 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 13:26:36.259652 98032 layer_factory.hpp:114] Creating layer data
I1031 13:26:36.415716 98032 net.cpp:160] Creating Layer data
I1031 13:26:36.468377 98032 net.cpp:570] data -> data
I1031 13:26:36.957932 98032 net.cpp:570] data -> label
I1031 13:26:44.360160 98032 net.cpp:210] Setting up data
I1031 13:26:44.443819 98032 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 13:26:44.550007 98032 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 13:26:44.557387 98032 net.cpp:225] Memory required for data: 184516
I1031 13:26:44.637603 98032 layer_factory.hpp:114] Creating layer conv1
I1031 13:26:44.983765 98032 net.cpp:160] Creating Layer conv1
I1031 13:26:45.036859 98032 net.cpp:596] conv1 <- data
I1031 13:26:45.163592 98032 net.cpp:570] conv1 -> conv1
I1031 13:27:22.482177 98032 net.cpp:210] Setting up conv1
I1031 13:27:22.556829 98032 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:27:22.566679 98032 net.cpp:225] Memory required for data: 7805124
I1031 13:27:22.888937 98032 layer_factory.hpp:114] Creating layer relu1
I1031 13:27:23.030422 98032 net.cpp:160] Creating Layer relu1
I1031 13:27:23.035761 98032 net.cpp:596] relu1 <- conv1
I1031 13:27:23.071163 98032 net.cpp:557] relu1 -> conv1 (in-place)
I1031 13:27:23.285982 98032 net.cpp:210] Setting up relu1
I1031 13:27:23.288662 98032 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:27:23.289023 98032 net.cpp:225] Memory required for data: 15425732
I1031 13:27:23.289263 98032 layer_factory.hpp:114] Creating layer dropout1
I1031 13:27:23.321451 98032 net.cpp:160] Creating Layer dropout1
I1031 13:27:23.321779 98032 net.cpp:596] dropout1 <- conv1
I1031 13:27:23.324615 98032 net.cpp:570] dropout1 -> drop1
I1031 13:27:23.446770 98032 net.cpp:210] Setting up dropout1
I1031 13:27:23.460925 98032 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:27:23.461326 98032 net.cpp:225] Memory required for data: 23046340
I1031 13:27:23.461697 98032 layer_factory.hpp:114] Creating layer pool1
I1031 13:27:23.561630 98032 net.cpp:160] Creating Layer pool1
I1031 13:27:23.562041 98032 net.cpp:596] pool1 <- drop1
I1031 13:27:23.562423 98032 net.cpp:570] pool1 -> pool1
I1031 13:27:23.980079 98032 net.cpp:210] Setting up pool1
I1031 13:27:23.984904 98032 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 13:27:23.985278 98032 net.cpp:225] Memory required for data: 24951492
I1031 13:27:23.985570 98032 layer_factory.hpp:114] Creating layer conv2
I1031 13:27:24.046365 98032 net.cpp:160] Creating Layer conv2
I1031 13:27:24.050659 98032 net.cpp:596] conv2 <- pool1
I1031 13:27:24.065760 98032 net.cpp:570] conv2 -> conv2
I1031 13:27:30.900931 98032 net.cpp:210] Setting up conv2
I1031 13:27:30.901325 98032 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:27:30.907668 98032 net.cpp:225] Memory required for data: 26733764
I1031 13:27:30.970636 98032 layer_factory.hpp:114] Creating layer relu2
I1031 13:27:30.976090 98032 net.cpp:160] Creating Layer relu2
I1031 13:27:30.984308 98032 net.cpp:596] relu2 <- conv2
I1031 13:27:30.992998 98032 net.cpp:557] relu2 -> conv2 (in-place)
I1031 13:27:31.001736 98032 net.cpp:210] Setting up relu2
I1031 13:27:31.016317 98032 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:27:31.019330 98032 net.cpp:225] Memory required for data: 28516036
I1031 13:27:31.023929 98032 layer_factory.hpp:114] Creating layer dropout2
I1031 13:27:31.030508 98032 net.cpp:160] Creating Layer dropout2
I1031 13:27:31.038877 98032 net.cpp:596] dropout2 <- conv2
I1031 13:27:31.049767 98032 net.cpp:570] dropout2 -> drop2
I1031 13:27:31.052196 98032 net.cpp:210] Setting up dropout2
I1031 13:27:31.056843 98032 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:27:31.063232 98032 net.cpp:225] Memory required for data: 30298308
I1031 13:27:31.065613 98032 layer_factory.hpp:114] Creating layer pool2
I1031 13:27:31.075745 98032 net.cpp:160] Creating Layer pool2
I1031 13:27:31.078243 98032 net.cpp:596] pool2 <- drop2
I1031 13:27:31.084877 98032 net.cpp:570] pool2 -> pool2
I1031 13:27:31.095335 98032 net.cpp:210] Setting up pool2
I1031 13:27:31.099989 98032 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 13:27:31.106269 98032 net.cpp:225] Memory required for data: 30759108
I1031 13:27:31.110581 98032 layer_factory.hpp:114] Creating layer conv3
I1031 13:27:31.115284 98032 net.cpp:160] Creating Layer conv3
I1031 13:27:31.123797 98032 net.cpp:596] conv3 <- pool2
I1031 13:27:31.128252 98032 net.cpp:570] conv3 -> conv3
I1031 13:27:31.728417 98032 net.cpp:210] Setting up conv3
I1031 13:27:31.738716 98032 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:27:31.744868 98032 net.cpp:225] Memory required for data: 31160516
I1031 13:27:31.763172 98032 layer_factory.hpp:114] Creating layer relu3
I1031 13:27:31.768026 98032 net.cpp:160] Creating Layer relu3
I1031 13:27:31.778300 98032 net.cpp:596] relu3 <- conv3
I1031 13:27:31.784272 98032 net.cpp:557] relu3 -> conv3 (in-place)
I1031 13:27:31.786696 98032 net.cpp:210] Setting up relu3
I1031 13:27:31.799232 98032 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:27:31.809653 98032 net.cpp:225] Memory required for data: 31561924
I1031 13:27:31.815850 98032 layer_factory.hpp:114] Creating layer dropout3
I1031 13:27:31.826215 98032 net.cpp:160] Creating Layer dropout3
I1031 13:27:31.832139 98032 net.cpp:596] dropout3 <- conv3
I1031 13:27:31.836632 98032 net.cpp:570] dropout3 -> drop3
I1031 13:27:31.841092 98032 net.cpp:210] Setting up dropout3
I1031 13:27:31.847479 98032 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:27:31.847913 98032 net.cpp:225] Memory required for data: 31963332
I1031 13:27:31.862498 98032 layer_factory.hpp:114] Creating layer pool3
I1031 13:27:31.868922 98032 net.cpp:160] Creating Layer pool3
I1031 13:27:31.871475 98032 net.cpp:596] pool3 <- drop3
I1031 13:27:31.880414 98032 net.cpp:570] pool3 -> pool3
I1031 13:27:31.882659 98032 net.cpp:210] Setting up pool3
I1031 13:27:31.885375 98032 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 13:27:31.889858 98032 net.cpp:225] Memory required for data: 32063684
I1031 13:27:31.906203 98032 layer_factory.hpp:114] Creating layer conv4
I1031 13:27:31.912232 98032 net.cpp:160] Creating Layer conv4
I1031 13:27:31.917237 98032 net.cpp:596] conv4 <- pool3
I1031 13:27:31.927259 98032 net.cpp:570] conv4 -> conv4
I1031 13:27:32.323942 98032 net.cpp:210] Setting up conv4
I1031 13:27:32.333884 98032 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:27:32.341104 98032 net.cpp:225] Memory required for data: 32137412
I1031 13:27:32.353947 98032 layer_factory.hpp:114] Creating layer relu4
I1031 13:27:32.362515 98032 net.cpp:160] Creating Layer relu4
I1031 13:27:32.364261 98032 net.cpp:596] relu4 <- conv4
I1031 13:27:32.368865 98032 net.cpp:557] relu4 -> conv4 (in-place)
I1031 13:27:32.373642 98032 net.cpp:210] Setting up relu4
I1031 13:27:32.376058 98032 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:27:32.382211 98032 net.cpp:225] Memory required for data: 32211140
I1031 13:27:32.384706 98032 layer_factory.hpp:114] Creating layer dropout4
I1031 13:27:32.398488 98032 net.cpp:160] Creating Layer dropout4
I1031 13:27:32.403314 98032 net.cpp:596] dropout4 <- conv4
I1031 13:27:32.411782 98032 net.cpp:570] dropout4 -> drop4
I1031 13:27:32.414048 98032 net.cpp:210] Setting up dropout4
I1031 13:27:32.414327 98032 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:27:32.422657 98032 net.cpp:225] Memory required for data: 32284868
I1031 13:27:32.430589 98032 layer_factory.hpp:114] Creating layer pool4
I1031 13:27:32.438887 98032 net.cpp:160] Creating Layer pool4
I1031 13:27:32.447165 98032 net.cpp:596] pool4 <- drop4
I1031 13:27:32.452879 98032 net.cpp:570] pool4 -> pool4
I1031 13:27:32.468204 98032 net.cpp:210] Setting up pool4
I1031 13:27:32.470660 98032 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 13:27:32.473095 98032 net.cpp:225] Memory required for data: 32303300
I1031 13:27:32.483692 98032 layer_factory.hpp:114] Creating layer fc1
I1031 13:27:32.548707 98032 net.cpp:160] Creating Layer fc1
I1031 13:27:32.555171 98032 net.cpp:596] fc1 <- pool4
I1031 13:27:32.557814 98032 net.cpp:570] fc1 -> fc1
I1031 13:27:33.399313 98032 net.cpp:210] Setting up fc1
I1031 13:27:33.403914 98032 net.cpp:217] Top shape: 1 1024 (1024)
I1031 13:27:33.410429 98032 net.cpp:225] Memory required for data: 32307396
I1031 13:27:33.419430 98032 layer_factory.hpp:114] Creating layer dropout5
I1031 13:27:33.423914 98032 net.cpp:160] Creating Layer dropout5
I1031 13:27:33.426429 98032 net.cpp:596] dropout5 <- fc1
I1031 13:27:33.430414 98032 net.cpp:570] dropout5 -> drop5
I1031 13:27:33.430806 98032 net.cpp:210] Setting up dropout5
I1031 13:27:33.437677 98032 net.cpp:217] Top shape: 1 1024 (1024)
I1031 13:27:33.444365 98032 net.cpp:225] Memory required for data: 32311492
I1031 13:27:33.452739 98032 layer_factory.hpp:114] Creating layer fc2
I1031 13:27:33.457231 98032 net.cpp:160] Creating Layer fc2
I1031 13:27:33.461087 98032 net.cpp:596] fc2 <- drop5
I1031 13:27:33.465973 98032 net.cpp:570] fc2 -> fc2
I1031 13:27:33.501862 98032 net.cpp:210] Setting up fc2
I1031 13:27:33.520836 98032 net.cpp:217] Top shape: 1 2 (2)
I1031 13:27:33.530764 98032 net.cpp:225] Memory required for data: 32311500
I1031 13:27:33.544232 98032 layer_factory.hpp:114] Creating layer loss
I1031 13:27:33.576660 98032 net.cpp:160] Creating Layer loss
I1031 13:27:33.581318 98032 net.cpp:596] loss <- fc2
I1031 13:27:33.590389 98032 net.cpp:596] loss <- label
I1031 13:27:33.652191 98032 net.cpp:570] loss -> (automatic)
I1031 13:27:33.717710 98032 layer_factory.hpp:114] Creating layer loss
I1031 13:27:34.144093 98032 net.cpp:210] Setting up loss
I1031 13:27:34.156831 98032 net.cpp:217] Top shape: (1)
I1031 13:27:34.171674 98032 net.cpp:220]     with loss weight 1
I1031 13:27:34.310338 98032 net.cpp:225] Memory required for data: 32311504
I1031 13:27:34.361351 98032 net.cpp:287] loss needs backward computation.
I1031 13:27:34.465414 98032 net.cpp:287] fc2 needs backward computation.
I1031 13:27:34.480695 98032 net.cpp:287] dropout5 needs backward computation.
I1031 13:27:34.482882 98032 net.cpp:287] fc1 needs backward computation.
I1031 13:27:34.483719 98032 net.cpp:287] pool4 needs backward computation.
I1031 13:27:34.484036 98032 net.cpp:287] dropout4 needs backward computation.
I1031 13:27:34.484251 98032 net.cpp:287] relu4 needs backward computation.
I1031 13:27:34.494257 98032 net.cpp:287] conv4 needs backward computation.
I1031 13:27:34.511587 98032 net.cpp:287] pool3 needs backward computation.
I1031 13:27:34.525261 98032 net.cpp:287] dropout3 needs backward computation.
I1031 13:27:34.530042 98032 net.cpp:287] relu3 needs backward computation.
I1031 13:27:34.530369 98032 net.cpp:287] conv3 needs backward computation.
I1031 13:27:34.530694 98032 net.cpp:287] pool2 needs backward computation.
I1031 13:27:34.530964 98032 net.cpp:287] dropout2 needs backward computation.
I1031 13:27:34.531163 98032 net.cpp:287] relu2 needs backward computation.
I1031 13:27:34.531355 98032 net.cpp:287] conv2 needs backward computation.
I1031 13:27:34.531594 98032 net.cpp:287] pool1 needs backward computation.
I1031 13:27:34.531787 98032 net.cpp:287] dropout1 needs backward computation.
I1031 13:27:34.531980 98032 net.cpp:287] relu1 needs backward computation.
I1031 13:27:34.532166 98032 net.cpp:287] conv1 needs backward computation.
I1031 13:27:34.552044 98032 net.cpp:289] data does not need backward computation.
I1031 13:27:34.608919 98032 net.cpp:345] Network initialization done.
I1031 13:27:34.792510 98032 caffe.cpp:452] Performing Forward
I1031 13:27:46.759852 98032 caffe.cpp:457] Initial loss: 87.3365
I1031 13:27:46.883651 98032 caffe.cpp:459] Performing Backward
I1031 13:27:49.936552 98032 caffe.cpp:468] *** Benchmark begins ***
I1031 13:27:49.956356 98032 caffe.cpp:469] Testing for 1 iterations.
I1031 13:27:50.111150 98032 caffe.cpp:482] Profiling Layer: fc2 forward
I1031 13:27:51.717988 98032 caffe.cpp:512] Iteration: 1 forward-backward time: 1601 ms.
I1031 13:27:51.818586 98032 caffe.cpp:519] Average time per layer: 
I1031 13:27:51.839035 98032 caffe.cpp:522]       data	forward: 50.841 ms.
I1031 13:27:51.903105 98032 caffe.cpp:526]       data	backward: 8.516 ms.
I1031 13:27:51.932097 98032 caffe.cpp:522]      conv1	forward: 60.056 ms.
I1031 13:27:51.959101 98032 caffe.cpp:526]      conv1	backward: 49.587 ms.
I1031 13:27:51.964371 98032 caffe.cpp:522]      relu1	forward: 30.131 ms.
I1031 13:27:51.968384 98032 caffe.cpp:526]      relu1	backward: 59.438 ms.
I1031 13:27:51.973465 98032 caffe.cpp:522]   dropout1	forward: 84.471 ms.
I1031 13:27:51.980041 98032 caffe.cpp:526]   dropout1	backward: 64.537 ms.
I1031 13:27:51.989243 98032 caffe.cpp:522]      pool1	forward: 129.936 ms.
I1031 13:27:51.989557 98032 caffe.cpp:526]      pool1	backward: 141.821 ms.
I1031 13:27:51.992115 98032 caffe.cpp:522]      conv2	forward: 74.152 ms.
I1031 13:27:51.992386 98032 caffe.cpp:526]      conv2	backward: 61.102 ms.
I1031 13:27:51.992635 98032 caffe.cpp:522]      relu2	forward: 18.524 ms.
I1031 13:27:51.992866 98032 caffe.cpp:526]      relu2	backward: 7.036 ms.
I1031 13:27:51.993217 98032 caffe.cpp:522]   dropout2	forward: 63.429 ms.
I1031 13:27:51.994364 98032 caffe.cpp:526]   dropout2	backward: 7.773 ms.
I1031 13:27:51.994622 98032 caffe.cpp:522]      pool2	forward: 29.961 ms.
I1031 13:27:51.994832 98032 caffe.cpp:526]      pool2	backward: 25.846 ms.
I1031 13:27:51.995034 98032 caffe.cpp:522]      conv3	forward: 65.697 ms.
I1031 13:27:51.995236 98032 caffe.cpp:526]      conv3	backward: 2.59 ms.
I1031 13:27:51.995481 98032 caffe.cpp:522]      relu3	forward: 19.915 ms.
I1031 13:27:51.995687 98032 caffe.cpp:526]      relu3	backward: 1.471 ms.
I1031 13:27:51.995887 98032 caffe.cpp:522]   dropout3	forward: 59.046 ms.
I1031 13:27:51.996122 98032 caffe.cpp:526]   dropout3	backward: 8.996 ms.
I1031 13:27:51.996336 98032 caffe.cpp:522]      pool3	forward: 19.314 ms.
I1031 13:27:51.996613 98032 caffe.cpp:526]      pool3	backward: 5.931 ms.
I1031 13:27:51.996956 98032 caffe.cpp:522]      conv4	forward: 50.702 ms.
I1031 13:27:51.997174 98032 caffe.cpp:526]      conv4	backward: 9.686 ms.
I1031 13:27:51.997375 98032 caffe.cpp:522]      relu4	forward: 18.674 ms.
I1031 13:27:51.997577 98032 caffe.cpp:526]      relu4	backward: 5.41 ms.
I1031 13:27:51.997773 98032 caffe.cpp:522]   dropout4	forward: 38.68 ms.
I1031 13:27:51.997990 98032 caffe.cpp:526]   dropout4	backward: 12.512 ms.
I1031 13:27:51.998190 98032 caffe.cpp:522]      pool4	forward: 16.313 ms.
I1031 13:27:51.998391 98032 caffe.cpp:526]      pool4	backward: 1.201 ms.
I1031 13:27:51.998589 98032 caffe.cpp:522]        fc1	forward: 43.351 ms.
I1031 13:27:51.998793 98032 caffe.cpp:526]        fc1	backward: 11.688 ms.
I1031 13:27:51.998994 98032 caffe.cpp:522]   dropout5	forward: 34.997 ms.
I1031 13:27:51.999194 98032 caffe.cpp:526]   dropout5	backward: 0.074 ms.
I1031 13:27:52.019582 98032 caffe.cpp:522]        fc2	forward: 34.627 ms.
I1031 13:27:52.019999 98032 caffe.cpp:526]        fc2	backward: 0.208 ms.
I1031 13:27:52.020264 98032 caffe.cpp:522]       loss	forward: 40.503 ms.
I1031 13:27:52.020478 98032 caffe.cpp:526]       loss	backward: 33.967 ms.
I1031 13:27:52.026396 98032 caffe.cpp:532] Average Forward pass: 1043.15 ms.
I1031 13:27:52.040302 98032 caffe.cpp:535] Average Backward pass: 529.56 ms.
I1031 13:27:52.051931 98032 caffe.cpp:537] Average Forward-Backward: 2021 ms.
I1031 13:27:52.067312 98032 caffe.cpp:540] Total Time: 2021 ms.
I1031 13:27:52.080350 98032 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 14
elements_fp_single_4 = 0
elements_fp_single_8 = 8
elements_fp_single_16 = 258
elements_fp_double_1 = 0
elements_fp_double_2 = 0
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 4206
--->Total double-precision FLOPs = 0
--->Total FLOPs = 4206
mem-read-1 = 227167
mem-read-2 = 40
mem-read-4 = 1819746
mem-read-8 = 2505028
mem-read-16 = 0
mem-read-32 = 1
mem-read-64 = 198
mem-write-1 = 62
mem-write-2 = 17
mem-write-4 = 888
mem-write-8 = 230502
mem-write-16 = 0
mem-write-32 = 0
mem-write-64 = 4
--->Total Bytes read = 27559159
--->Total Bytes written = 1847920
--->Total Bytes = 29407079
