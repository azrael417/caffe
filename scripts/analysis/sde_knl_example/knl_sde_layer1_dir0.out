sde64 -knl -d -iform 1 -omix sde_knl/knl_sde_layer1_dir0.out.mix -global_region -start_ssc_mark 111:repeat -stop_ssc_mark 222:repeat -- /project/projectdirs/mpccc/tmalas/intelcaffe/install_carl/bin/caffe time -model=train_val.prototxt -iterations=1 -prof_layer=1 -prof_forward_direction=0
I1031 13:44:52.590677 98767 caffe.cpp:444] Use CPU.
I1031 13:45:10.534318 98767 cpu_info.cpp:452] Processor speed [MHz]: 0
I1031 13:45:10.597633 98767 cpu_info.cpp:455] Total number of sockets: 1
I1031 13:45:10.610780 98767 cpu_info.cpp:458] Total number of CPU cores: 64
I1031 13:45:10.622776 98767 cpu_info.cpp:461] Total number of processors: 256
I1031 13:45:10.640100 98767 cpu_info.cpp:464] GPU is used: no
I1031 13:45:10.649446 98767 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1031 13:45:10.658305 98767 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1031 13:45:10.670835 98767 cpu_info.cpp:473] Number of OpenMP threads: 16
I1031 13:45:19.916247 98767 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1031 13:45:20.607818 98767 net.cpp:120] Initializing net from parameters: 
name: "HEP_CLASSIFIER"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 1
      dim: 3
      dim: 124
      dim: 124
    }
    shape {
      dim: 1
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "conv1"
  top: "drop1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "drop1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "conv2"
  top: "drop2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "drop2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "drop3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "drop3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "conv4"
  top: "drop4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "drop4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dropout5"
  type: "Dropout"
  bottom: "fc1"
  top: "drop5"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop5"
  top: "fc2"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
}
I1031 13:45:23.051314 98767 layer_factory.hpp:114] Creating layer data
I1031 13:45:23.228816 98767 net.cpp:160] Creating Layer data
I1031 13:45:23.280194 98767 net.cpp:570] data -> data
I1031 13:45:23.780300 98767 net.cpp:570] data -> label
I1031 13:45:31.294651 98767 net.cpp:210] Setting up data
I1031 13:45:31.379130 98767 net.cpp:217] Top shape: 1 3 124 124 (46128)
I1031 13:45:31.486433 98767 net.cpp:217] Top shape: 1 1 1 1 (1)
I1031 13:45:31.493836 98767 net.cpp:225] Memory required for data: 184516
I1031 13:45:31.572966 98767 layer_factory.hpp:114] Creating layer conv1
I1031 13:45:31.929666 98767 net.cpp:160] Creating Layer conv1
I1031 13:45:31.984220 98767 net.cpp:596] conv1 <- data
I1031 13:45:32.112356 98767 net.cpp:570] conv1 -> conv1
I1031 13:46:09.365634 98767 net.cpp:210] Setting up conv1
I1031 13:46:09.450064 98767 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:46:09.459735 98767 net.cpp:225] Memory required for data: 7805124
I1031 13:46:09.785761 98767 layer_factory.hpp:114] Creating layer relu1
I1031 13:46:09.934890 98767 net.cpp:160] Creating Layer relu1
I1031 13:46:09.940794 98767 net.cpp:596] relu1 <- conv1
I1031 13:46:09.976529 98767 net.cpp:557] relu1 -> conv1 (in-place)
I1031 13:46:10.203899 98767 net.cpp:210] Setting up relu1
I1031 13:46:10.206555 98767 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:46:10.206934 98767 net.cpp:225] Memory required for data: 15425732
I1031 13:46:10.207156 98767 layer_factory.hpp:114] Creating layer dropout1
I1031 13:46:10.241029 98767 net.cpp:160] Creating Layer dropout1
I1031 13:46:10.241376 98767 net.cpp:596] dropout1 <- conv1
I1031 13:46:10.244187 98767 net.cpp:570] dropout1 -> drop1
I1031 13:46:10.356071 98767 net.cpp:210] Setting up dropout1
I1031 13:46:10.369652 98767 net.cpp:217] Top shape: 1 128 122 122 (1905152)
I1031 13:46:10.370048 98767 net.cpp:225] Memory required for data: 23046340
I1031 13:46:10.370388 98767 layer_factory.hpp:114] Creating layer pool1
I1031 13:46:10.478677 98767 net.cpp:160] Creating Layer pool1
I1031 13:46:10.478997 98767 net.cpp:596] pool1 <- drop1
I1031 13:46:10.479337 98767 net.cpp:570] pool1 -> pool1
I1031 13:46:10.887400 98767 net.cpp:210] Setting up pool1
I1031 13:46:10.896073 98767 net.cpp:217] Top shape: 1 128 61 61 (476288)
I1031 13:46:10.896477 98767 net.cpp:225] Memory required for data: 24951492
I1031 13:46:10.896769 98767 layer_factory.hpp:114] Creating layer conv2
I1031 13:46:10.958158 98767 net.cpp:160] Creating Layer conv2
I1031 13:46:10.962586 98767 net.cpp:596] conv2 <- pool1
I1031 13:46:10.977830 98767 net.cpp:570] conv2 -> conv2
I1031 13:46:17.827461 98767 net.cpp:210] Setting up conv2
I1031 13:46:17.836352 98767 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:46:17.845912 98767 net.cpp:225] Memory required for data: 26733764
I1031 13:46:17.921653 98767 layer_factory.hpp:114] Creating layer relu2
I1031 13:46:17.957167 98767 net.cpp:160] Creating Layer relu2
I1031 13:46:17.964342 98767 net.cpp:596] relu2 <- conv2
I1031 13:46:17.971285 98767 net.cpp:557] relu2 -> conv2 (in-place)
I1031 13:46:17.982319 98767 net.cpp:210] Setting up relu2
I1031 13:46:17.986316 98767 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:46:17.994702 98767 net.cpp:225] Memory required for data: 28516036
I1031 13:46:18.003412 98767 layer_factory.hpp:114] Creating layer dropout2
I1031 13:46:18.009232 98767 net.cpp:160] Creating Layer dropout2
I1031 13:46:18.016497 98767 net.cpp:596] dropout2 <- conv2
I1031 13:46:18.025343 98767 net.cpp:570] dropout2 -> drop2
I1031 13:46:18.029398 98767 net.cpp:210] Setting up dropout2
I1031 13:46:18.031780 98767 net.cpp:217] Top shape: 1 128 59 59 (445568)
I1031 13:46:18.042469 98767 net.cpp:225] Memory required for data: 30298308
I1031 13:46:18.050796 98767 layer_factory.hpp:114] Creating layer pool2
I1031 13:46:18.055308 98767 net.cpp:160] Creating Layer pool2
I1031 13:46:18.057781 98767 net.cpp:596] pool2 <- drop2
I1031 13:46:18.060159 98767 net.cpp:570] pool2 -> pool2
I1031 13:46:18.072732 98767 net.cpp:210] Setting up pool2
I1031 13:46:18.074585 98767 net.cpp:217] Top shape: 1 128 30 30 (115200)
I1031 13:46:18.077296 98767 net.cpp:225] Memory required for data: 30759108
I1031 13:46:18.089979 98767 layer_factory.hpp:114] Creating layer conv3
I1031 13:46:18.091995 98767 net.cpp:160] Creating Layer conv3
I1031 13:46:18.104943 98767 net.cpp:596] conv3 <- pool2
I1031 13:46:18.109467 98767 net.cpp:570] conv3 -> conv3
I1031 13:46:18.702687 98767 net.cpp:210] Setting up conv3
I1031 13:46:18.713896 98767 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:46:18.722523 98767 net.cpp:225] Memory required for data: 31160516
I1031 13:46:18.738610 98767 layer_factory.hpp:114] Creating layer relu3
I1031 13:46:18.747772 98767 net.cpp:160] Creating Layer relu3
I1031 13:46:18.756117 98767 net.cpp:596] relu3 <- conv3
I1031 13:46:18.758366 98767 net.cpp:557] relu3 -> conv3 (in-place)
I1031 13:46:18.765394 98767 net.cpp:210] Setting up relu3
I1031 13:46:18.773387 98767 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:46:18.782105 98767 net.cpp:225] Memory required for data: 31561924
I1031 13:46:18.790233 98767 layer_factory.hpp:114] Creating layer dropout3
I1031 13:46:18.794975 98767 net.cpp:160] Creating Layer dropout3
I1031 13:46:18.801483 98767 net.cpp:596] dropout3 <- conv3
I1031 13:46:18.803793 98767 net.cpp:570] dropout3 -> drop3
I1031 13:46:18.809840 98767 net.cpp:210] Setting up dropout3
I1031 13:46:18.816190 98767 net.cpp:217] Top shape: 1 128 28 28 (100352)
I1031 13:46:18.820564 98767 net.cpp:225] Memory required for data: 31963332
I1031 13:46:18.826900 98767 layer_factory.hpp:114] Creating layer pool3
I1031 13:46:18.837519 98767 net.cpp:160] Creating Layer pool3
I1031 13:46:18.839941 98767 net.cpp:596] pool3 <- drop3
I1031 13:46:18.844406 98767 net.cpp:570] pool3 -> pool3
I1031 13:46:18.848670 98767 net.cpp:210] Setting up pool3
I1031 13:46:18.848958 98767 net.cpp:217] Top shape: 1 128 14 14 (25088)
I1031 13:46:18.855659 98767 net.cpp:225] Memory required for data: 32063684
I1031 13:46:18.866487 98767 layer_factory.hpp:114] Creating layer conv4
I1031 13:46:18.872993 98767 net.cpp:160] Creating Layer conv4
I1031 13:46:18.877463 98767 net.cpp:596] conv4 <- pool3
I1031 13:46:18.890075 98767 net.cpp:570] conv4 -> conv4
I1031 13:46:19.276408 98767 net.cpp:210] Setting up conv4
I1031 13:46:19.282315 98767 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:46:19.292889 98767 net.cpp:225] Memory required for data: 32137412
I1031 13:46:19.304466 98767 layer_factory.hpp:114] Creating layer relu4
I1031 13:46:19.315210 98767 net.cpp:160] Creating Layer relu4
I1031 13:46:19.318976 98767 net.cpp:596] relu4 <- conv4
I1031 13:46:19.325950 98767 net.cpp:557] relu4 -> conv4 (in-place)
I1031 13:46:19.334491 98767 net.cpp:210] Setting up relu4
I1031 13:46:19.345271 98767 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:46:19.354339 98767 net.cpp:225] Memory required for data: 32211140
I1031 13:46:19.364959 98767 layer_factory.hpp:114] Creating layer dropout4
I1031 13:46:19.371400 98767 net.cpp:160] Creating Layer dropout4
I1031 13:46:19.382339 98767 net.cpp:596] dropout4 <- conv4
I1031 13:46:19.394923 98767 net.cpp:570] dropout4 -> drop4
I1031 13:46:19.403048 98767 net.cpp:210] Setting up dropout4
I1031 13:46:19.409976 98767 net.cpp:217] Top shape: 1 128 12 12 (18432)
I1031 13:46:19.417937 98767 net.cpp:225] Memory required for data: 32284868
I1031 13:46:19.422760 98767 layer_factory.hpp:114] Creating layer pool4
I1031 13:46:19.431850 98767 net.cpp:160] Creating Layer pool4
I1031 13:46:19.441045 98767 net.cpp:596] pool4 <- drop4
I1031 13:46:19.452834 98767 net.cpp:570] pool4 -> pool4
I1031 13:46:19.467764 98767 net.cpp:210] Setting up pool4
I1031 13:46:19.468112 98767 net.cpp:217] Top shape: 1 128 6 6 (4608)
I1031 13:46:19.470556 98767 net.cpp:225] Memory required for data: 32303300
I1031 13:46:19.470808 98767 layer_factory.hpp:114] Creating layer fc1
I1031 13:46:19.534970 98767 net.cpp:160] Creating Layer fc1
I1031 13:46:19.535311 98767 net.cpp:596] fc1 <- pool4
I1031 13:46:19.535758 98767 net.cpp:570] fc1 -> fc1
I1031 13:46:20.401078 98767 net.cpp:210] Setting up fc1
I1031 13:46:20.403709 98767 net.cpp:217] Top shape: 1 1024 (1024)
I1031 13:46:20.408519 98767 net.cpp:225] Memory required for data: 32307396
I1031 13:46:20.417537 98767 layer_factory.hpp:114] Creating layer dropout5
I1031 13:46:20.422070 98767 net.cpp:160] Creating Layer dropout5
I1031 13:46:20.424496 98767 net.cpp:596] dropout5 <- fc1
I1031 13:46:20.437222 98767 net.cpp:570] dropout5 -> drop5
I1031 13:46:20.443837 98767 net.cpp:210] Setting up dropout5
I1031 13:46:20.448465 98767 net.cpp:217] Top shape: 1 1024 (1024)
I1031 13:46:20.448865 98767 net.cpp:225] Memory required for data: 32311492
I1031 13:46:20.457749 98767 layer_factory.hpp:114] Creating layer fc2
I1031 13:46:20.465703 98767 net.cpp:160] Creating Layer fc2
I1031 13:46:20.472442 98767 net.cpp:596] fc2 <- drop5
I1031 13:46:20.480722 98767 net.cpp:570] fc2 -> fc2
I1031 13:46:20.507411 98767 net.cpp:210] Setting up fc2
I1031 13:46:20.523048 98767 net.cpp:217] Top shape: 1 2 (2)
I1031 13:46:20.531971 98767 net.cpp:225] Memory required for data: 32311500
I1031 13:46:20.540833 98767 layer_factory.hpp:114] Creating layer loss
I1031 13:46:20.576012 98767 net.cpp:160] Creating Layer loss
I1031 13:46:20.581895 98767 net.cpp:596] loss <- fc2
I1031 13:46:20.593608 98767 net.cpp:596] loss <- label
I1031 13:46:20.649626 98767 net.cpp:570] loss -> (automatic)
I1031 13:46:20.692149 98767 layer_factory.hpp:114] Creating layer loss
I1031 13:46:21.106057 98767 net.cpp:210] Setting up loss
I1031 13:46:21.117313 98767 net.cpp:217] Top shape: (1)
I1031 13:46:21.136023 98767 net.cpp:220]     with loss weight 1
I1031 13:46:21.275653 98767 net.cpp:225] Memory required for data: 32311504
I1031 13:46:21.325043 98767 net.cpp:287] loss needs backward computation.
I1031 13:46:21.422426 98767 net.cpp:287] fc2 needs backward computation.
I1031 13:46:21.429884 98767 net.cpp:287] dropout5 needs backward computation.
I1031 13:46:21.432202 98767 net.cpp:287] fc1 needs backward computation.
I1031 13:46:21.433012 98767 net.cpp:287] pool4 needs backward computation.
I1031 13:46:21.433293 98767 net.cpp:287] dropout4 needs backward computation.
I1031 13:46:21.433542 98767 net.cpp:287] relu4 needs backward computation.
I1031 13:46:21.443706 98767 net.cpp:287] conv4 needs backward computation.
I1031 13:46:21.460966 98767 net.cpp:287] pool3 needs backward computation.
I1031 13:46:21.474687 98767 net.cpp:287] dropout3 needs backward computation.
I1031 13:46:21.479547 98767 net.cpp:287] relu3 needs backward computation.
I1031 13:46:21.479876 98767 net.cpp:287] conv3 needs backward computation.
I1031 13:46:21.480211 98767 net.cpp:287] pool2 needs backward computation.
I1031 13:46:21.480476 98767 net.cpp:287] dropout2 needs backward computation.
I1031 13:46:21.480679 98767 net.cpp:287] relu2 needs backward computation.
I1031 13:46:21.480870 98767 net.cpp:287] conv2 needs backward computation.
I1031 13:46:21.481065 98767 net.cpp:287] pool1 needs backward computation.
I1031 13:46:21.481257 98767 net.cpp:287] dropout1 needs backward computation.
I1031 13:46:21.481448 98767 net.cpp:287] relu1 needs backward computation.
I1031 13:46:21.481636 98767 net.cpp:287] conv1 needs backward computation.
I1031 13:46:21.501813 98767 net.cpp:289] data does not need backward computation.
I1031 13:46:21.561513 98767 net.cpp:345] Network initialization done.
I1031 13:46:21.740551 98767 caffe.cpp:452] Performing Forward
I1031 13:46:33.448952 98767 caffe.cpp:457] Initial loss: 4.88759e-06
I1031 13:46:33.526226 98767 caffe.cpp:459] Performing Backward
I1031 13:46:36.853114 98767 caffe.cpp:468] *** Benchmark begins ***
I1031 13:46:36.870988 98767 caffe.cpp:469] Testing for 1 iterations.
I1031 13:46:37.033762 98767 caffe.cpp:485] Profiling Layer: conv1 backward
I1031 13:46:38.666124 98767 caffe.cpp:512] Iteration: 1 forward-backward time: 1623 ms.
I1031 13:46:38.831358 98767 caffe.cpp:519] Average time per layer: 
I1031 13:46:38.851742 98767 caffe.cpp:522]       data	forward: 46.945 ms.
I1031 13:46:38.929817 98767 caffe.cpp:526]       data	backward: 3.657 ms.
I1031 13:46:38.959633 98767 caffe.cpp:522]      conv1	forward: 73.775 ms.
I1031 13:46:38.966825 98767 caffe.cpp:526]      conv1	backward: 48.788 ms.
I1031 13:46:38.985126 98767 caffe.cpp:522]      relu1	forward: 22.331 ms.
I1031 13:46:38.991858 98767 caffe.cpp:526]      relu1	backward: 62.423 ms.
I1031 13:46:38.996263 98767 caffe.cpp:522]   dropout1	forward: 93.826 ms.
I1031 13:46:39.002837 98767 caffe.cpp:526]   dropout1	backward: 68.733 ms.
I1031 13:46:39.009029 98767 caffe.cpp:522]      pool1	forward: 130.281 ms.
I1031 13:46:39.024479 98767 caffe.cpp:526]      pool1	backward: 109.74 ms.
I1031 13:46:39.031452 98767 caffe.cpp:522]      conv2	forward: 71.566 ms.
I1031 13:46:39.031898 98767 caffe.cpp:526]      conv2	backward: 14.305 ms.
I1031 13:46:39.037168 98767 caffe.cpp:522]      relu2	forward: 18.796 ms.
I1031 13:46:39.038771 98767 caffe.cpp:526]      relu2	backward: 6.69 ms.
I1031 13:46:39.039623 98767 caffe.cpp:522]   dropout2	forward: 63.813 ms.
I1031 13:46:39.040026 98767 caffe.cpp:526]   dropout2	backward: 7.763 ms.
I1031 13:46:39.040235 98767 caffe.cpp:522]      pool2	forward: 29.958 ms.
I1031 13:46:39.040442 98767 caffe.cpp:526]      pool2	backward: 25.45 ms.
I1031 13:46:39.040645 98767 caffe.cpp:522]      conv3	forward: 64.874 ms.
I1031 13:46:39.040848 98767 caffe.cpp:526]      conv3	backward: 2.549 ms.
I1031 13:46:39.041046 98767 caffe.cpp:522]      relu3	forward: 16.112 ms.
I1031 13:46:39.041862 98767 caffe.cpp:526]      relu3	backward: 1.343 ms.
I1031 13:46:39.042094 98767 caffe.cpp:522]   dropout3	forward: 58.77 ms.
I1031 13:46:39.042302 98767 caffe.cpp:526]   dropout3	backward: 1.874 ms.
I1031 13:46:39.042501 98767 caffe.cpp:522]      pool3	forward: 21.333 ms.
I1031 13:46:39.042740 98767 caffe.cpp:526]      pool3	backward: 5.886 ms.
I1031 13:46:39.042954 98767 caffe.cpp:522]      conv4	forward: 61.729 ms.
I1031 13:46:39.043229 98767 caffe.cpp:526]      conv4	backward: 12.61 ms.
I1031 13:46:39.043624 98767 caffe.cpp:522]      relu4	forward: 13.876 ms.
I1031 13:46:39.043841 98767 caffe.cpp:526]      relu4	backward: 5.458 ms.
I1031 13:46:39.044041 98767 caffe.cpp:522]   dropout4	forward: 58.111 ms.
I1031 13:46:39.044246 98767 caffe.cpp:526]   dropout4	backward: 12.764 ms.
I1031 13:46:39.044450 98767 caffe.cpp:522]      pool4	forward: 29.278 ms.
I1031 13:46:39.044653 98767 caffe.cpp:526]      pool4	backward: 1.165 ms.
I1031 13:46:39.044850 98767 caffe.cpp:522]        fc1	forward: 52.326 ms.
I1031 13:46:39.045053 98767 caffe.cpp:526]        fc1	backward: 11.599 ms.
I1031 13:46:39.045258 98767 caffe.cpp:522]   dropout5	forward: 43.603 ms.
I1031 13:46:39.045460 98767 caffe.cpp:526]   dropout5	backward: 0.099 ms.
I1031 13:46:39.051995 98767 caffe.cpp:522]        fc2	forward: 15.827 ms.
I1031 13:46:39.052402 98767 caffe.cpp:526]        fc2	backward: 0.234 ms.
I1031 13:46:39.052669 98767 caffe.cpp:522]       loss	forward: 96.246 ms.
I1031 13:46:39.052881 98767 caffe.cpp:526]       loss	backward: 37.996 ms.
I1031 13:46:39.058660 98767 caffe.cpp:532] Average Forward pass: 1143.56 ms.
I1031 13:46:39.072142 98767 caffe.cpp:535] Average Backward pass: 450.775 ms.
I1031 13:46:39.085522 98767 caffe.cpp:537] Average Forward-Backward: 2134 ms.
I1031 13:46:39.100890 98767 caffe.cpp:540] Total Time: 2134 ms.
I1031 13:46:39.113533 98767 caffe.cpp:541] *** Benchmark ends ***
Search stanza is "EMIT_GLOBAL_DYNAMIC_STATS"
elements_fp_single_1 = 0
elements_fp_single_4 = 0
elements_fp_single_8 = 0
elements_fp_single_16 = 6548960
elements_fp_double_1 = 0
elements_fp_double_2 = 0
elements_fp_double_4 = 0
elements_fp_double_8 = 0
--->Total single-precision FLOPs = 104783360
--->Total double-precision FLOPs = 0
--->Total FLOPs = 104783360
mem-read-1 = 47002
mem-read-2 = 79
mem-read-4 = 3597393
mem-read-8 = 529960
mem-read-16 = 1
mem-read-32 = 2
mem-read-64 = 1905372
mem-write-1 = 102
mem-write-2 = 34
mem-write-4 = 2205
mem-write-8 = 52615
mem-write-16 = 5
mem-write-32 = 2
mem-write-64 = 444
--->Total Bytes read = 140620300
--->Total Bytes written = 458470
--->Total Bytes = 141078770
